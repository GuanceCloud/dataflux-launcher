-- -------------------------------------------------------------
-- TablePlus 3.5.4(317)
--
-- https://tableplus.com/
--
-- Database: ft_data_processor
-- Generation Time: 2020-07-01 03:29:20.5090
-- -------------------------------------------------------------


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;


DROP TABLE IF EXISTS `biz_cache_df_workspace`;
CREATE TABLE `biz_cache_df_workspace` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `uuid` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT 'UUID',
  `wsName` varchar(256) DEFAULT NULL COMMENT '名称',
  `dataJSON` json NOT NULL COMMENT '数据JSON',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `BIZ` (`id`),
  UNIQUE KEY `UUID` (`uuid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='DataFlux工作空间（缓存）';

DROP TABLE IF EXISTS `biz_main_auth_link`;
CREATE TABLE `biz_main_auth_link` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '兼任Token',
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '函数ID',
  `funcKwargsJSON` json NOT NULL COMMENT '函数参数JSON (kwargs)',
  `expireTime` datetime DEFAULT NULL COMMENT '过期时间（NULL表示永不过期）',
  `throttlingJSON` json DEFAULT NULL COMMENT '限流JSON（value="<From Parameter>"表示从参数获取）',
  `origin` varchar(64) NOT NULL DEFAULT 'API' COMMENT '来源 API|UI',
  `showInDoc` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否在文档中显示',
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否禁用',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='授权链接';

DROP TABLE IF EXISTS `biz_main_batch`;
CREATE TABLE `biz_main_batch` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '兼任Token',
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '函数ID',
  `funcKwargsJSON` json NOT NULL COMMENT '函数参数JSON (kwargs)',
  `tagsJSON` json DEFAULT NULL COMMENT '批处理标签JSON',
  `origin` varchar(64) NOT NULL DEFAULT 'API' COMMENT '来源 API|UI',
  `showInDoc` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否在文档中显示',
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否禁用',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='批处理';

DROP TABLE IF EXISTS `biz_main_batch_task_info`;
CREATE TABLE `biz_main_batch_task_info` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `batchId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '批处理ID',
  `queueTime` timestamp NULL DEFAULT NULL COMMENT '入队时间',
  `startTime` timestamp NULL DEFAULT NULL COMMENT '启动时间',
  `endTime` timestamp NULL DEFAULT NULL COMMENT '结束时间',
  `status` varchar(64) NOT NULL DEFAULT 'queued' COMMENT '状态 queued|pending|success|failure',
  `logMessageTEXT` text COMMENT '日志信息TEXT',
  `einfoTEXT` text COMMENT '错误信息TEXT',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `BATCH_ID` (`batchId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='批处理任务信息';

DROP TABLE IF EXISTS `biz_main_crontab_config`;
CREATE TABLE `biz_main_crontab_config` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '函数ID',
  `funcKwargsJSON` json NOT NULL COMMENT '函数参数JSON (kwargs)',
  `crontab` varchar(64) DEFAULT NULL COMMENT '执行频率（Crontab语法）',
  `tagsJSON` json DEFAULT NULL COMMENT '自动触发配置标签JSON',
  `saveResult` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否需要保存结果',
  `scope` varchar(256) NOT NULL DEFAULT 'GLOBAL' COMMENT '范围',
  `configHash` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL COMMENT '配置哈希',
  `expireTime` timestamp NULL DEFAULT NULL COMMENT '过期时间',
  `origin` varchar(64) NOT NULL DEFAULT 'API' COMMENT '来源 API|UI',
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否已禁用',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `SCOPE_CONFIG_HASH` (`scope`,`configHash`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='自动触发配置';

DROP TABLE IF EXISTS `biz_main_crontab_task_info`;
CREATE TABLE `biz_main_crontab_task_info` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `crontabConfigId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '自动触发配置ID',
  `queueTime` timestamp NULL DEFAULT NULL COMMENT '入队时间',
  `startTime` timestamp NULL DEFAULT NULL COMMENT '启动时间',
  `endTime` timestamp NULL DEFAULT NULL COMMENT '结束时间',
  `status` varchar(64) NOT NULL DEFAULT 'queued' COMMENT '状态 queued|pending|success|failure',
  `logMessageTEXT` text COMMENT '日志信息TEXT',
  `einfoTEXT` text COMMENT '错误信息TEXT',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `CRONTAB_CONFIG_ID` (`crontabConfigId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='自动触发任务信息';

DROP TABLE IF EXISTS `biz_main_data_source`;
CREATE TABLE `biz_main_data_source` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述',
  `refName` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '引用名',
  `type` varchar(64) NOT NULL COMMENT '类型 influxdb|mysql|redis|..',
  `configJSON` json NOT NULL COMMENT '配置JSON',
  `isBuiltin` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否为内建数据源',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`refName`)
) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8mb4 COMMENT='数据源';

DROP TABLE IF EXISTS `biz_main_env_variable`;
CREATE TABLE `biz_main_env_variable` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述',
  `refName` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '引用名',
  `valueTEXT` text NOT NULL COMMENT '值',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`refName`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='环境变量';

DROP TABLE IF EXISTS `biz_main_feedback`;
CREATE TABLE `biz_main_feedback` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `descriptionTEXT` text COMMENT '描述TEXT',
  `locationHrefTEXT` text COMMENT '浏览器location.href内容TEXT',
  `vuexStateJSON` json DEFAULT NULL COMMENT 'vuex state内容JSON',
  `screenshotBase64` longtext COMMENT '屏幕截图Base64',
  `contactMail` text COMMENT '联系邮件',
  `status` varchar(64) NOT NULL DEFAULT 'open' COMMENT '状态 open|close',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='反馈';

DROP TABLE IF EXISTS `biz_main_func`;
CREATE TABLE `biz_main_func` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述（函数文档）',
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '所属脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '所属脚本ID',
  `refName` varchar(256) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '引用名（函数名）',
  `definition` varchar(256) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '定义（函数签名）',
  `argsJSON` json DEFAULT NULL COMMENT '位置参数JSON',
  `kwargsJSON` json DEFAULT NULL COMMENT '命名参数JSON',
  `extraConfigJSON` json DEFAULT NULL COMMENT '函数额外配置JSON',
  `category` varchar(64) NOT NULL DEFAULT 'general' COMMENT '类别 general|prediction|transformation|action|command|query|check',
  `tagsJSON` json DEFAULT NULL COMMENT '函数标签JSON',
  `defOrder` int(11) NOT NULL DEFAULT '0' COMMENT '定义顺序',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6),
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`scriptId`,`refName`),
  KEY `REF_NAME` (`refName`),
  KEY `CATEGORY` (`category`)
) ENGINE=InnoDB AUTO_INCREMENT=50 DEFAULT CHARSET=utf8mb4 COMMENT='函数';

DROP TABLE IF EXISTS `biz_main_func_store`;
CREATE TABLE `biz_main_func_store` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `key` varchar(256) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '键名',
  `valueJSON` json NOT NULL COMMENT '值JSON',
  `scope` varchar(256) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT 'GLOBAL' COMMENT '范围',
  `expireAt` int(11) DEFAULT NULL COMMENT '过期时间（秒级UNIX时间戳）',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`scope`,`key`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='函数存储';

DROP TABLE IF EXISTS `biz_main_script`;
CREATE TABLE `biz_main_script` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述',
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '所属脚本集ID',
  `refName` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '引用名',
  `publishVersion` bigint(20) NOT NULL DEFAULT '0' COMMENT '发布版本',
  `type` varchar(64) NOT NULL DEFAULT 'python' COMMENT '类型 python|javascript',
  `code` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_bin COMMENT '代码',
  `codeMD5` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL COMMENT '代码MD5',
  `codeDraft` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_bin COMMENT '代码（编辑中草稿）',
  `codeDraftMD5` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL COMMENT '代码（编辑中草稿）MD5',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`scriptSetId`,`refName`),
  FULLTEXT KEY `FT` (`refName`,`code`,`codeDraft`)
) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8mb4 COMMENT='脚本';

DROP TABLE IF EXISTS `biz_main_script_failure`;
CREATE TABLE `biz_main_script_failure` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `scriptPublishVersion` bigint(20) NOT NULL COMMENT '脚本发布版本',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '函数ID',
  `execMode` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '执行模式 sync|async|crontab',
  `einfoTEXT` text COMMENT '错误信息',
  `exception` varchar(64) DEFAULT NULL COMMENT '异常',
  `traceInfoJSON` json DEFAULT NULL COMMENT '跟踪信息JSON',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ENTRY` (`scriptId`,`scriptPublishVersion`,`funcId`),
  KEY `CREATE_TIME` (`createTime`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本故障信息';

DROP TABLE IF EXISTS `biz_main_script_log`;
CREATE TABLE `biz_main_script_log` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `scriptPublishVersion` bigint(20) NOT NULL COMMENT '脚本发布版本',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '函数ID',
  `execMode` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '执行模式 sync|async|crontab',
  `messageTEXT` text COMMENT '日志信息TEXT',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ENTRY` (`scriptId`,`scriptPublishVersion`,`funcId`),
  KEY `CREATE_TIME` (`createTime`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本日志信息';

DROP TABLE IF EXISTS `biz_main_script_publish_history`;
CREATE TABLE `biz_main_script_publish_history` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `scriptSetId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `scriptPublishVersion` bigint(20) NOT NULL COMMENT '脚本发布版本',
  `scriptCode_cache` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本代码（缓存字段）',
  `note` text COMMENT '发布备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`scriptId`,`scriptPublishVersion`),
  FULLTEXT KEY `FT` (`scriptCode_cache`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本发布历史';

DROP TABLE IF EXISTS `biz_main_script_recover_point`;
CREATE TABLE `biz_main_script_recover_point` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `type` varchar(64) NOT NULL COMMENT '类型 publish|import|manual',
  `tableDumpJSON` json NOT NULL COMMENT '表备份数据JSON',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本还原点';

DROP TABLE IF EXISTS `biz_main_script_set`;
CREATE TABLE `biz_main_script_set` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述',
  `refName` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '引用名',
  `type` varchar(64) NOT NULL DEFAULT 'user' COMMENT '类型 user|official',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`refName`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 COMMENT='脚本集';

DROP TABLE IF EXISTS `biz_main_script_set_export_history`;
CREATE TABLE `biz_main_script_set_export_history` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `importToken` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '导入令牌',
  `exportType` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '导出类型 user|official',
  `operatorName` text COMMENT '操作人',
  `operationNote` text COMMENT '操作备注',
  `summaryJSON` json NOT NULL COMMENT '导出摘要JSON',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='导出历史';

DROP TABLE IF EXISTS `biz_main_script_set_import_history`;
CREATE TABLE `biz_main_script_set_import_history` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `importType` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '导入类型 user|official',
  `operationNote` text COMMENT '操作备注',
  `summaryJSON` json NOT NULL COMMENT '导出摘要JSON',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='导出历史';

DROP TABLE IF EXISTS `biz_main_task_result_data_processor`;
CREATE TABLE `biz_main_task_result_data_processor` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `task` varchar(128) NOT NULL,
  `origin` varchar(128) DEFAULT NULL,
  `startTime` int(11) DEFAULT NULL COMMENT '任务开始时间(秒级UNIX时间戳)',
  `endTime` int(11) DEFAULT NULL COMMENT '任务结束时间(秒级UNIX时间戳)',
  `argsJSON` json DEFAULT NULL COMMENT '列表参数JSON',
  `kwargsJSON` json DEFAULT NULL COMMENT '字典参数JSON',
  `retvalJSON` json DEFAULT NULL COMMENT '执行结果JSON',
  `status` varchar(64) NOT NULL COMMENT '任务状态: SUCCESS|FAILURE',
  `einfoTEXT` text COMMENT '错误信息TEXT',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `TASK` (`task`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='FTDataProcessor 任务结果';

DROP TABLE IF EXISTS `biz_rel_func_df_workspace`;
CREATE TABLE `biz_rel_func_df_workspace` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `funcId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '函数ID',
  `dfWorkspaceUUID` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT 'DataFlux 工作空间 UUID',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `BIZ` (`funcId`,`dfWorkspaceUUID`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='函数-DataFlux工作空间关联';

DROP TABLE IF EXISTS `biz_rel_script_running_info`;
CREATE TABLE `biz_rel_script_running_info` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `scriptSetId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '函数ID',
  `execMode` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '执行模式 sync|async|crontab',
  `succeedCount` int(11) NOT NULL DEFAULT '0' COMMENT '成功次数',
  `failCount` int(11) NOT NULL DEFAULT '0' COMMENT '失败次数',
  `minCost` int(11) DEFAULT NULL COMMENT '最小耗时（毫秒）',
  `maxCost` int(11) DEFAULT NULL COMMENT '最大耗时（毫秒）',
  `totalCost` bigint(11) DEFAULT NULL COMMENT '累积耗时（毫秒）',
  `latestCost` int(11) DEFAULT NULL COMMENT '最近消耗（毫秒）',
  `latestSucceedTime` timestamp NULL DEFAULT NULL COMMENT '最近成功时间',
  `latestFailTime` timestamp NULL DEFAULT NULL COMMENT '最近失败时间',
  `status` varchar(64) NOT NULL DEFAULT 'neverStarted' COMMENT '状态 neverStarted|succeed|fail|timeout',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `BIZ` (`scriptSetId`,`scriptId`,`funcId`,`execMode`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本执行信息';

DROP TABLE IF EXISTS `wat_main_access_key`;
CREATE TABLE `wat_main_access_key` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `organizationId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `userId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `name` varchar(256) NOT NULL,
  `secret` varchar(64) NOT NULL,
  `webhookURL` text,
  `webhookEvents` text,
  `allowWebhookEcho` tinyint(1) NOT NULL DEFAULT '0',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORGANIZATION_ID` (`organizationId`),
  KEY `USER_ID` (`userId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_file`;
CREATE TABLE `wat_main_file` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `organizationId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `userId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `originalFileName` text,
  `md5Sum` char(32) DEFAULT NULL,
  `byteSize` int(11) DEFAULT NULL,
  `note` text,
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORGANIZATION_ID` (`organizationId`),
  KEY `USER_ID` (`userId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_organization`;
CREATE TABLE `wat_main_organization` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `uniqueId` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `name` varchar(256) NOT NULL,
  `markers` text,
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `UNIQUE_ID` (`uniqueId`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_post`;
CREATE TABLE `wat_main_post` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `organizationId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `userId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) NOT NULL,
  `content` text,
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORGANIZATION_ID` (`organizationId`),
  KEY `USER_ID` (`userId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_system_config`;
CREATE TABLE `wat_main_system_config` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `value` text CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '值',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_task_result_example`;
CREATE TABLE `wat_main_task_result_example` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `task` varchar(128) NOT NULL,
  `origin` varchar(128) DEFAULT NULL,
  `startTime` int(11) DEFAULT NULL,
  `endTime` int(11) DEFAULT NULL,
  `argsJSON` text,
  `kwargsJSON` text,
  `retvalJSON` text,
  `status` varchar(64) NOT NULL,
  `einfoTEXT` text,
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `TASK` (`task`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_user`;
CREATE TABLE `wat_main_user` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `organizationId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `username` varchar(64) NOT NULL,
  `passwordHash` text,
  `name` varchar(256) DEFAULT NULL,
  `mobile` varchar(32) DEFAULT NULL,
  `markers` text,
  `roles` text,
  `customPrivileges` text,
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `USERNAME` (`username`),
  KEY `ORGANIZATION_ID` (`organizationId`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_ref_tag`;
CREATE TABLE `wat_ref_tag` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `entityId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `entityName` varchar(64) NOT NULL,
  `tagK` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `tagV` text,
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `TAG` (`entityId`,`tagK`),
  KEY `ENTITY_ID` (`entityId`),
  KEY `TAG_K` (`tagK`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

INSERT INTO `biz_main_data_source` (`seq`, `id`, `title`, `description`, `refName`, `type`, `configJSON`, `isBuiltin`, `createTime`, `updateTime`) VALUES
('1', 'dsrc-demo_influxdb', '示例InfluxDB', '一个用于演示的InfluxDB', 'demo_influxdb', 'influxdb', '{\"host\": \"127.0.0.1\", \"port\": 8086, \"user\": null, \"charset\": null, \"database\": \"demo\", \"password\": null}', '0', '2019-12-09 15:30:11', '2020-02-26 00:57:15'),
('2', 'dsrc-demo_mysql', '示例MySQL', '一个用于演示的MySQL', 'demo_mysql', 'mysql', '{\"host\": \"127.0.0.1\", \"port\": 3306, \"user\": \"dev\", \"charset\": \"utf8mb4\", \"database\": \"demo\", \"password\": \"dev\"}', '0', '2019-12-09 15:33:02', '2020-02-26 00:57:18'),
('3', 'dsrc-demo_redis', '示例Redis', '一个用于演示的Redis', 'demo_redis', 'redis', '{\"host\": \"127.0.0.1\", \"port\": 6379, \"database\": \"15\", \"password\": null}', '0', '2019-12-09 15:33:57', '2020-02-26 00:57:22'),
('4', 'dsrc-demo_clickhouse', '示例ClickHouse', '一个用于演示的ClickHouse', 'demo_clickhouse', 'clickhouse', '{\"host\": \"127.0.0.1\", \"port\": 9000, \"user\": \"default\", \"database\": \"demo\", \"passwordCipher\": \"sfbR0odZIw5oYEudbhc6lw==\"}', '0', '2020-02-25 15:58:55', '2020-02-26 00:57:27');

INSERT INTO `biz_main_func` (`seq`, `id`, `title`, `description`, `scriptSetId`, `scriptId`, `refName`, `definition`, `argsJSON`, `kwargsJSON`, `extraConfigJSON`, `category`, `tagsJSON`, `defOrder`, `createTime`, `updateTime`) VALUES
('1', 'func-CdQEmEhgJjVZ24iAgcT3ZK', '日志处理（按JSON字段提取）', '按照JSON方式处理日志', 'sset-ft_lib', 'scpt-ft_bat_log', 'log_process_json_abstract', 'log_process_json_abstract(log_data, rules)', '[\"log_data\", \"rules\"]', '{\"rules\": {}, \"log_data\": {}}', NULL, 'builtinBatchLog', NULL, '0', '2020-06-29 09:39:41', '2020-06-29 09:39:41.789752'),
('2', 'func-x8Qpr3MHYGVEZucwgmo9s7', '日志处理（按正则分组提取）', '按照正则分组方式处理日志', 'sset-ft_lib', 'scpt-ft_bat_log', 'log_process_regexp_abstract', 'log_process_regexp_abstract(log_data, regexp, rules)', '[\"log_data\", \"regexp\", \"rules\"]', '{\"rules\": {}, \"regexp\": {}, \"log_data\": {}}', '{\"isHidden\": true}', 'builtinBatchLog', NULL, '1', '2020-06-29 09:39:41', '2020-06-29 09:39:41.789752'),
('3', 'func-BEELfaMsyqwAWCmarsGmwj', '日志处理（grok方式提取）', '按照grok方式处理日志', 'sset-ft_lib', 'scpt-ft_bat_log', 'log_process_grok_abstract', 'log_process_grok_abstract(log_data, pattern)', '[\"log_data\", \"pattern\"]', '{\"pattern\": {}, \"log_data\": {}}', NULL, 'builtinBatchLog', NULL, '2', '2020-06-29 09:39:41', '2020-06-29 09:39:41.789752'),
('4', 'func-UYqwk9p3XdNNJJw8fJyTAC', '简单检测', '简单检测函数', 'sset-ft_lib', 'scpt-ft_chk', 'simple_check', 'simple_check(trigger_info, targets, condition_check_setting, no_data_check_setting=None, silent_timeout=None, extra_alert_tags=None)', '[\"trigger_info\", \"targets\", \"condition_check_setting\", \"no_data_check_setting\", \"silent_timeout\", \"extra_alert_tags\"]', '{\"targets\": {}, \"trigger_info\": {}, \"silent_timeout\": {\"default\": null}, \"extra_alert_tags\": {\"default\": null}, \"no_data_check_setting\": {\"default\": null}, \"condition_check_setting\": {}}', '{\"isHidden\": true}', 'builtinCheck', NULL, '0', '2020-06-29 09:39:48', '2020-06-29 09:39:48.258803'),
('5', 'func-fTxhhVj3b6gqnsKTSEyjte', '事件检测', '事件检测函数', 'sset-ft_lib', 'scpt-ft_chk', 'keyevent_check', 'keyevent_check(trigger_info, target, condition_check_setting, silent_timeout=None, extra_alert_tags=None)', '[\"trigger_info\", \"target\", \"condition_check_setting\", \"silent_timeout\", \"extra_alert_tags\"]', '{\"target\": {}, \"trigger_info\": {}, \"silent_timeout\": {\"default\": null}, \"extra_alert_tags\": {\"default\": null}, \"condition_check_setting\": {}}', '{\"isHidden\": true}', 'builtinCheck', NULL, '1', '2020-06-29 09:39:48', '2020-06-29 09:39:48.258803'),
('6', 'func-WmsMjxrBesLPNTFpMoLWUe', '平均值预测', '一个对时序数据进行平均值预测\n参数：\n    dps【必须】: [ , ... ]\n返回：\n    [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_pred', 'avg_prediction', 'avg_prediction(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'prediction', NULL, '0', '2020-06-29 09:39:54', '2020-06-29 09:39:54.931718'),
('7', 'func-kNQM6frnawiKyvRxxTwLLc', '移动均值预测', '一个对时序数据进行平均值预测\n参数：\n    dps【必须】: [ , ... ]\n返回：\n    [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_pred', 'moving_prediction', 'moving_prediction(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'prediction', NULL, '1', '2020-06-29 09:39:54', '2020-06-29 09:39:54.931718'),
('8', 'func-tdsfTorGHBHGqX2DypkbJn', 'Holt函数预测', '一个对时序数据进行平均值预测\n参数：\n    dps【必须】: [ , ... ]\n返回：\n    [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_pred', 'holt_prediction', 'holt_prediction(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'prediction', NULL, '2', '2020-06-29 09:39:54', '2020-06-29 09:39:54.931718'),
('9', 'func-gggLctbayAhMhB72XJEXG6', 'MA函数预测', NULL, 'sset-ft_lib', 'scpt-ft_pred', 'ma_prediction', 'ma_prediction(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'prediction', NULL, '3', '2020-06-29 09:39:54', '2020-06-29 09:39:54.931718'),
('10', 'func-W4avvcyk5PzKQoyPn2dJAR', 'AR函数预测', NULL, 'sset-ft_lib', 'scpt-ft_pred', 'ar_prediction', 'ar_prediction(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'prediction', NULL, '4', '2020-06-29 09:39:54', '2020-06-29 09:39:54.931718'),
('11', 'func-SJ4T2jPvowQGDdayDNbMyP', 'ARIMA函数预测', NULL, 'sset-ft_lib', 'scpt-ft_pred', 'arima_prediction', 'arima_prediction(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'prediction', NULL, '5', '2020-06-29 09:39:54', '2020-06-29 09:39:54.931718'),
('12', 'func-yeJoDBaWFqUN4cXRiRJHhL', 'ABS绝对值转换', '返回数字的绝对值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'abs_transformation', 'abs_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '0', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('13', 'func-PnKuMRJCezk2wPn9a39Xt9', 'AVG平均值转换', '返回数字的平均值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'avg_transformation', 'avg_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '1', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('14', 'func-SqugQfs7eAZrb3Y7HFr7A8', 'MAX最大值转换', '返回数字的最大值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'max_transformation', 'max_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '2', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('15', 'func-BQbZ5hymZyngZQz2bVrgJP', 'MIN最小值转换', '返回数字的最小值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'min_transformation', 'min_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '3', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('16', 'func-mF3Tpm5k2dpSJMJMPH4kQn', 'Even向上取偶数转换', '将数字向上舍入为最接近的偶数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'even_transformation', 'even_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '4', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('17', 'func-xFhi3RpAVn8ADmRigdC3b4', 'Odd向上取奇数转换', '将数字向上舍入为最接近的奇数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'odd_transformation', 'odd_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '5', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('18', 'func-guZeCRgvaGrKCMe3bAavBT', 'Fact取整阶乘转换', '返回数字的阶乘。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'fact_transformation', 'fact_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '6', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('19', 'func-6RcmqQ6LWBzJALR5mN43JX', 'SumSQ平方和转换', '返回数字的平方和。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'sumsq_transformation', 'sumsq_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '7', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('20', 'func-3KfGMDXCiLqtYikoYMqtQT', 'RoundUp绝对值增大取整转换', ' 向绝对值增大的方向舍入数字。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'roundup_transformation', 'roundup_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '8', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('21', 'func-DJ4Z77G3z6Ghjz6tCRMk4d', 'RoundDown绝对值减小取整转换', '  向绝对值减小的方向舍入数字。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'rounddown_transformation', 'rounddown_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '9', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('22', 'func-3W2y4ADKnQMAxyhUmjxcpn', 'Power指数幂转换', '   返回给定次幂的结果。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    significance:指定指数', 'sset-ft_lib', 'scpt-ft_tran', 'power_transformation', 'power_transformation(dps, significance=1)', '[\"dps\", \"significance\"]', '{\"dps\": {}, \"significance\": {\"default\": 1}}', NULL, 'transformation', NULL, '10', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('23', 'func-8UcWPvd4AC35bybJKCdTAH', 'Ceiling取整转换', '将数字四舍五入为最接近的整数或最接近的指定基数的倍数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    significance:指定基数的倍数', 'sset-ft_lib', 'scpt-ft_tran', 'ceiling_transformation', 'ceiling_transformation(dps, significance)', '[\"dps\", \"significance\"]', '{\"dps\": {}, \"significance\": {}}', NULL, 'transformation', NULL, '11', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('24', 'func-mSakGXLnC58wm82y7mDeBB', 'AccumuAll累加转换', '将前n个元素累加求和。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'accumuall_transformation', 'accumuall_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '12', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('25', 'func-ppeJAW8CAQFQBJtvRuxTJG', 'Accumu2求和转换', '将每相邻2个元素累加求和。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'accumu2_transformation', 'accumu2_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '13', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('26', 'func-o7YnpcRJCcTN2tEiBkMZ8F', 'ACOSH反双曲余弦转换', '返回数字的反双曲余弦值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'acosh_transformation', 'acosh_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '14', '2020-06-29 09:40:02', '2020-06-29 09:40:02.634510'),
('27', 'func-SWvP6bzYLCvu2YoCsbvA5U', 'Clean删除不可打印字符', '从文本中删除不可打印的字符，去掉了前31个特殊的ASCII字符。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'clean_transformation', 'clean_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '0', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('28', 'func-CARHseV33FfsjSY8oXZkP4', 'Fixed数字格式化', '将数字格式设置为具有固定小数位数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    decimals:进行四舍五入后小数点后面需要保留的位数,如果是负数，则在小数点左侧进行四舍五入。若此参数                         decimals省略，则默认此参数为2。\n    no_commas:如果是 true，返回的结果不包含逗号千分位。如果是false或省略不写，返回的结果包含逗号千分位。', 'sset-ft_lib', 'scpt-ft_tran_str', 'fixed_transformation', 'fixed_transformation(dps, no_commas=\'True\', decimals=2)', '[\"dps\", \"no_commas\", \"decimals\"]', '{\"dps\": {}, \"decimals\": {\"default\": 2}, \"no_commas\": {\"default\": \"True\"}}', NULL, 'transformation', NULL, '1', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('29', 'func-V7UtvTq2hMpqD4q5m8iCjH', 'Left截取左边字符', '返回文本值中最左边的字符。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    num_chars:需要截取字符的个数。默认截取1个', 'sset-ft_lib', 'scpt-ft_tran_str', 'left_transformation', 'left_transformation(dps, num_chars=1)', '[\"dps\", \"num_chars\"]', '{\"dps\": {}, \"num_chars\": {\"default\": 1}}', NULL, 'transformation', NULL, '2', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('30', 'func-eaYrA9wLeJLZPuvjaB5Tjd', 'Right截取右边字符', '返回文本值中最右边的字符。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    num_chars:需要截取字符的个数。默认截取1个', 'sset-ft_lib', 'scpt-ft_tran_str', 'right_transformation', 'right_transformation(dps, num_chars=None)', '[\"dps\", \"num_chars\"]', '{\"dps\": {}, \"num_chars\": {\"default\": null}}', NULL, 'transformation', NULL, '3', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('31', 'func-svkUj2CTi7mPMQogfGvTBB', 'LEN字符串长度', '返回文本字符串中的字符数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'len_transformation', 'len_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '4', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('32', 'func-XtQ7cZsvRqEtKj34JHHDTc', 'Lower字符小写转换', '将文本转换为小写。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'lower_transformation', 'lower_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '5', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('33', 'func-PSq9c97zudhPzkHhkXdpRi', 'MID截取字符', ' 在文本字符串中,从您所指定的位置开始返回特定数量的字符。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    start_num:从左侧第几位开始截取。\n    num_chars:需要截取字符的个数。默认截取1个。', 'sset-ft_lib', 'scpt-ft_tran_str', 'mid_transformation', 'mid_transformation(dps, start_num=1, num_chars=1)', '[\"dps\", \"start_num\", \"num_chars\"]', '{\"dps\": {}, \"num_chars\": {\"default\": 1}, \"start_num\": {\"default\": 1}}', NULL, 'transformation', NULL, '6', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('34', 'func-G6x4PKF2HUKpBHXR2iCUN6', 'PROPER首字母转大写', '将字符串的首字母转为大写，其余变为小写规范化输出\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    sep【默认是空格】：要转换的文本的切分符', 'sset-ft_lib', 'scpt-ft_tran_str', 'proper_transformation', 'proper_transformation(dps, sep=None)', '[\"dps\", \"sep\"]', '{\"dps\": {}, \"sep\": {\"default\": null}}', NULL, 'transformation', NULL, '7', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('35', 'func-tuyhhfeRrvjPPdtWyeVxjg', 'REPLACE替换目标字符', '替换字符串中的目标字符\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    target：被替换的字符\n    replace:要替换的字符', 'sset-ft_lib', 'scpt-ft_tran_str', 'replace_transformation', 'replace_transformation(dps, target, replace)', '[\"dps\", \"target\", \"replace\"]', '{\"dps\": {}, \"target\": {}, \"replace\": {}}', NULL, 'transformation', NULL, '8', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('36', 'func-NQD39Hb9HNgTojP6orwwg5', 'REPT复制目标文本', '复制目标文本追加到文本末尾\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    target：要复制的字符\n    num:复制次数', 'sset-ft_lib', 'scpt-ft_tran_str', 'rept_transformation', 'rept_transformation(dps, target, num)', '[\"dps\", \"target\", \"num\"]', '{\"dps\": {}, \"num\": {}, \"target\": {}}', NULL, 'transformation', NULL, '9', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('37', 'func-qytKQtGsTHT5TYqQKcv5YS', 'SUBSTITUTE替换文本', '在文本字符串中用新文本替换旧文本\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    replace：进行替换的字符', 'sset-ft_lib', 'scpt-ft_tran_str', 'substitute_transformation', 'substitute_transformation(dps, replace)', '[\"dps\", \"replace\"]', '{\"dps\": {}, \"replace\": {}}', NULL, 'transformation', NULL, '10', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('38', 'func-wG5mztbsDTgn5eTSke5cGa', 'TRIM去除文本空格', '去除文本中空格\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'trim_transformation', 'trim_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '11', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('39', 'func-zcoiFwWcUHsYvA4WYK4BM9', 'UPPER将文本转为大写', '将文本转为大写\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'upper_transformation', 'upper_transformation(dps)', '[\"dps\"]', '{\"dps\": {}}', NULL, 'transformation', NULL, '12', '2020-06-29 09:40:11', '2020-06-29 09:40:11.647866'),
('40', 'func-AwoAYD9FwSLUMRwTNqVzvJ', '水位检测', '水位检测，用于检测连续频繁的异常点，对于偶发的突变点不敏感\n参数如下：\n    1. trigger_info {dict} 异常检测规则（以前叫触发器）的基本信息，结构如下：\n        {\n          \"uuid\"          : \"str, <UUID> 即规则ID\",\n          \"name\"          : \"str, <名称> 即规则名称\",\n          \"workspaceToken\": \"str, <工作空间令牌> 用于判断出事件之后写入__keyevent\",\n          \"dimensions\"    : \"<触发维度> list类型，元素为str\"\n        }\n\n    2. targets {list} 检测的目标，结构如下：\n        [\n          {\n            \"alias\"     : \"str, <别名> 默认即M1, M2 ...，用户可以改为其他内容，用于各类告警文案、数据填充\",\n            \"influxQL\"  : \"str, <InfluxDB 查询语句>\", 注：实际查询时需 替换占位符（$now) 为当前的时刻;\n                            注：influxQL里，where的时间窗里面，取 $now - （聚合周期个数+100）* 聚合周期 到 $now 的数据\",\n            \"qtype\"     : \"str, 如http，注：该字段内容算法用不上，只是当写入keyevennt时原封不动一并传入\",\n            \"query\"     : {\"注：该字段内容算法用不上，只是当写入keyevennt时一并传入，目的是使前端可以拉取keyevent同时也能根                                        据query获取原始的时序数据。该字段结构由后端和前端一起定义\"}\n          }\n        ]\n\n    3. condition_check_setting {dict} 异常检测的触发条件配置，结构如下，分为检测规则和告警动作：\n        {\n          \"rules\": [\n          {\n            \"triggerCondition\": {\n                \"periodNum\": \"int, 检测几个聚合周期，实际就是检测最近的多少个点\",\n                \"strength\": \"int, 检测的强度，目前页面上可选强，中，弱，传过来是数字3，2，1，强度越高代表异常的判定越苛刻\",\n                \"direction\": \"str, 检测的方向, 页面上三个单选项对应的是 up, down, both\",\n                \"checkCount\": \"int, 当连续的异常点数>=该值时，触发事件\"\n            },\n            \"level\": \"str, 异常级别，目前默认 warning，写死\"\n        }],\n          \"actions\": [\n            {\n              \"type\"   : \"mail\",\n              \"to\"     : [\"邮件地址1\", \"邮件地址2\"],\n              \"title\"  : \"<邮件标题>\",\n              \"content\": \"<邮件内容>\"\n            },\n            {\n              \"type\"    : \"dingTalkRobot\",\n              \"webhook\" : [\"钉钉机器人调用地址1\", \"钉钉机器人调用地址2\"],\n              \"title\"   : \"<通知标题>\",\n              \"markdown\": \"<MarkDown格式通知内容>\"\n            },\n            {\n              \"type\"   : \"ProfWangAlertHub\",\n              \"url\"    : [\"王教授AlertHub调用地址1\", \"王教授AlertHub调用地址2\"],\n              \"title\"  : \"<情报标题>\",\n              \"content\": \"<情报内容>\"\n            },\n            {\n              \"type\"    : \"HTTPRequest\",\n              \"url\"     : [\"请求地址1\", \"请求地址2\"],\n              \"method\"  : \"<请求方式> GET|POST\",\n              \"headers\" : {\"Header1\": \"HeaderValue1\", \"Header2\": \"HeaderValue2\"},\n              \"body\"    : \"<method=POST 请求体>\",\n              \"bodyType\": \"<method=POST 请求体类型> text|json|form\"\n            },\n            {\n              \"type\"            : \"DataFluxFunc\",\n              \"scriptSetRefName\": \"<脚本集引用名>\",\n              \"scriptRefName\"   : \"<脚本引用名>\",\n              \"funcRefName\"     : \"<函数名>\",\n              \"kwargs\"          : { \"<调用参数名>\": \"<调用参数值>\" }\n            }\n          ]\n        }\n\n    4. all_time:  \"dict, 检测频率，数据聚合的周期\"\n        {\n            \"every_time\": \"str, 每隔多久检测一次\",\n            \"group_time\": \"str, 时间聚合周期\",\n        }\n\n    5. extra_alert_tags: {dict} 额外添加的告警数据标签\n        {\n            \"extra_alert_tags\": \"str, 异常\"\n        }', 'sset-ft_lib', 'scpt-level_shift_chk', 'level_shift_check', 'level_shift_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None)', '[\"trigger_info\", \"targets\", \"condition_check_setting\", \"all_time\", \"extra_alert_tags\"]', '{\"targets\": {}, \"all_time\": {}, \"trigger_info\": {}, \"extra_alert_tags\": {\"default\": null}, \"condition_check_setting\": {}}', '{\"isHidden\": true}', 'builtinCheck', NULL, '0', '2020-06-29 09:40:19', '2020-06-29 09:40:19.038622'),
('41', 'func-49ZGdAMujPqSdzCvn4YyjR', '区间检测', '区间检测，利用历史数据预测出数据点的上下区间，超出区间外的为异常点\n参数如下：\n    1. trigger_info {dict} 异常检测规则（以前叫触发器）的基本信息，结构如下：\n        {\n          \"uuid\"          : \"str, <UUID> 即规则ID\",\n          \"name\"          : \"str, <名称> 即规则名称\",\n          \"workspaceToken\": \"str, <工作空间令牌> 用于判断出事件之后写入__keyevent\",\n          \"dimensions\"    : \"<触发维度> list类型，元素为str\"\n        }\n\n    2. targets {list} 检测的目标，结构如下：\n        [\n          {\n            \"alias\"     : \"str, <别名> 默认即M1, M2 ...，用户可以改为其他内容，用于各类告警文案、数据填充\",\n            \"influxQL\"  : \"str, <InfluxDB 查询语句>\", 注：实际查询时需 替换占位符（$now) 为当前的时刻;\n                            注：influxQL里，where的时间窗里面，取 $now - （聚合周期个数+100）* 聚合周期 到 $now 的数据\",\n            \"qtype\"     : \"str, 如http，注：该字段内容算法用不上，只是当写入keyevennt时原封不动一并传入\",\n            \"query\"     : {\"注：该字段内容算法用不上，只是当写入keyevennt时一并传入，目的是使前端可以拉取keyevent同时也能根                                        据query获取原始的时序数据。该字段结构由后端和前端一起定义\"}\n          }\n        ]\n\n    3. condition_check_setting {dict} 异常检测的触发条件配置，结构如下，分为检测规则和告警动作：\n        {\n          \"rules\": [\n          {\n            \"triggerCondition\": {\n                \"periodNum\": \"int, 检测几个聚合周期\",\n                \"strength\": \"int, 检测的强度，目前页面上可选强，中，弱，传过来是数字3，2，1，强度越高代表异常的判定越苛刻\",\n                \"direction\": \"str, 检测的方向, 页面上三个单选项对应的是 up, down, both\",\n                \"checkPercent\": \"int，1-100，当异常点数百分比 >= 该值时，触发事件\"\n            },\n            \"level\": \"str, 异常级别，目前默认 warning，写死\"\n        }],\n          \"actions\": [\n            {\n              \"type\"   : \"mail\",\n              \"to\"     : [\"邮件地址1\", \"邮件地址2\"],\n              \"title\"  : \"<邮件标题>\",\n              \"content\": \"<邮件内容>\"\n            },\n            {\n              \"type\"    : \"dingTalkRobot\",\n              \"webhook\" : [\"钉钉机器人调用地址1\", \"钉钉机器人调用地址2\"],\n              \"title\"   : \"<通知标题>\",\n              \"markdown\": \"<MarkDown格式通知内容>\"\n            },\n            {\n              \"type\"   : \"ProfWangAlertHub\",\n              \"url\"    : [\"王教授AlertHub调用地址1\", \"王教授AlertHub调用地址2\"],\n              \"title\"  : \"<情报标题>\",\n              \"content\": \"<情报内容>\"\n            },\n            {\n              \"type\"    : \"HTTPRequest\",\n              \"url\"     : [\"请求地址1\", \"请求地址2\"],\n              \"method\"  : \"<请求方式> GET|POST\",\n              \"headers\" : {\"Header1\": \"HeaderValue1\", \"Header2\": \"HeaderValue2\"},\n              \"body\"    : \"<method=POST 请求体>\",\n              \"bodyType\": \"<method=POST 请求体类型> text|json|form\"\n            },\n            {\n              \"type\"            : \"DataFluxFunc\",\n              \"scriptSetRefName\": \"<脚本集引用名>\",\n              \"scriptRefName\"   : \"<脚本引用名>\",\n              \"funcRefName\"     : \"<函数名>\",\n              \"kwargs\"          : { \"<调用参数名>\": \"<调用参数值>\" }\n            }\n          ]\n        }\n\n    4. all_time:  \"dict, 检测频率，数据聚合的周期\"\n        {\n            \"every_time\": \"str, 每隔多久检测一次\",\n            \"group_time\": \"str, 时间聚合周期\",\n        }\n\n    5. extra_alert_tags: {dict} 额外添加的告警数据标签\n        {\n            \"extra_alert_tags\": \"str, 异常\"\n        }', 'sset-ft_lib', 'scpt-range_chk', 'range_check', 'range_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None)', '[\"trigger_info\", \"targets\", \"condition_check_setting\", \"all_time\", \"extra_alert_tags\"]', '{\"targets\": {}, \"all_time\": {}, \"trigger_info\": {}, \"extra_alert_tags\": {\"default\": null}, \"condition_check_setting\": {}}', '{\"isHidden\": true}', 'builtinCheck', NULL, '0', '2020-06-29 09:40:26', '2020-06-29 09:40:26.596715'),
('42', 'func-gxo85CiMfNJxxP3Lzh33kh', '突变检测', '突变检测，检测突然上升或者下降的点\n参数如下：\n    1. trigger_info {dict} 异常检测规则（以前叫触发器）的基本信息，结构如下：\n        {\n          \"uuid\"          : \"str, <UUID> 即规则ID\",\n          \"name\"          : \"str, <名称> 即规则名称\",\n          \"workspaceToken\": \"str, <工作空间令牌> 用于判断出事件之后写入__keyevent\",\n          \"dimensions\"    : \"<触发维度> list类型，元素为str\"\n        }\n\n    2. targets {list} 检测的目标，结构如下：\n        [\n          {\n            \"alias\"     : \"str, <别名> 默认即M1, M2 ...，用户可以改为其他内容，用于各类告警文案、数据填充\",\n            \"influxQL\"  : \"str, <InfluxDB 查询语句>\", 注：实际查询时需 替换占位符（$now) 为当前的时刻;\n                            注：influxQL里，where的时间窗里面，取 $now - （聚合周期个数+100）* 聚合周期 到 $now 的数据\",\n            \"qtype\"     : \"str, 如http，注：该字段内容算法用不上，只是当写入keyevennt时原封不动一并传入\",\n            \"query\"     : {\"注：该字段内容算法用不上，只是当写入keyevennt时一并传入，目的是使前端可以拉取keyevent同时也能根                                        据query获取原始的时序数据。该字段结构由后端和前端一起定义\"}\n          }\n        ]\n\n    3. condition_check_setting {dict} 异常检测的触发条件配置，结构如下，分为检测规则和告警动作：\n        {\n          \"rules\": [\n          {\n            \"triggerCondition\": {\n                \"periodNum\": \"int, 检测几个聚合周期\",\n                \"strength\": \"int, 检测的强度，目前页面上可选强，中，弱，传过来是数字3，2，1，强度越高代表异常的判定越苛刻\",\n                \"direction\": \"str, 检测的方向, 页面上三个单选项对应的是 up, down, both\",\n                \"checkCount\": \"int, 当异常点数>=该值时，触发事件\"\n            },\n            \"level\": \"str, 异常级别，目前默认 warning，写死\"\n        }],\n          \"actions\": [\n            {\n              \"type\"   : \"mail\",\n              \"to\"     : [\"邮件地址1\", \"邮件地址2\"],\n              \"title\"  : \"<邮件标题>\",\n              \"content\": \"<邮件内容>\"\n            },\n            {\n              \"type\"    : \"dingTalkRobot\",\n              \"webhook\" : [\"钉钉机器人调用地址1\", \"钉钉机器人调用地址2\"],\n              \"title\"   : \"<通知标题>\",\n              \"markdown\": \"<MarkDown格式通知内容>\"\n            },\n            {\n              \"type\"   : \"ProfWangAlertHub\",\n              \"url\"    : [\"王教授AlertHub调用地址1\", \"王教授AlertHub调用地址2\"],\n              \"title\"  : \"<情报标题>\",\n              \"content\": \"<情报内容>\"\n            },\n            {\n              \"type\"    : \"HTTPRequest\",\n              \"url\"     : [\"请求地址1\", \"请求地址2\"],\n              \"method\"  : \"<请求方式> GET|POST\",\n              \"headers\" : {\"Header1\": \"HeaderValue1\", \"Header2\": \"HeaderValue2\"},\n              \"body\"    : \"<method=POST 请求体>\",\n              \"bodyType\": \"<method=POST 请求体类型> text|json|form\"\n            },\n            {\n              \"type\"            : \"DataFluxFunc\",\n              \"scriptSetRefName\": \"<脚本集引用名>\",\n              \"scriptRefName\"   : \"<脚本引用名>\",\n              \"funcRefName\"     : \"<函数名>\",\n              \"kwargs\"          : { \"<调用参数名>\": \"<调用参数值>\" }\n            }\n          ]\n        }\n\n    4. all_time:  \"dict, 检测频率，数据聚合的周期\"\n        {\n            \"every_time\": \"str, 每隔多久检测一次\",\n            \"group_time\": \"str, 时间聚合周期\",\n        }\n\n    5. extra_alert_tags: {dict} 额外添加的告警数据标签\n        {\n            \"extra_alert_tags\": \"str, 异常\"\n        }', 'sset-ft_lib', 'scpt-spike_chk', 'spike_check', 'spike_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None)', '[\"trigger_info\", \"targets\", \"condition_check_setting\", \"all_time\", \"extra_alert_tags\"]', '{\"targets\": {}, \"all_time\": {}, \"trigger_info\": {}, \"extra_alert_tags\": {\"default\": null}, \"condition_check_setting\": {}}', '{\"isHidden\": true}', 'builtinCheck', NULL, '0', '2020-06-29 09:40:33', '2020-06-29 09:40:33.276618'),
('43', 'func-nWTzXJzPtpVJEY9QiPL4y3', 'Hello, world!', '一个最简单的示例，直接返回\"Hello, world!\"字符串', 'sset-demo', 'scpt-demo', 'hello_world', 'hello_world()', '[]', '{}', '{\"cacheResult\": 300}', 'general', NULL, '0', '2020-06-30 19:28:14', '2020-06-30 19:28:14.851234'),
('44', 'func-kBQ2ANTg79pehd6qCjHdEE', '问候', '一个简单的带参数示例，返回JSON', 'sset-demo', 'scpt-demo', 'greeting', 'greeting(your_name)', '[\"your_name\"]', '{\"your_name\": {}}', '{\"cacheResult\": 300}', 'general', NULL, '1', '2020-06-30 19:28:14', '2020-06-30 19:28:14.851234'),
('45', 'func-res7t9X5WR3nLiMhk35RWC', 'InfluxDB操作演示', '本函数从数据源`demo_influxdb`中查询3条`demo`指标并返回', 'sset-demo', 'scpt-demo', 'influxdb_demo', 'influxdb_demo()', '[]', '{}', NULL, 'general', NULL, '2', '2020-06-30 19:28:14', '2020-06-30 19:28:14.851234'),
('46', 'func-kiFSXSPnqdbTjkNMABzKe', 'MySQL操作演示', '本函数从数据源`demo_mysql`中查询`demo`表数据并返回', 'sset-demo', 'scpt-demo', 'mysql_demo', 'mysql_demo()', '[]', '{}', NULL, 'general', NULL, '3', '2020-06-30 19:28:14', '2020-06-30 19:28:14.851234'),
('47', 'func-VFhYxCbCLo4xEBLrUGGV8g', 'Redis操作演示', '本函数对数据源`demo_redis`中的`demo`键进行加一计数\n并返回当前`demo`键的值', 'sset-demo', 'scpt-demo', 'redis_demo', 'redis_demo()', '[]', '{}', NULL, 'general', NULL, '4', '2020-06-30 19:28:14', '2020-06-30 19:28:14.851234'),
('48', 'func-BtEZPB5xQGFpEvZAnpqeZY', 'ClickHouse操作演示', '本函数从数据源`demo_clickhouse`中查询`demo_table`表数据并返回', 'sset-demo', 'scpt-demo', 'clickhouse_demo', 'clickhouse_demo()', '[]', '{}', NULL, 'general', NULL, '5', '2020-06-30 19:28:14', '2020-06-30 19:28:14.851234'),
('49', 'func-Q4gKsywduZBWgrkV7eQiJL', '内置DataFlux DataWay操作演示', '本函数对数据源`df_dataway`添加一个数据点，并返回是否成功', 'sset-demo', 'scpt-demo', 'df_dataway_demo', 'df_dataway_demo()', '[]', '{}', NULL, 'general', NULL, '6', '2020-06-30 19:28:14', '2020-06-30 19:28:14.851234');

INSERT INTO `biz_main_script` (`seq`, `id`, `title`, `description`, `scriptSetId`, `refName`, `publishVersion`, `type`, `code`, `codeMD5`, `codeDraft`, `codeDraftMD5`, `createTime`, `updateTime`) VALUES
('1', 'scpt-demo', '示例代码', '主要包含了一些用于展示DataFlux.f(x) 功能的代码，可以删除', 'sset-demo', 'demo', '1', 'python', '@DFF.API(\'Hello, world!\', cache_result=300)\ndef hello_world():\n    \'\'\'\n    一个最简单的示例，直接返回\"Hello, world!\"字符串\n    \'\'\'\n    return \'Hello, world!\'\n\n@DFF.API(\'问候\', cache_result=300)\ndef greeting(your_name):\n    \'\'\'\n    一个简单的带参数示例，返回JSON\n    \'\'\'\n    ret = {\n        \'message\': \'Hello! {}\'.format(your_name)\n    }\n    return ret\n\n@DFF.API(\'InfluxDB操作演示\')\ndef influxdb_demo():\n    \'\'\'\n    本函数从数据源`demo_influxdb`中查询3条`demo`指标并返回\n    \'\'\'\n    db = DFF.SRC(\'demo_influxdb\')\n    return db.query(\'SELECT * FROM demo LIMIT 3\')\n\n@DFF.API(\'MySQL操作演示\')\ndef mysql_demo():\n    \'\'\'\n    本函数从数据源`demo_mysql`中查询`demo`表数据并返回\n    \'\'\'\n    helper = DFF.SRC(\'demo_mysql\')\n    return helper.query(\'SELECT * FROM demo\')\n\n@DFF.API(\'Redis操作演示\')\ndef redis_demo():\n    \'\'\'\n    本函数对数据源`demo_redis`中的`demo`键进行加一计数\n    并返回当前`demo`键的值\n    \'\'\'\n    helper = DFF.SRC(\'demo_redis\')\n    helper.run(\'incr\', \'demo\')\n    return helper.run(\'get\', \'demo\')\n\n@DFF.API(\'ClickHouse操作演示\')\ndef clickhouse_demo():\n    \'\'\'\n    本函数从数据源`demo_clickhouse`中查询`demo_table`表数据并返回\n    \'\'\'\n    helper = DFF.SRC(\'demo_clickhouse\')\n    return helper.query(\'SELECT * FROM demo_table\')\n\n@DFF.API(\'内置DataFlux DataWay操作演示\')\ndef df_dataway_demo():\n    \'\'\'\n    本函数对数据源`df_dataway`添加一个数据点，并返回是否成功\n    \'\'\'\n    helper = DFF.SRC(\'df_dataway\')\n    return helper.write_point(\n        measurement=\'some_measurement\',\n        tags={\'name\': \'Tom\'},\n        fields={\'value\': 10},\n        timestamp=None)', '5cd6e2f732e45afbc8fc92659735111b', '@DFF.API(\'Hello, world!\', cache_result=300)\ndef hello_world():\n    \'\'\'\n    一个最简单的示例，直接返回\"Hello, world!\"字符串\n    \'\'\'\n    return \'Hello, world!\'\n\n@DFF.API(\'问候\', cache_result=300)\ndef greeting(your_name):\n    \'\'\'\n    一个简单的带参数示例，返回JSON\n    \'\'\'\n    ret = {\n        \'message\': \'Hello! {}\'.format(your_name)\n    }\n    return ret\n\n@DFF.API(\'InfluxDB操作演示\')\ndef influxdb_demo():\n    \'\'\'\n    本函数从数据源`demo_influxdb`中查询3条`demo`指标并返回\n    \'\'\'\n    db = DFF.SRC(\'demo_influxdb\')\n    return db.query(\'SELECT * FROM demo LIMIT 3\')\n\n@DFF.API(\'MySQL操作演示\')\ndef mysql_demo():\n    \'\'\'\n    本函数从数据源`demo_mysql`中查询`demo`表数据并返回\n    \'\'\'\n    helper = DFF.SRC(\'demo_mysql\')\n    return helper.query(\'SELECT * FROM demo\')\n\n@DFF.API(\'Redis操作演示\')\ndef redis_demo():\n    \'\'\'\n    本函数对数据源`demo_redis`中的`demo`键进行加一计数\n    并返回当前`demo`键的值\n    \'\'\'\n    helper = DFF.SRC(\'demo_redis\')\n    helper.run(\'incr\', \'demo\')\n    return helper.run(\'get\', \'demo\')\n\n@DFF.API(\'ClickHouse操作演示\')\ndef clickhouse_demo():\n    \'\'\'\n    本函数从数据源`demo_clickhouse`中查询`demo_table`表数据并返回\n    \'\'\'\n    helper = DFF.SRC(\'demo_clickhouse\')\n    return helper.query(\'SELECT * FROM demo_table\')\n\n@DFF.API(\'内置DataFlux DataWay操作演示\')\ndef df_dataway_demo():\n    \'\'\'\n    本函数对数据源`df_dataway`添加一个数据点，并返回是否成功\n    \'\'\'\n    helper = DFF.SRC(\'df_dataway\')\n    return helper.write_point(\n        measurement=\'some_measurement\',\n        tags={\'name\': \'Tom\'},\n        fields={\'value\': 10},\n        timestamp=None)', '5cd6e2f732e45afbc8fc92659735111b', '2019-12-09 15:25:22', '2020-06-30 19:28:52'),
('2', 'scpt-ft_bat_log', 'FT批处理函数-日志', '批处理函数脚本-日志处理', 'sset-ft_lib', 'ft_bat_log', '1', 'python', '\'\'\'\n注意！\n本脚本由驻云官方提供，且在脚本中大量使用了DataFlux.f(x)的内部接口。\n这些内部接口可能会跟随系统更新随时改变，请勿在您的用户脚本中使用！\n\'\'\'\n\nimport time\nimport re\nimport simplejson as json\nimport jsonpath_ng\nimport pygrok\nimport time\nimport arrow\nimport dateparser\n\n_DFF_is_debug = True\n\ndef _jsonpath_value_dumps(matched):\n    values = []\n    for m in matched:\n        if isinstance(m.value, (list, tuple, dict)):\n            v = json.dumps(m.value, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n        else:\n            v = str(m.value)\n\n        values.append(v)\n\n    return \',\'.join(values)\n\ndef _overwrite_fields(log_data):\n    try:\n        points = log_data[\'points\']\n    except Exception as e:\n        return\n\n    for p in points:\n        try:\n            tags = p.get(\'__tags\')\n            if not tags:\n                continue\n\n            timestamp_us = tags.pop(\'__timestampUs\', None)\n            if not timestamp_us:\n                continue\n\n            parsed_datetime = dateparser.parse(timestamp_us)\n            p[\'__timestampUs\'] = arrow.get(parsed_datetime).timestamp * 1000000\n\n        except Exception as e:\n            continue\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'rules\'   : { \'type\': \'list\', \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（按JSON字段提取）\', category=\'builtinBatchLog\')\ndef log_process_json_abstract(log_data, rules):\n    \'\'\'\n    按照JSON方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        jsonpath_obj_map = {}\n        for r in rules:\n            alias     = r[\'alias\']\n            json_path = r[\'jsonPath\']\n            jsonpath_obj_map[alias] = jsonpath_ng.parse(json_path)\n\n        for p in points:\n            try:\n                target = json.loads(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n\n            for r in rules:\n                alias = r[\'alias\']\n\n                abstracted_value = None\n                try:\n                    matched = jsonpath_obj_map[alias].find(target)\n                    abstracted_value = _jsonpath_value_dumps(matched)\n\n                except Exception as e:\n                    continue\n\n                if abstracted_value:\n                    alias = r[\'alias\']\n                    p[\'__tags\'] = p.get(\'__tags\') or {}\n                    p[\'__tags\'][alias] = abstracted_value\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'regexp\'  : { \'type\': \'str\',  \'raiseTypeError\': True },\n    \'rules\'   : { \'type\': \'list\', \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（按正则分组提取）\', category=\'builtinBatchLog\', is_hidden=True)\ndef log_process_regexp_abstract(log_data, regexp, rules):\n    \'\'\'\n    按照正则分组方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        regexp = re.compile(regexp)\n\n        for p in points:\n            try:\n                m = regexp.search(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n            else:\n                if not m:\n                    continue\n\n            for r in rules:\n                abstracted_value = None\n                try:\n                    group_index = int(r[\'groupIndex\'].replace(\'$\', \'\'))\n                    abstracted_value = m.group(group_index)\n                except Exception as e:\n                    continue\n\n                if abstracted_value:\n                    alias = r[\'alias\']\n                    p[\'__tags\'] = p.get(\'__tags\') or {}\n                    p[\'__tags\'][alias] = abstracted_value\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'pattern\' : { \'type\': \'str\',  \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（grok方式提取）\', category=\'builtinBatchLog\')\ndef log_process_grok_abstract(log_data, pattern):\n    \'\'\'\n    按照grok方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        grok   = pygrok.Grok(pattern)\n\n        for p in points:\n            try:\n                m = grok.match(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n            else:\n                if not m:\n                    continue\n\n            p[\'__tags\'] = p.get(\'__tags\') or {}\n            p[\'__tags\'].update(m)\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n', '3941c69e26a7df5e7267a1412e709cbc', '\'\'\'\n注意！\n本脚本由驻云官方提供，且在脚本中大量使用了DataFlux.f(x)的内部接口。\n这些内部接口可能会跟随系统更新随时改变，请勿在您的用户脚本中使用！\n\'\'\'\n\nimport time\nimport re\nimport simplejson as json\nimport jsonpath_ng\nimport pygrok\nimport time\nimport arrow\nimport dateparser\n\n_DFF_is_debug = True\n\ndef _jsonpath_value_dumps(matched):\n    values = []\n    for m in matched:\n        if isinstance(m.value, (list, tuple, dict)):\n            v = json.dumps(m.value, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n        else:\n            v = str(m.value)\n\n        values.append(v)\n\n    return \',\'.join(values)\n\ndef _overwrite_fields(log_data):\n    try:\n        points = log_data[\'points\']\n    except Exception as e:\n        return\n\n    for p in points:\n        try:\n            tags = p.get(\'__tags\')\n            if not tags:\n                continue\n\n            timestamp_us = tags.pop(\'__timestampUs\', None)\n            if not timestamp_us:\n                continue\n\n            parsed_datetime = dateparser.parse(timestamp_us)\n            p[\'__timestampUs\'] = arrow.get(parsed_datetime).timestamp * 1000000\n\n        except Exception as e:\n            continue\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'rules\'   : { \'type\': \'list\', \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（按JSON字段提取）\', category=\'builtinBatchLog\')\ndef log_process_json_abstract(log_data, rules):\n    \'\'\'\n    按照JSON方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        jsonpath_obj_map = {}\n        for r in rules:\n            alias     = r[\'alias\']\n            json_path = r[\'jsonPath\']\n            jsonpath_obj_map[alias] = jsonpath_ng.parse(json_path)\n\n        for p in points:\n            try:\n                target = json.loads(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n\n            for r in rules:\n                alias = r[\'alias\']\n\n                abstracted_value = None\n                try:\n                    matched = jsonpath_obj_map[alias].find(target)\n                    abstracted_value = _jsonpath_value_dumps(matched)\n\n                except Exception as e:\n                    continue\n\n                if abstracted_value:\n                    alias = r[\'alias\']\n                    p[\'__tags\'] = p.get(\'__tags\') or {}\n                    p[\'__tags\'][alias] = abstracted_value\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'regexp\'  : { \'type\': \'str\',  \'raiseTypeError\': True },\n    \'rules\'   : { \'type\': \'list\', \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（按正则分组提取）\', category=\'builtinBatchLog\', is_hidden=True)\ndef log_process_regexp_abstract(log_data, regexp, rules):\n    \'\'\'\n    按照正则分组方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        regexp = re.compile(regexp)\n\n        for p in points:\n            try:\n                m = regexp.search(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n            else:\n                if not m:\n                    continue\n\n            for r in rules:\n                abstracted_value = None\n                try:\n                    group_index = int(r[\'groupIndex\'].replace(\'$\', \'\'))\n                    abstracted_value = m.group(group_index)\n                except Exception as e:\n                    continue\n\n                if abstracted_value:\n                    alias = r[\'alias\']\n                    p[\'__tags\'] = p.get(\'__tags\') or {}\n                    p[\'__tags\'][alias] = abstracted_value\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'pattern\' : { \'type\': \'str\',  \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（grok方式提取）\', category=\'builtinBatchLog\')\ndef log_process_grok_abstract(log_data, pattern):\n    \'\'\'\n    按照grok方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        grok   = pygrok.Grok(pattern)\n\n        for p in points:\n            try:\n                m = grok.match(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n            else:\n                if not m:\n                    continue\n\n            p[\'__tags\'] = p.get(\'__tags\') or {}\n            p[\'__tags\'].update(m)\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n', '3941c69e26a7df5e7267a1412e709cbc', '2020-06-29 09:37:40', '2020-06-29 09:41:07'),
('3', 'scpt-ft_chk', 'FT检测函数', '检测函数脚本', 'sset-ft_lib', 'ft_chk', '1', 'python', '\'\'\'\n注意！\n本脚本由驻云官方提供，且在脚本中大量使用了DataFlux.f(x)的内部接口。\n这些内部接口可能会跟随系统更新随时改变，请勿在您的用户脚本中使用！\n\n2020-04-18 优化了动作执行速度\n2020-05-21 新增进阶模板语法，支持指标间运算\n2020-05-25 根据最新数据结构，$alert合并进$keyevent\n2020-06-09 $keyevent改为__keyevent（即相关字段同步修改）\n\'\'\'\n\nimport hashlib\nimport re\nimport time\n\nimport arrow\nimport simplejson as json\nimport six\n\n_DFF_is_debug = True\n\nMOCK_DATA               = None\nRAISE_DATAWAY_EXCEPTION = True\n\nALL_LEVELS          = (\'critical\', \'error\', \'warning\', \'info\')\nOK_LEVEL            = \'ok\'\nLEVEL_NAME_MAP      = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nCONDITION_LOGICS    = (\'and\', \'or\')\nCONDITION_OPERATORS = (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nACTION_TYPES        = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS        = (\'GET\', \'POST\')\nHTTP_BODY_TYPES     = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\ndef _set_mock_data(mock_data):\n    global MOCK_DATA\n    MOCK_DATA = mock_data\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n\n    return six.ensure_str(s)\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\ndef _to_cn_time(d=None):\n    return arrow.get(d).to(\'Asia/Shanghai\').format(\'YYYY-MM-DD HH:mm:ss\')\n\ndef _to_iso_time(d=None):\n    return arrow.get(d).isoformat()\n\ndef _get_store_scope(trigger_uuid):\n    return \'builtinCheck.simpleCheck@{}\'.format(_get_md5(trigger_uuid))\n\ndef _get_alert_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    if _DFF_is_debug:\n        print(\'【告警对象】{}\'.format(str_to_md5))\n    return _get_md5(str_to_md5)\n\ndef _get_alert_store(trigger_uuid, key, alert_id):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.GET(store_key, scope=store_scope)\n\ndef _set_alert_store(trigger_uuid, key, alert_id, value, only_not_exists=False):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.SET(store_key, value=value, scope=store_scope, only_not_exists=only_not_exists)\n\ndef _delete_alert_store(trigger_uuid, key, alert_id):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.DEL(store_key, scope=store_scope)\n\ndef _check_if_alert_level_changed(trigger_uuid, alert_id, level):\n    prev_level = _get_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id) or OK_LEVEL\n    is_level_changed = prev_level != level\n\n    if _DFF_is_debug:\n        print(\'上次检测结果：{}，本次检测结果：{}\'.format(prev_level, level))\n\n    return is_level_changed, prev_level\n\ndef _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed=None, silent_timeout=None):\n    if is_level_changed:\n        # 级别发生改变一定触发动作\n        if _DFF_is_debug:\n            print(\'检测结果改变，一定触发动作。当前检测结果：{}\'.format(level))\n\n        return True\n\n    else:\n        if _DFF_is_debug:\n            print(\'检测结果不变，当前检测结果：{}\'.format(level))\n\n        if level != OK_LEVEL and silent_timeout is not None:\n            # 级别未发生改变时，且保持非`ok`级别，则到达超时时间才触发动作\n            alert_report_time = _get_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id) or 0\n            past_time = _DFF_start_time - alert_report_time\n            should_do_action = past_time > silent_timeout\n\n            if _DFF_is_debug:\n                print(\'距离上次告警经过：{}s\'.format(past_time))\n                if should_do_action:\n                    print(\'由于距离上次告警时间（{}s）已超过沉默时长（{}s），因此需要触发动作\'.format(past_time, silent_timeout))\n                else:\n                    print(\'由于距离上次告警时间（{}s）尚未超过沉默时长（{}s），因此不需要触发动作\'.format(past_time, silent_timeout))\n\n            return should_do_action\n\n    if _DFF_is_debug:\n        print(\'当前检测结果：{}，不需要触发动作\'.format(level))\n\n    return False\n\ndef _update_no_data_duration(trigger_uuid, alert_id, is_no_data):\n    prev_no_data_time = _get_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id)\n    if _DFF_is_debug:\n        if prev_no_data_time:\n            print(\'无数据开始时间：{}\'.format(_to_cn_time(prev_no_data_time)))\n        else:\n            print(\'首次处理无数据告警\')\n\n    if is_no_data:\n        if prev_no_data_time:\n            if _DFF_is_debug:\n                print(\'已经记录过无数据时间，继续...\')\n\n            return _DFF_start_time - prev_no_data_time\n\n        else:\n            _set_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id, value=_DFF_start_time)\n\n            if _DFF_is_debug:\n                print(\'记录无数据时间\')\n\n            return 0\n\n    else:\n        if prev_no_data_time:\n            _delete_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id)\n\n            if _DFF_is_debug:\n                print(\'清除无数据时间\')\n\n            return _DFF_start_time - prev_no_data_time\n\n        else:\n            if _DFF_is_debug:\n                print(\'数据正常，无需处理\')\n\n            return 0\n\ndef _get_alert_duration(trigger_uuid, alert_id):\n    alert_level_change_time = _get_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id) or _DFF_start_time\n\n    if _DFF_is_debug:\n        print(\'告警级别变化时间：{}\'.format(_to_cn_time(alert_level_change_time)))\n\n    return _DFF_start_time - alert_level_change_time\n\ndef _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action):\n    if level == OK_LEVEL:\n        _delete_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id)\n        _delete_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id)\n        _delete_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id)\n\n    else:\n        if is_level_changed:\n            _set_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id, value=_DFF_start_time)\n            _set_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id, value=level)\n\n        if should_do_action:\n            _set_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id, value=_DFF_start_time)\n\ndef _get_check_value(series_data):\n    # 获取非`time`列的索引号\n    for index, name in enumerate(series_data[\'columns\']):\n        if name == \'time\':\n            continue\n\n        return series_data[\'values\'][0][index]\n\n    return None\n\ndef _get_alert_item_tags_no_data(target_alias):\n    alert_item_tags = {\n        \'noData\': target_alias\n    }\n    return alert_item_tags\n\ndef _get_alert_item_tags_json(alert_item_tags=None):\n    return _json_dumps(alert_item_tags or {})\n\ndef _from_alert_item_tags_json(alert_item_tags_json):\n    return json.loads(alert_item_tags_json)\n\ndef __do_condition_math(check_value, operator, operand):\n    if isinstance(operand, list):\n        operand = operand[0]\n\n    try:\n        T = type(check_value)\n        operand = T(operand)\n\n        if operator == \'==\':\n            return check_value == operand\n        elif operator == \'!=\':\n            return check_value != operand\n        elif operator == \'>\':\n            return check_value > operand\n        elif operator == \'>=\':\n            return check_value >= operand\n        elif operator == \'<\':\n            return check_value < operand\n        elif operator == \'<=\':\n            return check_value <= operand\n\n    except Exception as e:\n        print(e)\n        return False\n\ndef _do_condition_eq(check_value, operand):\n    return __do_condition_math(check_value, \'==\', operand[0])\n\ndef _do_condition_ne(check_value, operand):\n    return __do_condition_math(check_value, \'!=\', operand[0])\n\ndef _do_condition_gt(check_value, operand):\n    return __do_condition_math(check_value, \'>\', operand[0])\n\ndef _do_condition_ge(check_value, operand):\n    return __do_condition_math(check_value, \'>=\', operand[0])\n\ndef _do_condition_lt(check_value, operand):\n    return __do_condition_math(check_value, \'<\', operand[0])\n\ndef _do_condition_le(check_value, operand):\n    return __do_condition_math(check_value, \'<=\', operand[0])\n\ndef _do_condition_between(check_value, operand):\n    return __do_condition_math(check_value, \'>=\', operand[0]) and __do_condition_math(check_value, \'<=\', operand[1])\n\ndef _do_condition_like(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return operand[0] in check_value\n\ndef _do_condition_not_like(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return not _do_condition_like(check_value, operand)\n\ndef _do_condition_in(check_value, operand):\n    check_value = _as_str(check_value)\n    return check_value in operand\n\ndef _do_condition_not_in(check_value, operand):\n    return not _do_condition_in(check_value, operand)\n\ndef _do_condition_regexp(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return re.search(operand[0], check_value) is not None\n\ndef _do_condition_not_regexp(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return not _do_condition_regexp(check_value, operand)\n\nDO_CONDITION_FUNC_MAP = {\n    \'=\'        : _do_condition_eq,\n    \'==\'       : _do_condition_eq,\n    \'!=\'       : _do_condition_ne,\n    \'>\'        : _do_condition_gt,\n    \'>=\'       : _do_condition_ge,\n    \'<\'        : _do_condition_lt,\n    \'<=\'       : _do_condition_le,\n    \'between\'  : _do_condition_between,\n    \'like\'     : _do_condition_like,\n    \'notlike\'  : _do_condition_not_like,\n    \'in\'       : _do_condition_in,\n    \'notin\'    : _do_condition_not_in,\n    \'regexp\'   : _do_condition_regexp,\n    \'notregexp\': _do_condition_not_regexp,\n}\n\ndef _merge_condition_result(a, b, logic):\n    if a is None:\n        return b\n\n    if b is None:\n        return a\n\n    if logic == \'and\':\n        return a and b\n    elif logic == \'or\':\n        return a or b\n\n    return None\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, alert_item_tags, alert_id, duration, check_value_map=None, prev_level=None):\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\'        : level,\n        \'prevLevel\'    : prev_level,\n        \'levelName\'    : LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'tags\'         : _json_dumps(alert_item_tags),\n        \'tagsEscaped\'  : None,\n        \'fields\'       : None,\n        \'fieldsEscaped\': None,\n        \'duration\'     : duration * 1000,\n        \'durationHuman\': None,\n        \'ruleId\'       : trigger_uuid,\n        \'ruleName\'     : trigger_name,\n        \'alertId\'      : alert_id,\n    }\n\n    # $alert 改为 __keyevent 后需要兼容的字段\n    var_dict[\'status\']         = var_dict[\'level\']\n    var_dict[\'prevStatus\']     = var_dict[\'prevLevel\']\n    var_dict[\'statusName\']     = var_dict[\'levelName\']\n    var_dict[\'prevStatusName\'] = var_dict[\'prevLevelName\']\n    var_dict[\'eventId\']        = var_dict[\'alertId\']\n\n    # 补全tags.KEY\n    for k, v in alert_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全fieldsEscaped、tagsEscaped\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    _m, _s = divmod(duration, 60)\n    _h, _m = divmod(_m, 60)\n    _d, _h = divmod(_h, 24)\n    _human = \'{}秒\'.format(_s)\n    if _m > 0:\n        _human = \'{}分钟\'.format(_m) + _human\n    if _h > 0:\n        _human = \'{}小时\'.format(_h) + _human\n    if _d > 0:\n        _human = \'{}天\'.format(_d) + _human\n\n    var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        # print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep      = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from  = from_unit.lower()\n    _search_to    = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n        carry=1000)\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n        carry=1024)\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n        carry=1024)\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n        carry=1024)\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\'    : __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\'   : __filter_volumn,\n    \'volumni\'  : __filter_volumn_i,\n    \'bitrate\'  : __filter_bit_rate,\n    \'byterate\' : __filter_byte_rate,\n    \'upper\'    : __filter_upper,\n    \'lower\'    : __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\'   : __filter_if_true,\n    \'iffalse\'  : __filter_if_false,\n}\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break;\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break;\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, (six.string_types)):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result =  DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title   = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title   = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to      = action[\'to\']\n    title   = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title   = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\'     : to,\n            \'title\'  : title,\n            \'content\': { \'text\': content },\n            \'sender\' : \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook  = action[\'webhook\']\n    title    = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title    = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\'  : title,\n                \'text\'   : markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url        = action[\'url\']\n    title      = action[\'title\']\n    content    = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title      = __render_text(title, var_dict)\n    content    = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\'    : var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\'      : profwang_level,\n            \'type\'       : \'alarm\',\n            \'title\'      : title,\n            \'content\'    : content,\n            \'suggestion\' : suggestion,\n            \'origin\'     : \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\'  : \'POST\',\n                \'url\'     : _url,\n                \'body\'    : _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'     : title,\n        \'content\'   : content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title    = var_dict[\'ruleName\']\n    url      = action[\'url\']\n    method   = action[\'method\'].upper()\n    headers  = action.get(\'headers\')\n    body     = action.get(\'body\')     or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\'  : method,\n                \'url\'     : _url,\n                \'headers\' : headers,\n                \'query\'   : None,\n                \'body\'    : body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': _json_dumps({\n            \'url\'     : url,\n            \'method\'  : method,\n            \'headers\' : headers,\n            \'body\'    : body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title               = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name     = action[\'scriptRefName\']\n    func_ref_name       = action[\'funcRefName\']\n    kwargs              = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\'   : script_ref_name,\n            \'funcRefName\'     : func_ref_name,\n            \'kwargs\'          : kwargs,\n        }),\n    }\n    return action_content, None\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\'           : _create_action_debug,\n    \'mail\'            : _create_action_mail,\n    \'dingTalkRobot\'   : _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\'     : _create_action_http_request,\n    \'DataFluxFunc\'    : _create_action_dataflux_func,\n}\n\ndef _create_keyevents_and_actions(trigger_info, actions, level, prev_level, alert_item_tags, alert_id, duration, check_value_map, extra_alert_tags=None, is_no_data=False):\n    trigger_uuid = trigger_info[\'uuid\']\n    trigger_name = trigger_info[\'name\']\n    dimensions   = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    var_dict = _create_var_dict(\n            trigger_uuid=trigger_uuid,\n            trigger_name=trigger_name,\n            level=level,\n            prev_level=prev_level,\n            alert_item_tags=alert_item_tags,\n            alert_id=alert_id,\n            duration=duration,\n            check_value_map=check_value_map)\n\n    for action in actions:\n        action_type = action[\'type\']\n\n        # 记录执行动作\n        _action = _json_copy(action)\n        action_content = None\n        action_task    = None\n        try:\n            action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n        except Exception as e:\n            print(e)\n        else:\n            if action_task:\n                if isinstance(action_task, list):\n                    actions_to_perform.extend(action_task)\n                else:\n                    actions_to_perform.append(action_task)\n\n        # 记录写入__keyevent\n        _title              = action_content[\'title\']\n        _content            = action_content.get(\'content\')          or None\n        _suggestion         = action_content.get(\'suggestion\')       or None\n        _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n        _keyevent_type = None\n        if is_no_data:\n            _keyevent_type = \'noData\'\n\n        keyevent = {\n            \'title\'          : _title,\n            \'timestamp\'      : _DFF_start_time,\n            \'event_id\'       : alert_id,\n            \'source\'         : \'datafluxTrigger\',\n            \'status\'         : level,\n            \'rule_id\'        : trigger_uuid,\n            \'rule_name\'      : trigger_name,\n            \'type\'           : _keyevent_type,\n            \'alert_item_tags\': alert_item_tags,\n            \'action_type\'    : action_type,\n            \'content\'        : _content,\n            \'suggestion\'     : _suggestion,\n            \'duration\'       : duration,\n            \'dimensions\'     : dimensions,\n            \'tags\'           : extra_alert_tags,\n            \'fields\'         : _extra_alert_fields,\n        }\n        keyevents_to_write.append(keyevent)\n\n    return keyevents_to_write, actions_to_perform\n\n@DFF.API(\'简单检测\', category=\'builtinCheck\', is_hidden=True)\ndef simple_check(trigger_info, targets, condition_check_setting, no_data_check_setting=None, silent_timeout=None, extra_alert_tags=None):\n    \'\'\'\n    简单检测函数\n    \'\'\'\n    # 包装上下文函数\n    trigger_uuid    = trigger_info[\'uuid\']\n    trigger_name    = trigger_info[\'name\']\n    workspace_token = trigger_info[\'workspaceToken\']\n    dimensions      = trigger_info.get(\'dimensions\') or None\n\n    dataway_ref_name = \'df_dataway\'\n\n    dataway = DFF.SRC(dataway_ref_name, token=workspace_token)\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    def get_alert_id(alert_item_tags):\n        return _get_alert_id(trigger_uuid, alert_item_tags)\n\n    def check_if_alert_level_changed(alert_id, level):\n        return _check_if_alert_level_changed(trigger_uuid, alert_id, level)\n\n    def check_if_alert_should_do_action(alert_id, level, is_level_changed):\n        return _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed, silent_timeout)\n\n    def update_no_data_duration(alert_id, is_no_data):\n        return _update_no_data_duration(trigger_uuid, alert_id, is_no_data)\n\n    def get_alert_duration(alert_id):\n        return _get_alert_duration(trigger_uuid, alert_id)\n\n    def store_alert_info(alert_id, level, is_level_changed, should_do_action):\n        return _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action)\n\n    # 查询检测目标数据\n    # 目标数据，结构如下：\n    #     {\n    #         \"<Alert Item Tags JSON>\": {\n    #             \"<Target Alias>\": \"<Check Value>\"\n    #         }\n    #     }\n\n    target_data_map    = {}\n    no_data_alias_list = []\n\n    # 先搜集查询数据\n    influxdb = DFF.SRC(\'df_influxdb\')\n    for target in filter(lambda x: x.get(\'influxQL\'), targets):\n        db_res = None\n        if MOCK_DATA:\n            db_res = MOCK_DATA[target[\'alias\']]\n        else:\n            bind_params = {}\n            if \'$now\' in target[\'influxQL\']:\n                bind_params[\'now\'] = _to_iso_time(_DFF_trigger_time)\n\n            if _DFF_is_debug:\n                print(\'[{}] InfluxQL查询语句：{} <- {}\'.format(target[\'alias\'], target[\'influxQL\'], _json_dumps(bind_params)))\n\n            db_res = influxdb.query(target[\'influxQL\'], bind_params=bind_params)\n\n            if _DFF_is_debug:\n                print(\'[{}] InfluxQL查询结果：{}\'.format(target[\'alias\'], _json_dumps(db_res)))\n\n        target_alias = target[\'alias\']\n\n        valid_series_data = []\n        if \'series\' in db_res:\n            for s in db_res[\'series\']:\n                if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                    continue\n\n                valid_series_data.append(s)\n\n        if valid_series_data:\n            for series_data in valid_series_data:\n                alert_item_tags_json = _get_alert_item_tags_json(series_data[\'tags\'])\n\n                if alert_item_tags_json not in target_data_map:\n                    target_data_map[alert_item_tags_json] = {}\n\n                target_data_map[alert_item_tags_json][target_alias] = _get_check_value(series_data)\n\n        else:\n            no_data_alias_list.append(target_alias)\n\n    # 再搜集expression数据\n    for alert_item_tags_json, alias_value_map in target_data_map.items():\n        for target in filter(lambda x: x.get(\'expression\'), targets):\n            target_alias      = target[\'alias\']\n            target_expression = target[\'expression\']\n\n            target_data_map[alert_item_tags_json][target_alias] = _compute_target_expression(alias_value_map, target_expression)\n\n    if _DFF_is_debug:\n        print(\'检测对象及数据：{}\'.format(target_data_map))\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    # 1. 执行无数据判断\n    no_data_check_level   = no_data_check_setting.get(\'level\') or \'critical\'\n    no_data_check_timeout = no_data_check_setting.get(\'timeout\') or 0\n    no_data_check_actions = no_data_check_setting.get(\'actions\')\n\n    # 配置了触发动作才实际运行\n    if _DFF_is_debug:\n        print(\'=== 开始执行无数据判断 ===\')\n\n    if no_data_check_actions:\n        for target in filter(lambda x: x.get(\'influxQL\'), targets):\n            # 无数据告警仅针对查询数据\n            target_alias = target[\'alias\']\n\n            alert_item_tags      = _get_alert_item_tags_no_data(target_alias)\n            alert_item_tags_json = _get_alert_item_tags_json(alert_item_tags)\n            alert_id             = get_alert_id(alert_item_tags_json)\n\n            is_no_data = target_alias in no_data_alias_list\n            no_data_duration = update_no_data_duration(alert_id, is_no_data)\n\n            # 计算告警级别\n            level = OK_LEVEL\n            if is_no_data:\n                if no_data_duration > no_data_check_timeout:\n                    level = no_data_check_level\n\n                    if _DFF_is_debug:\n                        print(\'`{}`没有数据持续（{}s），超过指定容许时长（{}s）！\'.format(target_alias, no_data_duration, no_data_check_timeout))\n\n                else:\n                    if _DFF_is_debug:\n                        print(\'`{}`没有数据持续（{}s），但尚未超过指定容许时长（{}s）！\'.format(target_alias, no_data_duration, no_data_check_timeout))\n\n            else:\n                if _DFF_is_debug:\n                    print(\'`{}`存在数据\'.format(target_alias))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, level)\n\n            # 处于沉默期间的，不执行动作\n            if not check_if_alert_should_do_action(alert_id, level, is_level_changed):\n                should_do_action = False\n\n            # 触发动作\n            if should_do_action:\n                check_value_map = {\n                    target_alias: None\n                }\n\n                _keyevents, _actions = _create_keyevents_and_actions(\n                        trigger_info=trigger_info,\n                        actions=no_data_check_actions,\n                        level=level,\n                        prev_level=prev_level,\n                        alert_item_tags=alert_item_tags,\n                        alert_id=alert_id,\n                        duration=no_data_duration,\n                        check_value_map=check_value_map,\n                        extra_alert_tags=extra_alert_tags,\n                        is_no_data=True)\n                keyevents_to_write.extend(_keyevents)\n                actions_to_perform.extend(_actions)\n\n            # 记录告警信息\n            store_alert_info(alert_id, level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 2. 执行条件判断（级别从高到低）\n    if _DFF_is_debug:\n        print(\'=== 开始执行条件判断 ===\')\n\n    condition_rules = condition_check_setting[\'rules\']\n    condition_rules.sort(key=lambda x: ALL_LEVELS.index(x[\'level\']))\n    condition_actions = condition_check_setting[\'actions\']\n\n    # 配置了触发动作才实际运行\n    if condition_actions:\n        # 遍历检测对象\n        for alert_item_tags_json, check_value_map in target_data_map.items():\n            alert_id        = get_alert_id(alert_item_tags_json)\n            alert_item_tags = _from_alert_item_tags_json(alert_item_tags_json)\n            duration        = get_alert_duration(alert_id)\n\n            if _DFF_is_debug:\n                print(\'检测对象：{}\'.format(alert_item_tags_json))\n\n            # 遍历规则\n            matched_level = OK_LEVEL\n            for rule in condition_rules:\n                rule_level      = rule[\'level\']\n                condition_logic = rule[\'conditionLogic\']\n                conditions      = rule[\'conditions\']\n\n                # 遍历条件\n                is_rule_matched = None\n                for index, condition in enumerate(conditions):\n                    target_alias = condition[\'targetAlias\']\n                    check_value = check_value_map.get(target_alias)\n\n                    condition_result = False\n                    if check_value is not None:\n                        operator = condition[\'operator\']\n                        operands = condition[\'operands\']\n\n                        condition_func = DO_CONDITION_FUNC_MAP[operator.lower()]\n                        condition_result = condition_func(check_value, operands)\n\n                        if _DFF_is_debug:\n                            if index == 0:\n                                print(\'开始检查 {} 规则\'.format(rule_level))\n\n                            _logic_name = \'\'\n                            if index > 0:\n                                _logic_name = condition_logic.upper() + \' \'\n                            print(\'> {}条件{}：{}({}) {} {} 结果：{}\'.format(\n                                    _logic_name, index + 1, target_alias, check_value, operator, operands, condition_result))\n\n                    is_rule_matched = _merge_condition_result(a=is_rule_matched, b=condition_result, logic=condition_logic)\n\n                    # 短路判断\n                    if condition_logic == \'and\' and not is_rule_matched:\n                        # `and`逻辑遇到`false`\n                        break\n                    elif condition_logic == \'or\' and is_rule_matched:\n                        # `or`逻辑遇到`true`\n                        break\n\n                # 计算告警级别\n                if is_rule_matched:\n                    matched_level = rule_level\n                    break\n\n            if _DFF_is_debug:\n                print(\'检测结果：{}\'.format(matched_level))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, matched_level)\n\n            # 处于沉默期间的，不做任何处理\n            if not check_if_alert_should_do_action(alert_id, matched_level, is_level_changed):\n                should_do_action = False\n\n            # 触发动作\n            if should_do_action:\n                _keyevents, _actions = _create_keyevents_and_actions(\n                        trigger_info=trigger_info,\n                        actions=condition_actions,\n                        level=matched_level,\n                        prev_level=prev_level,\n                        alert_item_tags=alert_item_tags,\n                        alert_id=alert_id,\n                        duration=duration,\n                        check_value_map=check_value_map,\n                        extra_alert_tags=extra_alert_tags)\n                keyevents_to_write.extend(_keyevents)\n                actions_to_perform.extend(_actions)\n\n            # 记录告警信息\n            store_alert_info(alert_id, matched_level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 批量写入告警数据\n    keyevents_to_write = _json_copy(keyevents_to_write)\n\n    if _DFF_is_debug:\n        print(\'写入__keyevent数据：{}\'.format(_json_dumps(keyevents_to_write)))\n\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.write_keyevents(keyevents_to_write)\n        print(\'DataWay返回: {}\'.format(_resp))\n\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n\ndef _convert_groups(aggs, dimensions):\n    value_groups = []\n\n    def __get_count_map(aggs, dimensions):\n        count_map = {}\n\n        if not dimensions:\n            return None\n\n        d = \'_AGG_\' + dimensions[0]\n        agg_res = aggs.get(d)\n\n        if not agg_res or not agg_res[\'buckets\']:\n            count = aggs.get(\'doc_count\')\n            if count:\n                count_map[\'\'] = count\n            return count_map\n\n        if \'buckets\' in agg_res:\n            for b in agg_res[\'buckets\']:\n                v = b[\'key\']\n                count_map[v] = None\n\n                sub = __get_count_map(b, dimensions[1:])\n                if sub:\n                    count_map[v] = sub\n                else:\n                    count_map[v] = b[\'doc_count\']\n\n        return count_map\n\n    def __group_values(count_map, prev_values=None):\n        prev_values = prev_values or []\n        for k, v in count_map.items():\n            forked_prev_values = json.loads(json.dumps(prev_values))\n            forked_prev_values.append(k)\n\n            if isinstance(v, dict):\n                __group_values(v, forked_prev_values)\n\n            else:\n                value_groups.append({\n                    \'values\': forked_prev_values,\n                    \'count\' : v\n                })\n\n    count_map = __get_count_map(aggs, dimensions)\n    __group_values(count_map)\n\n    groups = []\n    for g in value_groups:\n        groups.append({\n            \'tags\': dict(zip(dimensions, g[\'values\'])),\n            \'count\': g[\'count\'],\n        })\n\n    return groups\n\n@DFF.API(\'事件检测\', category=\'builtinCheck\', is_hidden=True)\ndef keyevent_check(trigger_info, target, condition_check_setting, silent_timeout=None, extra_alert_tags=None):\n    \'\'\'\n    事件检测函数\n    \'\'\'\n    # 包装上下文函数\n    trigger_uuid    = trigger_info[\'uuid\']\n    trigger_name    = trigger_info[\'name\']\n    workspace_token = trigger_info[\'workspaceToken\']\n    dimensions      = trigger_info.get(\'dimensions\') or None\n\n    dataway_ref_name = \'df_dataway\'\n\n    dataway = DFF.SRC(dataway_ref_name, token=workspace_token)\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    def get_alert_id(alert_item_tags):\n        return _get_alert_id(trigger_uuid, alert_item_tags)\n\n    def check_if_alert_level_changed(alert_id, level):\n        return _check_if_alert_level_changed(trigger_uuid, alert_id, level)\n\n    def check_if_alert_should_do_action(alert_id, level, is_level_changed):\n        return _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed, silent_timeout)\n\n    def get_alert_duration(alert_id):\n        return _get_alert_duration(trigger_uuid, alert_id)\n\n    def store_alert_info(alert_id, level, is_level_changed, should_do_action):\n        return _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action)\n\n    # 查询检测目标数据\n    # 目标数据，结构如下：\n    #     {\n    #         \"<Alert Item Tags JSON>\": <COUNT>\n    #     }\n    target_data_map = {}\n\n    # 先搜集查询数据\n    es = DFF.SRC(\'df_elasticsearch\')\n\n    db_res = None\n    if MOCK_DATA:\n        db_res = MOCK_DATA\n    else:\n        # 校准时间范围\n        try:\n            timestamp_range = sorted(list(target[\'esQueryBody\'][\'query\'][\'bool\'][\'filter\'][\'range\'][\'__timestampMs\'].values()))\n        except Exception as e:\n            print(e)\n        else:\n            if timestamp_range:\n                delta_t = abs(timestamp_range[-1] - timestamp_range[0])\n                target[\'esQueryBody\'][\'query\'][\'bool\'][\'filter\'][\'range\'][\'__timestampMs\'] = {\n                    \'gte\': _DFF_trigger_time * 1000 - delta_t,\n                    \'lt\' : _DFF_trigger_time * 1000,\n                }\n\n        if _DFF_is_debug:\n            print(\'ES查询路径：{}\'.format(target[\'esQueryPath\']))\n            print(\'ES查询内容：{}\'.format(_json_dumps(target[\'esQueryBody\'])))\n\n        db_res = es.query(\'GET\', path=target[\'esQueryPath\'], body=target[\'esQueryBody\'])\n\n        if _DFF_is_debug:\n            print(\'ES查询结果：{}\'.format(_json_dumps(db_res)))\n\n    if dimensions:\n        groups = _convert_groups(db_res[\'aggregations\'], dimensions)\n        for group_data in groups:\n            alert_item_tags_json = _get_alert_item_tags_json(group_data[\'tags\'])\n            target_data_map[alert_item_tags_json] = group_data[\'count\']\n\n    else:\n        alert_item_tags_json = _get_alert_item_tags_json()\n        target_data_map[alert_item_tags_json] = db_res[\'hits\'][\'total\'][\'value\']\n\n    if _DFF_is_debug:\n        print(\'检测对象及数据：{}\'.format(target_data_map))\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    # 执行条件判断（级别从高到低）\n    if _DFF_is_debug:\n        print(\'=== 开始执行条件判断 ===\')\n\n    condition_rules = condition_check_setting[\'rules\']\n    condition_rules.sort(key=lambda x: ALL_LEVELS.index(x[\'level\']))\n    condition_actions = condition_check_setting[\'actions\']\n\n    # 配置了触发动作才实际运行\n    if condition_actions:\n        # 遍历检测对象\n        for alert_item_tags_json, check_value in target_data_map.items():\n            alert_id        = get_alert_id(alert_item_tags_json)\n            alert_item_tags = _from_alert_item_tags_json(alert_item_tags_json)\n            duration        = get_alert_duration(alert_id)\n\n            if _DFF_is_debug:\n                print(\'检测对象：{}\'.format(alert_item_tags_json))\n\n            # 遍历规则\n            matched_level = OK_LEVEL\n            for rule in condition_rules:\n                rule_level = rule[\'level\']\n                operator   = rule[\'operator\']\n                operands   = rule[\'operands\']\n\n                condition_func = DO_CONDITION_FUNC_MAP[operator.lower()]\n                is_rule_matched = condition_func(check_value, operands)\n\n                if _DFF_is_debug:\n                    print(\'开始检查 {} 规则\'.format(rule_level))\n\n                    print(\'> 条件：事件数量({}) {} {} 结果：{}\'.format(\n                            check_value, operator, operands, is_rule_matched))\n\n                # 计算告警级别\n                if is_rule_matched:\n                    matched_level = rule_level\n                    break\n\n            if _DFF_is_debug:\n                print(\'检测结果：{}\'.format(matched_level))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, matched_level)\n\n            # 处于沉默期间的，不做任何处理\n            if not check_if_alert_should_do_action(alert_id, matched_level, is_level_changed):\n                should_do_action = False\n\n            # 记录触发动作\n            if should_do_action:\n                check_value_map = { \'count\': check_value }\n\n                _keyevents, _actions = _create_keyevents_and_actions(trigger_info, condition_actions,\n                        matched_level, prev_level, alert_item_tags, alert_id, duration,\n                        check_value_map, extra_alert_tags)\n                keyevents_to_write.extend(_keyevents)\n                actions_to_perform.extend(_actions)\n\n            # 记录告警信息\n            store_alert_info(alert_id, matched_level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 批量写入告警数据\n    keyevents_to_write = _json_copy(keyevents_to_write)\n\n    if _DFF_is_debug:\n        print(\'写入__keyevent数据：{}\'.format(_json_dumps(keyevents_to_write)))\n\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.write_keyevents(keyevents_to_write)\n        print(\'DataWay返回: {}\'.format(_resp))\n\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n', '037ef96b1c8c26295e2bae0c6601c809', '\'\'\'\n注意！\n本脚本由驻云官方提供，且在脚本中大量使用了DataFlux.f(x)的内部接口。\n这些内部接口可能会跟随系统更新随时改变，请勿在您的用户脚本中使用！\n\n2020-04-18 优化了动作执行速度\n2020-05-21 新增进阶模板语法，支持指标间运算\n2020-05-25 根据最新数据结构，$alert合并进$keyevent\n2020-06-09 $keyevent改为__keyevent（即相关字段同步修改）\n\'\'\'\n\nimport hashlib\nimport re\nimport time\n\nimport arrow\nimport simplejson as json\nimport six\n\n_DFF_is_debug = True\n\nMOCK_DATA               = None\nRAISE_DATAWAY_EXCEPTION = True\n\nALL_LEVELS          = (\'critical\', \'error\', \'warning\', \'info\')\nOK_LEVEL            = \'ok\'\nLEVEL_NAME_MAP      = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nCONDITION_LOGICS    = (\'and\', \'or\')\nCONDITION_OPERATORS = (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nACTION_TYPES        = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS        = (\'GET\', \'POST\')\nHTTP_BODY_TYPES     = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\ndef _set_mock_data(mock_data):\n    global MOCK_DATA\n    MOCK_DATA = mock_data\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n\n    return six.ensure_str(s)\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\ndef _to_cn_time(d=None):\n    return arrow.get(d).to(\'Asia/Shanghai\').format(\'YYYY-MM-DD HH:mm:ss\')\n\ndef _to_iso_time(d=None):\n    return arrow.get(d).isoformat()\n\ndef _get_store_scope(trigger_uuid):\n    return \'builtinCheck.simpleCheck@{}\'.format(_get_md5(trigger_uuid))\n\ndef _get_alert_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    if _DFF_is_debug:\n        print(\'【告警对象】{}\'.format(str_to_md5))\n    return _get_md5(str_to_md5)\n\ndef _get_alert_store(trigger_uuid, key, alert_id):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.GET(store_key, scope=store_scope)\n\ndef _set_alert_store(trigger_uuid, key, alert_id, value, only_not_exists=False):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.SET(store_key, value=value, scope=store_scope, only_not_exists=only_not_exists)\n\ndef _delete_alert_store(trigger_uuid, key, alert_id):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.DEL(store_key, scope=store_scope)\n\ndef _check_if_alert_level_changed(trigger_uuid, alert_id, level):\n    prev_level = _get_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id) or OK_LEVEL\n    is_level_changed = prev_level != level\n\n    if _DFF_is_debug:\n        print(\'上次检测结果：{}，本次检测结果：{}\'.format(prev_level, level))\n\n    return is_level_changed, prev_level\n\ndef _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed=None, silent_timeout=None):\n    if is_level_changed:\n        # 级别发生改变一定触发动作\n        if _DFF_is_debug:\n            print(\'检测结果改变，一定触发动作。当前检测结果：{}\'.format(level))\n\n        return True\n\n    else:\n        if _DFF_is_debug:\n            print(\'检测结果不变，当前检测结果：{}\'.format(level))\n\n        if level != OK_LEVEL and silent_timeout is not None:\n            # 级别未发生改变时，且保持非`ok`级别，则到达超时时间才触发动作\n            alert_report_time = _get_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id) or 0\n            past_time = _DFF_start_time - alert_report_time\n            should_do_action = past_time > silent_timeout\n\n            if _DFF_is_debug:\n                print(\'距离上次告警经过：{}s\'.format(past_time))\n                if should_do_action:\n                    print(\'由于距离上次告警时间（{}s）已超过沉默时长（{}s），因此需要触发动作\'.format(past_time, silent_timeout))\n                else:\n                    print(\'由于距离上次告警时间（{}s）尚未超过沉默时长（{}s），因此不需要触发动作\'.format(past_time, silent_timeout))\n\n            return should_do_action\n\n    if _DFF_is_debug:\n        print(\'当前检测结果：{}，不需要触发动作\'.format(level))\n\n    return False\n\ndef _update_no_data_duration(trigger_uuid, alert_id, is_no_data):\n    prev_no_data_time = _get_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id)\n    if _DFF_is_debug:\n        if prev_no_data_time:\n            print(\'无数据开始时间：{}\'.format(_to_cn_time(prev_no_data_time)))\n        else:\n            print(\'首次处理无数据告警\')\n\n    if is_no_data:\n        if prev_no_data_time:\n            if _DFF_is_debug:\n                print(\'已经记录过无数据时间，继续...\')\n\n            return _DFF_start_time - prev_no_data_time\n\n        else:\n            _set_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id, value=_DFF_start_time)\n\n            if _DFF_is_debug:\n                print(\'记录无数据时间\')\n\n            return 0\n\n    else:\n        if prev_no_data_time:\n            _delete_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id)\n\n            if _DFF_is_debug:\n                print(\'清除无数据时间\')\n\n            return _DFF_start_time - prev_no_data_time\n\n        else:\n            if _DFF_is_debug:\n                print(\'数据正常，无需处理\')\n\n            return 0\n\ndef _get_alert_duration(trigger_uuid, alert_id):\n    alert_level_change_time = _get_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id) or _DFF_start_time\n\n    if _DFF_is_debug:\n        print(\'告警级别变化时间：{}\'.format(_to_cn_time(alert_level_change_time)))\n\n    return _DFF_start_time - alert_level_change_time\n\ndef _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action):\n    if level == OK_LEVEL:\n        _delete_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id)\n        _delete_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id)\n        _delete_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id)\n\n    else:\n        if is_level_changed:\n            _set_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id, value=_DFF_start_time)\n            _set_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id, value=level)\n\n        if should_do_action:\n            _set_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id, value=_DFF_start_time)\n\ndef _get_check_value(series_data):\n    # 获取非`time`列的索引号\n    for index, name in enumerate(series_data[\'columns\']):\n        if name == \'time\':\n            continue\n\n        return series_data[\'values\'][0][index]\n\n    return None\n\ndef _get_alert_item_tags_no_data(target_alias):\n    alert_item_tags = {\n        \'noData\': target_alias\n    }\n    return alert_item_tags\n\ndef _get_alert_item_tags_json(alert_item_tags=None):\n    return _json_dumps(alert_item_tags or {})\n\ndef _from_alert_item_tags_json(alert_item_tags_json):\n    return json.loads(alert_item_tags_json)\n\ndef __do_condition_math(check_value, operator, operand):\n    if isinstance(operand, list):\n        operand = operand[0]\n\n    try:\n        T = type(check_value)\n        operand = T(operand)\n\n        if operator == \'==\':\n            return check_value == operand\n        elif operator == \'!=\':\n            return check_value != operand\n        elif operator == \'>\':\n            return check_value > operand\n        elif operator == \'>=\':\n            return check_value >= operand\n        elif operator == \'<\':\n            return check_value < operand\n        elif operator == \'<=\':\n            return check_value <= operand\n\n    except Exception as e:\n        print(e)\n        return False\n\ndef _do_condition_eq(check_value, operand):\n    return __do_condition_math(check_value, \'==\', operand[0])\n\ndef _do_condition_ne(check_value, operand):\n    return __do_condition_math(check_value, \'!=\', operand[0])\n\ndef _do_condition_gt(check_value, operand):\n    return __do_condition_math(check_value, \'>\', operand[0])\n\ndef _do_condition_ge(check_value, operand):\n    return __do_condition_math(check_value, \'>=\', operand[0])\n\ndef _do_condition_lt(check_value, operand):\n    return __do_condition_math(check_value, \'<\', operand[0])\n\ndef _do_condition_le(check_value, operand):\n    return __do_condition_math(check_value, \'<=\', operand[0])\n\ndef _do_condition_between(check_value, operand):\n    return __do_condition_math(check_value, \'>=\', operand[0]) and __do_condition_math(check_value, \'<=\', operand[1])\n\ndef _do_condition_like(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return operand[0] in check_value\n\ndef _do_condition_not_like(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return not _do_condition_like(check_value, operand)\n\ndef _do_condition_in(check_value, operand):\n    check_value = _as_str(check_value)\n    return check_value in operand\n\ndef _do_condition_not_in(check_value, operand):\n    return not _do_condition_in(check_value, operand)\n\ndef _do_condition_regexp(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return re.search(operand[0], check_value) is not None\n\ndef _do_condition_not_regexp(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return not _do_condition_regexp(check_value, operand)\n\nDO_CONDITION_FUNC_MAP = {\n    \'=\'        : _do_condition_eq,\n    \'==\'       : _do_condition_eq,\n    \'!=\'       : _do_condition_ne,\n    \'>\'        : _do_condition_gt,\n    \'>=\'       : _do_condition_ge,\n    \'<\'        : _do_condition_lt,\n    \'<=\'       : _do_condition_le,\n    \'between\'  : _do_condition_between,\n    \'like\'     : _do_condition_like,\n    \'notlike\'  : _do_condition_not_like,\n    \'in\'       : _do_condition_in,\n    \'notin\'    : _do_condition_not_in,\n    \'regexp\'   : _do_condition_regexp,\n    \'notregexp\': _do_condition_not_regexp,\n}\n\ndef _merge_condition_result(a, b, logic):\n    if a is None:\n        return b\n\n    if b is None:\n        return a\n\n    if logic == \'and\':\n        return a and b\n    elif logic == \'or\':\n        return a or b\n\n    return None\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, alert_item_tags, alert_id, duration, check_value_map=None, prev_level=None):\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\'        : level,\n        \'prevLevel\'    : prev_level,\n        \'levelName\'    : LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'tags\'         : _json_dumps(alert_item_tags),\n        \'tagsEscaped\'  : None,\n        \'fields\'       : None,\n        \'fieldsEscaped\': None,\n        \'duration\'     : duration * 1000,\n        \'durationHuman\': None,\n        \'ruleId\'       : trigger_uuid,\n        \'ruleName\'     : trigger_name,\n        \'alertId\'      : alert_id,\n    }\n\n    # $alert 改为 __keyevent 后需要兼容的字段\n    var_dict[\'status\']         = var_dict[\'level\']\n    var_dict[\'prevStatus\']     = var_dict[\'prevLevel\']\n    var_dict[\'statusName\']     = var_dict[\'levelName\']\n    var_dict[\'prevStatusName\'] = var_dict[\'prevLevelName\']\n    var_dict[\'eventId\']        = var_dict[\'alertId\']\n\n    # 补全tags.KEY\n    for k, v in alert_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全fieldsEscaped、tagsEscaped\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    _m, _s = divmod(duration, 60)\n    _h, _m = divmod(_m, 60)\n    _d, _h = divmod(_h, 24)\n    _human = \'{}秒\'.format(_s)\n    if _m > 0:\n        _human = \'{}分钟\'.format(_m) + _human\n    if _h > 0:\n        _human = \'{}小时\'.format(_h) + _human\n    if _d > 0:\n        _human = \'{}天\'.format(_d) + _human\n\n    var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        # print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep      = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from  = from_unit.lower()\n    _search_to    = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n        carry=1000)\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n        carry=1024)\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n        carry=1024)\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n        carry=1024)\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\'    : __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\'   : __filter_volumn,\n    \'volumni\'  : __filter_volumn_i,\n    \'bitrate\'  : __filter_bit_rate,\n    \'byterate\' : __filter_byte_rate,\n    \'upper\'    : __filter_upper,\n    \'lower\'    : __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\'   : __filter_if_true,\n    \'iffalse\'  : __filter_if_false,\n}\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break;\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break;\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, (six.string_types)):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result =  DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title   = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title   = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to      = action[\'to\']\n    title   = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title   = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\'     : to,\n            \'title\'  : title,\n            \'content\': { \'text\': content },\n            \'sender\' : \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook  = action[\'webhook\']\n    title    = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title    = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\'  : title,\n                \'text\'   : markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url        = action[\'url\']\n    title      = action[\'title\']\n    content    = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title      = __render_text(title, var_dict)\n    content    = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\'    : var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\'      : profwang_level,\n            \'type\'       : \'alarm\',\n            \'title\'      : title,\n            \'content\'    : content,\n            \'suggestion\' : suggestion,\n            \'origin\'     : \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\'  : \'POST\',\n                \'url\'     : _url,\n                \'body\'    : _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'     : title,\n        \'content\'   : content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title    = var_dict[\'ruleName\']\n    url      = action[\'url\']\n    method   = action[\'method\'].upper()\n    headers  = action.get(\'headers\')\n    body     = action.get(\'body\')     or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\'  : method,\n                \'url\'     : _url,\n                \'headers\' : headers,\n                \'query\'   : None,\n                \'body\'    : body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': _json_dumps({\n            \'url\'     : url,\n            \'method\'  : method,\n            \'headers\' : headers,\n            \'body\'    : body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title               = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name     = action[\'scriptRefName\']\n    func_ref_name       = action[\'funcRefName\']\n    kwargs              = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\'   : script_ref_name,\n            \'funcRefName\'     : func_ref_name,\n            \'kwargs\'          : kwargs,\n        }),\n    }\n    return action_content, None\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\'           : _create_action_debug,\n    \'mail\'            : _create_action_mail,\n    \'dingTalkRobot\'   : _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\'     : _create_action_http_request,\n    \'DataFluxFunc\'    : _create_action_dataflux_func,\n}\n\ndef _create_keyevents_and_actions(trigger_info, actions, level, prev_level, alert_item_tags, alert_id, duration, check_value_map, extra_alert_tags=None, is_no_data=False):\n    trigger_uuid = trigger_info[\'uuid\']\n    trigger_name = trigger_info[\'name\']\n    dimensions   = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    var_dict = _create_var_dict(\n            trigger_uuid=trigger_uuid,\n            trigger_name=trigger_name,\n            level=level,\n            prev_level=prev_level,\n            alert_item_tags=alert_item_tags,\n            alert_id=alert_id,\n            duration=duration,\n            check_value_map=check_value_map)\n\n    for action in actions:\n        action_type = action[\'type\']\n\n        # 记录执行动作\n        _action = _json_copy(action)\n        action_content = None\n        action_task    = None\n        try:\n            action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n        except Exception as e:\n            print(e)\n        else:\n            if action_task:\n                if isinstance(action_task, list):\n                    actions_to_perform.extend(action_task)\n                else:\n                    actions_to_perform.append(action_task)\n\n        # 记录写入__keyevent\n        _title              = action_content[\'title\']\n        _content            = action_content.get(\'content\')          or None\n        _suggestion         = action_content.get(\'suggestion\')       or None\n        _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n        _keyevent_type = None\n        if is_no_data:\n            _keyevent_type = \'noData\'\n\n        keyevent = {\n            \'title\'          : _title,\n            \'timestamp\'      : _DFF_start_time,\n            \'event_id\'       : alert_id,\n            \'source\'         : \'datafluxTrigger\',\n            \'status\'         : level,\n            \'rule_id\'        : trigger_uuid,\n            \'rule_name\'      : trigger_name,\n            \'type\'           : _keyevent_type,\n            \'alert_item_tags\': alert_item_tags,\n            \'action_type\'    : action_type,\n            \'content\'        : _content,\n            \'suggestion\'     : _suggestion,\n            \'duration\'       : duration,\n            \'dimensions\'     : dimensions,\n            \'tags\'           : extra_alert_tags,\n            \'fields\'         : _extra_alert_fields,\n        }\n        keyevents_to_write.append(keyevent)\n\n    return keyevents_to_write, actions_to_perform\n\n@DFF.API(\'简单检测\', category=\'builtinCheck\', is_hidden=True)\ndef simple_check(trigger_info, targets, condition_check_setting, no_data_check_setting=None, silent_timeout=None, extra_alert_tags=None):\n    \'\'\'\n    简单检测函数\n    \'\'\'\n    # 包装上下文函数\n    trigger_uuid    = trigger_info[\'uuid\']\n    trigger_name    = trigger_info[\'name\']\n    workspace_token = trigger_info[\'workspaceToken\']\n    dimensions      = trigger_info.get(\'dimensions\') or None\n\n    dataway_ref_name = \'df_dataway\'\n\n    dataway = DFF.SRC(dataway_ref_name, token=workspace_token)\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    def get_alert_id(alert_item_tags):\n        return _get_alert_id(trigger_uuid, alert_item_tags)\n\n    def check_if_alert_level_changed(alert_id, level):\n        return _check_if_alert_level_changed(trigger_uuid, alert_id, level)\n\n    def check_if_alert_should_do_action(alert_id, level, is_level_changed):\n        return _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed, silent_timeout)\n\n    def update_no_data_duration(alert_id, is_no_data):\n        return _update_no_data_duration(trigger_uuid, alert_id, is_no_data)\n\n    def get_alert_duration(alert_id):\n        return _get_alert_duration(trigger_uuid, alert_id)\n\n    def store_alert_info(alert_id, level, is_level_changed, should_do_action):\n        return _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action)\n\n    # 查询检测目标数据\n    # 目标数据，结构如下：\n    #     {\n    #         \"<Alert Item Tags JSON>\": {\n    #             \"<Target Alias>\": \"<Check Value>\"\n    #         }\n    #     }\n\n    target_data_map    = {}\n    no_data_alias_list = []\n\n    # 先搜集查询数据\n    influxdb = DFF.SRC(\'df_influxdb\')\n    for target in filter(lambda x: x.get(\'influxQL\'), targets):\n        db_res = None\n        if MOCK_DATA:\n            db_res = MOCK_DATA[target[\'alias\']]\n        else:\n            bind_params = {}\n            if \'$now\' in target[\'influxQL\']:\n                bind_params[\'now\'] = _to_iso_time(_DFF_trigger_time)\n\n            if _DFF_is_debug:\n                print(\'[{}] InfluxQL查询语句：{} <- {}\'.format(target[\'alias\'], target[\'influxQL\'], _json_dumps(bind_params)))\n\n            db_res = influxdb.query(target[\'influxQL\'], bind_params=bind_params)\n\n            if _DFF_is_debug:\n                print(\'[{}] InfluxQL查询结果：{}\'.format(target[\'alias\'], _json_dumps(db_res)))\n\n        target_alias = target[\'alias\']\n\n        valid_series_data = []\n        if \'series\' in db_res:\n            for s in db_res[\'series\']:\n                if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                    continue\n\n                valid_series_data.append(s)\n\n        if valid_series_data:\n            for series_data in valid_series_data:\n                alert_item_tags_json = _get_alert_item_tags_json(series_data[\'tags\'])\n\n                if alert_item_tags_json not in target_data_map:\n                    target_data_map[alert_item_tags_json] = {}\n\n                target_data_map[alert_item_tags_json][target_alias] = _get_check_value(series_data)\n\n        else:\n            no_data_alias_list.append(target_alias)\n\n    # 再搜集expression数据\n    for alert_item_tags_json, alias_value_map in target_data_map.items():\n        for target in filter(lambda x: x.get(\'expression\'), targets):\n            target_alias      = target[\'alias\']\n            target_expression = target[\'expression\']\n\n            target_data_map[alert_item_tags_json][target_alias] = _compute_target_expression(alias_value_map, target_expression)\n\n    if _DFF_is_debug:\n        print(\'检测对象及数据：{}\'.format(target_data_map))\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    # 1. 执行无数据判断\n    no_data_check_level   = no_data_check_setting.get(\'level\') or \'critical\'\n    no_data_check_timeout = no_data_check_setting.get(\'timeout\') or 0\n    no_data_check_actions = no_data_check_setting.get(\'actions\')\n\n    # 配置了触发动作才实际运行\n    if _DFF_is_debug:\n        print(\'=== 开始执行无数据判断 ===\')\n\n    if no_data_check_actions:\n        for target in filter(lambda x: x.get(\'influxQL\'), targets):\n            # 无数据告警仅针对查询数据\n            target_alias = target[\'alias\']\n\n            alert_item_tags      = _get_alert_item_tags_no_data(target_alias)\n            alert_item_tags_json = _get_alert_item_tags_json(alert_item_tags)\n            alert_id             = get_alert_id(alert_item_tags_json)\n\n            is_no_data = target_alias in no_data_alias_list\n            no_data_duration = update_no_data_duration(alert_id, is_no_data)\n\n            # 计算告警级别\n            level = OK_LEVEL\n            if is_no_data:\n                if no_data_duration > no_data_check_timeout:\n                    level = no_data_check_level\n\n                    if _DFF_is_debug:\n                        print(\'`{}`没有数据持续（{}s），超过指定容许时长（{}s）！\'.format(target_alias, no_data_duration, no_data_check_timeout))\n\n                else:\n                    if _DFF_is_debug:\n                        print(\'`{}`没有数据持续（{}s），但尚未超过指定容许时长（{}s）！\'.format(target_alias, no_data_duration, no_data_check_timeout))\n\n            else:\n                if _DFF_is_debug:\n                    print(\'`{}`存在数据\'.format(target_alias))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, level)\n\n            # 处于沉默期间的，不执行动作\n            if not check_if_alert_should_do_action(alert_id, level, is_level_changed):\n                should_do_action = False\n\n            # 触发动作\n            if should_do_action:\n                check_value_map = {\n                    target_alias: None\n                }\n\n                _keyevents, _actions = _create_keyevents_and_actions(\n                        trigger_info=trigger_info,\n                        actions=no_data_check_actions,\n                        level=level,\n                        prev_level=prev_level,\n                        alert_item_tags=alert_item_tags,\n                        alert_id=alert_id,\n                        duration=no_data_duration,\n                        check_value_map=check_value_map,\n                        extra_alert_tags=extra_alert_tags,\n                        is_no_data=True)\n                keyevents_to_write.extend(_keyevents)\n                actions_to_perform.extend(_actions)\n\n            # 记录告警信息\n            store_alert_info(alert_id, level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 2. 执行条件判断（级别从高到低）\n    if _DFF_is_debug:\n        print(\'=== 开始执行条件判断 ===\')\n\n    condition_rules = condition_check_setting[\'rules\']\n    condition_rules.sort(key=lambda x: ALL_LEVELS.index(x[\'level\']))\n    condition_actions = condition_check_setting[\'actions\']\n\n    # 配置了触发动作才实际运行\n    if condition_actions:\n        # 遍历检测对象\n        for alert_item_tags_json, check_value_map in target_data_map.items():\n            alert_id        = get_alert_id(alert_item_tags_json)\n            alert_item_tags = _from_alert_item_tags_json(alert_item_tags_json)\n            duration        = get_alert_duration(alert_id)\n\n            if _DFF_is_debug:\n                print(\'检测对象：{}\'.format(alert_item_tags_json))\n\n            # 遍历规则\n            matched_level = OK_LEVEL\n            for rule in condition_rules:\n                rule_level      = rule[\'level\']\n                condition_logic = rule[\'conditionLogic\']\n                conditions      = rule[\'conditions\']\n\n                # 遍历条件\n                is_rule_matched = None\n                for index, condition in enumerate(conditions):\n                    target_alias = condition[\'targetAlias\']\n                    check_value = check_value_map.get(target_alias)\n\n                    condition_result = False\n                    if check_value is not None:\n                        operator = condition[\'operator\']\n                        operands = condition[\'operands\']\n\n                        condition_func = DO_CONDITION_FUNC_MAP[operator.lower()]\n                        condition_result = condition_func(check_value, operands)\n\n                        if _DFF_is_debug:\n                            if index == 0:\n                                print(\'开始检查 {} 规则\'.format(rule_level))\n\n                            _logic_name = \'\'\n                            if index > 0:\n                                _logic_name = condition_logic.upper() + \' \'\n                            print(\'> {}条件{}：{}({}) {} {} 结果：{}\'.format(\n                                    _logic_name, index + 1, target_alias, check_value, operator, operands, condition_result))\n\n                    is_rule_matched = _merge_condition_result(a=is_rule_matched, b=condition_result, logic=condition_logic)\n\n                    # 短路判断\n                    if condition_logic == \'and\' and not is_rule_matched:\n                        # `and`逻辑遇到`false`\n                        break\n                    elif condition_logic == \'or\' and is_rule_matched:\n                        # `or`逻辑遇到`true`\n                        break\n\n                # 计算告警级别\n                if is_rule_matched:\n                    matched_level = rule_level\n                    break\n\n            if _DFF_is_debug:\n                print(\'检测结果：{}\'.format(matched_level))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, matched_level)\n\n            # 处于沉默期间的，不做任何处理\n            if not check_if_alert_should_do_action(alert_id, matched_level, is_level_changed):\n                should_do_action = False\n\n            # 触发动作\n            if should_do_action:\n                _keyevents, _actions = _create_keyevents_and_actions(\n                        trigger_info=trigger_info,\n                        actions=condition_actions,\n                        level=matched_level,\n                        prev_level=prev_level,\n                        alert_item_tags=alert_item_tags,\n                        alert_id=alert_id,\n                        duration=duration,\n                        check_value_map=check_value_map,\n                        extra_alert_tags=extra_alert_tags)\n                keyevents_to_write.extend(_keyevents)\n                actions_to_perform.extend(_actions)\n\n            # 记录告警信息\n            store_alert_info(alert_id, matched_level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 批量写入告警数据\n    keyevents_to_write = _json_copy(keyevents_to_write)\n\n    if _DFF_is_debug:\n        print(\'写入__keyevent数据：{}\'.format(_json_dumps(keyevents_to_write)))\n\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.write_keyevents(keyevents_to_write)\n        print(\'DataWay返回: {}\'.format(_resp))\n\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n\ndef _convert_groups(aggs, dimensions):\n    value_groups = []\n\n    def __get_count_map(aggs, dimensions):\n        count_map = {}\n\n        if not dimensions:\n            return None\n\n        d = \'_AGG_\' + dimensions[0]\n        agg_res = aggs.get(d)\n\n        if not agg_res or not agg_res[\'buckets\']:\n            count = aggs.get(\'doc_count\')\n            if count:\n                count_map[\'\'] = count\n            return count_map\n\n        if \'buckets\' in agg_res:\n            for b in agg_res[\'buckets\']:\n                v = b[\'key\']\n                count_map[v] = None\n\n                sub = __get_count_map(b, dimensions[1:])\n                if sub:\n                    count_map[v] = sub\n                else:\n                    count_map[v] = b[\'doc_count\']\n\n        return count_map\n\n    def __group_values(count_map, prev_values=None):\n        prev_values = prev_values or []\n        for k, v in count_map.items():\n            forked_prev_values = json.loads(json.dumps(prev_values))\n            forked_prev_values.append(k)\n\n            if isinstance(v, dict):\n                __group_values(v, forked_prev_values)\n\n            else:\n                value_groups.append({\n                    \'values\': forked_prev_values,\n                    \'count\' : v\n                })\n\n    count_map = __get_count_map(aggs, dimensions)\n    __group_values(count_map)\n\n    groups = []\n    for g in value_groups:\n        groups.append({\n            \'tags\': dict(zip(dimensions, g[\'values\'])),\n            \'count\': g[\'count\'],\n        })\n\n    return groups\n\n@DFF.API(\'事件检测\', category=\'builtinCheck\', is_hidden=True)\ndef keyevent_check(trigger_info, target, condition_check_setting, silent_timeout=None, extra_alert_tags=None):\n    \'\'\'\n    事件检测函数\n    \'\'\'\n    # 包装上下文函数\n    trigger_uuid    = trigger_info[\'uuid\']\n    trigger_name    = trigger_info[\'name\']\n    workspace_token = trigger_info[\'workspaceToken\']\n    dimensions      = trigger_info.get(\'dimensions\') or None\n\n    dataway_ref_name = \'df_dataway\'\n\n    dataway = DFF.SRC(dataway_ref_name, token=workspace_token)\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    def get_alert_id(alert_item_tags):\n        return _get_alert_id(trigger_uuid, alert_item_tags)\n\n    def check_if_alert_level_changed(alert_id, level):\n        return _check_if_alert_level_changed(trigger_uuid, alert_id, level)\n\n    def check_if_alert_should_do_action(alert_id, level, is_level_changed):\n        return _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed, silent_timeout)\n\n    def get_alert_duration(alert_id):\n        return _get_alert_duration(trigger_uuid, alert_id)\n\n    def store_alert_info(alert_id, level, is_level_changed, should_do_action):\n        return _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action)\n\n    # 查询检测目标数据\n    # 目标数据，结构如下：\n    #     {\n    #         \"<Alert Item Tags JSON>\": <COUNT>\n    #     }\n    target_data_map = {}\n\n    # 先搜集查询数据\n    es = DFF.SRC(\'df_elasticsearch\')\n\n    db_res = None\n    if MOCK_DATA:\n        db_res = MOCK_DATA\n    else:\n        # 校准时间范围\n        try:\n            timestamp_range = sorted(list(target[\'esQueryBody\'][\'query\'][\'bool\'][\'filter\'][\'range\'][\'__timestampMs\'].values()))\n        except Exception as e:\n            print(e)\n        else:\n            if timestamp_range:\n                delta_t = abs(timestamp_range[-1] - timestamp_range[0])\n                target[\'esQueryBody\'][\'query\'][\'bool\'][\'filter\'][\'range\'][\'__timestampMs\'] = {\n                    \'gte\': _DFF_trigger_time * 1000 - delta_t,\n                    \'lt\' : _DFF_trigger_time * 1000,\n                }\n\n        if _DFF_is_debug:\n            print(\'ES查询路径：{}\'.format(target[\'esQueryPath\']))\n            print(\'ES查询内容：{}\'.format(_json_dumps(target[\'esQueryBody\'])))\n\n        db_res = es.query(\'GET\', path=target[\'esQueryPath\'], body=target[\'esQueryBody\'])\n\n        if _DFF_is_debug:\n            print(\'ES查询结果：{}\'.format(_json_dumps(db_res)))\n\n    if dimensions:\n        groups = _convert_groups(db_res[\'aggregations\'], dimensions)\n        for group_data in groups:\n            alert_item_tags_json = _get_alert_item_tags_json(group_data[\'tags\'])\n            target_data_map[alert_item_tags_json] = group_data[\'count\']\n\n    else:\n        alert_item_tags_json = _get_alert_item_tags_json()\n        target_data_map[alert_item_tags_json] = db_res[\'hits\'][\'total\'][\'value\']\n\n    if _DFF_is_debug:\n        print(\'检测对象及数据：{}\'.format(target_data_map))\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    # 执行条件判断（级别从高到低）\n    if _DFF_is_debug:\n        print(\'=== 开始执行条件判断 ===\')\n\n    condition_rules = condition_check_setting[\'rules\']\n    condition_rules.sort(key=lambda x: ALL_LEVELS.index(x[\'level\']))\n    condition_actions = condition_check_setting[\'actions\']\n\n    # 配置了触发动作才实际运行\n    if condition_actions:\n        # 遍历检测对象\n        for alert_item_tags_json, check_value in target_data_map.items():\n            alert_id        = get_alert_id(alert_item_tags_json)\n            alert_item_tags = _from_alert_item_tags_json(alert_item_tags_json)\n            duration        = get_alert_duration(alert_id)\n\n            if _DFF_is_debug:\n                print(\'检测对象：{}\'.format(alert_item_tags_json))\n\n            # 遍历规则\n            matched_level = OK_LEVEL\n            for rule in condition_rules:\n                rule_level = rule[\'level\']\n                operator   = rule[\'operator\']\n                operands   = rule[\'operands\']\n\n                condition_func = DO_CONDITION_FUNC_MAP[operator.lower()]\n                is_rule_matched = condition_func(check_value, operands)\n\n                if _DFF_is_debug:\n                    print(\'开始检查 {} 规则\'.format(rule_level))\n\n                    print(\'> 条件：事件数量({}) {} {} 结果：{}\'.format(\n                            check_value, operator, operands, is_rule_matched))\n\n                # 计算告警级别\n                if is_rule_matched:\n                    matched_level = rule_level\n                    break\n\n            if _DFF_is_debug:\n                print(\'检测结果：{}\'.format(matched_level))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, matched_level)\n\n            # 处于沉默期间的，不做任何处理\n            if not check_if_alert_should_do_action(alert_id, matched_level, is_level_changed):\n                should_do_action = False\n\n            # 记录触发动作\n            if should_do_action:\n                check_value_map = { \'count\': check_value }\n\n                _keyevents, _actions = _create_keyevents_and_actions(trigger_info, condition_actions,\n                        matched_level, prev_level, alert_item_tags, alert_id, duration,\n                        check_value_map, extra_alert_tags)\n                keyevents_to_write.extend(_keyevents)\n                actions_to_perform.extend(_actions)\n\n            # 记录告警信息\n            store_alert_info(alert_id, matched_level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 批量写入告警数据\n    keyevents_to_write = _json_copy(keyevents_to_write)\n\n    if _DFF_is_debug:\n        print(\'写入__keyevent数据：{}\'.format(_json_dumps(keyevents_to_write)))\n\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.write_keyevents(keyevents_to_write)\n        print(\'DataWay返回: {}\'.format(_resp))\n\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n', '037ef96b1c8c26295e2bae0c6601c809', '2020-06-29 09:37:40', '2020-06-29 09:41:07'),
('4', 'scpt-ft_pred', 'FT预测函数', '预测函数脚本集', 'sset-ft_lib', 'ft_pred', '1', 'python', 'import numpy as np\nimport random\nimport requests\nimport pandas as pd\nfrom math import sqrt\nfrom statsmodels.tsa.api import ExponentialSmoothing\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n@DFF.API(\'平均值预测\', category=\'prediction\')\ndef avg_prediction(dps):\n    \'\'\'\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[\'avg_forecast\'] = train[1].mean()\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    mean = train[\'avg_forecast\'][0]\n\n    ret = []\n    for i in range(50):\n        time_point += time_diff\n        value = round(mean,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'移动均值预测\', category=\'prediction\')\ndef moving_prediction(dps):\n    \'\'\'\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[\'moving_avg_forecast\'] = train[1].rolling(60).mean().iloc[-1]\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    mean = train[\'moving_avg_forecast\'][0]\n    ret = []\n    for i in range(50):\n        time_point += time_diff\n        value = round(mean,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'Holt函数预测\', category=\'prediction\')\ndef holt_prediction(dps):\n    \"\"\"\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \"\"\"\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    fit1 = ExponentialSmoothing(np.asarray(train[1]),seasonal_periods=7,trend=\'add\',seasonal=\'add\').fit()\n    y_hat_avg = fit1.forecast(50)\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    ret = []\n    for i in y_hat_avg:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n@DFF.API(\'MA函数预测\', category=\'prediction\')\ndef ma_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = ARMA(data, order=(0,7))\n    model_fit = model.fit(disp=False)\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n    print(yhat)\n    print(time_diff)\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'AR函数预测\', category=\'prediction\')\ndef ar_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = AR(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+30)\n\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'ARIMA函数预测\', category=\'prediction\')\ndef arima_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = AR(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\ndef test_prediction():\n    dps =[[1581777800000, -33.55444223812124],[1581777810000, -50.83663612870732],[1581777820000, -39.55672134160139],[1581777830000, -38.39518359982622],[1581777840000, -36.22997922350511],[1581777850000, -36.78477147596941],\n[1581777860000, -20.12616034401329],\n[1581777870000, -36.54856240481572],\n[1581777880000, -30.31006152436570],\n[1581777890000, -42.53321660981094],\n[1581777900000, -13.20427195011425],\n[1581777910000, -14.53546668748857],\n[1581777920000, -15.55725246006902],\n[1581777930000, -26.76065716597191],\n[1581777940000, -35.55599811602032]]\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = ExponentialSmoothing(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n    print(yhat)\n    print(time_diff)\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n', '8bb88a6c1c66af816e187f161f2bdae4', 'import numpy as np\nimport random\nimport requests\nimport pandas as pd\nfrom math import sqrt\nfrom statsmodels.tsa.api import ExponentialSmoothing\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n@DFF.API(\'平均值预测\', category=\'prediction\')\ndef avg_prediction(dps):\n    \'\'\'\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[\'avg_forecast\'] = train[1].mean()\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    mean = train[\'avg_forecast\'][0]\n\n    ret = []\n    for i in range(50):\n        time_point += time_diff\n        value = round(mean,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'移动均值预测\', category=\'prediction\')\ndef moving_prediction(dps):\n    \'\'\'\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[\'moving_avg_forecast\'] = train[1].rolling(60).mean().iloc[-1]\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    mean = train[\'moving_avg_forecast\'][0]\n    ret = []\n    for i in range(50):\n        time_point += time_diff\n        value = round(mean,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'Holt函数预测\', category=\'prediction\')\ndef holt_prediction(dps):\n    \"\"\"\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \"\"\"\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    fit1 = ExponentialSmoothing(np.asarray(train[1]),seasonal_periods=7,trend=\'add\',seasonal=\'add\').fit()\n    y_hat_avg = fit1.forecast(50)\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    ret = []\n    for i in y_hat_avg:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n@DFF.API(\'MA函数预测\', category=\'prediction\')\ndef ma_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = ARMA(data, order=(0,7))\n    model_fit = model.fit(disp=False)\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n    print(yhat)\n    print(time_diff)\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'AR函数预测\', category=\'prediction\')\ndef ar_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = AR(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+30)\n\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'ARIMA函数预测\', category=\'prediction\')\ndef arima_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = AR(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\ndef test_prediction():\n    dps =[[1581777800000, -33.55444223812124],[1581777810000, -50.83663612870732],[1581777820000, -39.55672134160139],[1581777830000, -38.39518359982622],[1581777840000, -36.22997922350511],[1581777850000, -36.78477147596941],\n[1581777860000, -20.12616034401329],\n[1581777870000, -36.54856240481572],\n[1581777880000, -30.31006152436570],\n[1581777890000, -42.53321660981094],\n[1581777900000, -13.20427195011425],\n[1581777910000, -14.53546668748857],\n[1581777920000, -15.55725246006902],\n[1581777930000, -26.76065716597191],\n[1581777940000, -35.55599811602032]]\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = ExponentialSmoothing(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n    print(yhat)\n    print(time_diff)\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n', '8bb88a6c1c66af816e187f161f2bdae4', '2020-06-29 09:37:40', '2020-06-29 09:41:07'),
('5', 'scpt-ft_tran', 'FT转换函数', '转换函数脚本集', 'sset-ft_lib', 'ft_tran', '1', 'python', 'import pandas as pd\nimport numpy as np\nimport math\n# from functools import reduce\n# import traceback\n\n@DFF.API(\'ABS绝对值转换\', category=\'transformation\')\ndef abs_transformation(dps):\n    \'\'\'\n    返回数字的绝对值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").abs()\n    return tran.values.tolist()\n\n@DFF.API(\'AVG平均值转换\', category=\'transformation\')\ndef avg_transformation(dps):\n    \'\'\'\n    返回数字的平均值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").mean()\n    return tran.values.tolist()\n\n@DFF.API(\'MAX最大值转换\', category=\'transformation\')\ndef max_transformation(dps):\n    \'\'\'\n    返回数字的最大值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").max()\n    return tran.values.tolist()\n\n@DFF.API(\'MIN最小值转换\', category=\'transformation\')\ndef min_transformation(dps):\n    \'\'\'\n    返回数字的最小值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").min()\n    return tran.values.tolist()\n\n@DFF.API(\'Even向上取偶数转换\',category=\'transformation\')\ndef even_transformation(dps):\n    \'\'\'\n    将数字向上舍入为最接近的偶数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if math.ceil(df[1][i])%2==0:\n            df[1][i]=math.ceil(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])+1\n    return df.values.tolist()\n\n@DFF.API(\'Odd向上取奇数转换\',category=\'transformation\')\ndef odd_transformation(dps):\n    \'\'\'\n    将数字向上舍入为最接近的奇数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if math.ceil(df[1][i])%2!=0:\n            df[1][i]=math.ceil(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])+1\n    return df.values.tolist()\n\n@DFF.API(\'Fact取整阶乘转换\',category=\'transformation\')\ndef fact_transformation(dps):\n    \'\'\'\n    返回数字的阶乘。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        product = 1\n        for j in range(math.ceil(df[1][i])):\n            product=product*(j+1)\n        df[1][i]=product\n        # df[1][i]=math.factorial(math.ceil(df[1][i]))\n    return df.values.tolist()\n\n@DFF.API(\'SumSQ平方和转换\',category=\'transformation\')\ndef sumsq_transformation(dps):\n    \'\'\'\n    返回数字的平方和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    df[1]=list(map(lambda x:x**2*2,df[1]))\n    return df.values.tolist()\n\n\n@DFF.API(\'RoundUp绝对值增大取整转换\',category=\'transformation\')\ndef roundup_transformation(dps):\n    \'\'\'\n     向绝对值增大的方向舍入数字。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if df[1][i]<0:\n            df[1][i]=math.floor(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'RoundDown绝对值减小取整转换\',category=\'transformation\')\ndef rounddown_transformation(dps):\n    \'\'\'\n      向绝对值减小的方向舍入数字。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if df[1][i]>0:\n            df[1][i]=math.floor(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Power指数幂转换\',category=\'transformation\')\ndef power_transformation(dps,significance=1):\n    \'\'\'\n       返回给定次幂的结果。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        significance:指定指数\n    \'\'\'\n    try:\n        significance=float(significance)\n        df=pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i]**significance\n        return df.values.tolist()\n    except:\n        # info=traceback.format_exc()\n        info=significance+\' is not a number. \'\n        return info\n\n@DFF.API(\'Ceiling取整转换\',category=\'transformation\')\ndef ceiling_transformation(dps,significance):\n    \'\'\'\n    将数字四舍五入为最接近的整数或最接近的指定基数的倍数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        significance:指定基数的倍数\n    \'\'\'\n    try:\n        significance=float(significance)\n        df=pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            if (df[1][i]/significance)>=1:\n                df[1][i]=round(df[1][i]/significance)*significance\n            else:df[1][i]=(round(df[1][i]/significance)+1)*significance\n        return df.values.tolist()\n    except:\n        # info=traceback.format_exc()\n        info=significance+\' is not a number. \'\n        print(info)\n        return info\n\n@DFF.API(\'AccumuAll累加转换\',category=\'transformation\')\ndef accumuall_transformation(dps):\n    \'\'\'\n    将前n个元素累加求和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    total=0\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        total+=df[1][i]\n        df[1][i]=total\n    return df.values.tolist()\n\n@DFF.API(\'Accumu2求和转换\',category=\'transformation\')\ndef accumu2_transformation(dps):\n    \'\'\'\n    将每相邻2个元素累加求和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    m=df[1].copy()\n    for i in range(0,len(df[1])):\n        if i>0:\n            df[1][i]=m[i-1]+df[1][i]\n    return df.values.tolist()\n\n@DFF.API(\'ACOSH反双曲余弦转换\',category=\'transformation\')\ndef acosh_transformation(dps):\n    \'\'\'\n    返回数字的反双曲余弦值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    #acosh(number),number必须大于/等于1\n    df[1]=df[1].abs()\n    for i in range(0,len(df[1])):\n        if df[1][i]<1:\n            df[1][i]=math.acosh(df[1][i]+1)\n        else:\n             df[1][i]=math.acosh(df[1][i])\n    return df.values.tolist()\n\n\n\ndef test_acosh():\n    dps = [[1576765000100, 20.3], [1576765000200, -33.5], [1576765000300, 25.8]]\n    return acosh_transformation(dps)\n\ndef test_accumu2():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return accumu2_transformation(dps)\n\n\ndef test_abs():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return abs_transformation(dps)\n\ndef test_avg():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return avg_transformation(dps)\n\ndef test_max():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return max_transformation(dps)\n\ndef test_min():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return min_transformation(dps)\n\ndef test_even():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return even_transformation(dps)\n\ndef test_odd():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return odd_transformation(dps)\n\ndef test_fact():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return fact_transformation(dps)\n\ndef test_sumsq():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return sumsq_transformation(dps)\n\ndef test_roundup():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return roundup_transformation(dps)\n\ndef test_rounddown():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return rounddown_transformation(dps)\ndef test_power():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    a=\"2\"\n    return power_transformation(dps,a)\n\ndef test_ceiling():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return ceiling_transformation(dps,2)\n\ndef test_accumall():\n    dps = [[1576765000100, 20.3], [1576765000200, 50], [1576765000300, 25.8]]\n    return accumuall_transformation(dps)\n\n\n\n\n', '3f5018265f67e82fe69dc38628dc6426', 'import pandas as pd\nimport numpy as np\nimport math\n# from functools import reduce\n# import traceback\n\n@DFF.API(\'ABS绝对值转换\', category=\'transformation\')\ndef abs_transformation(dps):\n    \'\'\'\n    返回数字的绝对值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").abs()\n    return tran.values.tolist()\n\n@DFF.API(\'AVG平均值转换\', category=\'transformation\')\ndef avg_transformation(dps):\n    \'\'\'\n    返回数字的平均值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").mean()\n    return tran.values.tolist()\n\n@DFF.API(\'MAX最大值转换\', category=\'transformation\')\ndef max_transformation(dps):\n    \'\'\'\n    返回数字的最大值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").max()\n    return tran.values.tolist()\n\n@DFF.API(\'MIN最小值转换\', category=\'transformation\')\ndef min_transformation(dps):\n    \'\'\'\n    返回数字的最小值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").min()\n    return tran.values.tolist()\n\n@DFF.API(\'Even向上取偶数转换\',category=\'transformation\')\ndef even_transformation(dps):\n    \'\'\'\n    将数字向上舍入为最接近的偶数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if math.ceil(df[1][i])%2==0:\n            df[1][i]=math.ceil(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])+1\n    return df.values.tolist()\n\n@DFF.API(\'Odd向上取奇数转换\',category=\'transformation\')\ndef odd_transformation(dps):\n    \'\'\'\n    将数字向上舍入为最接近的奇数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if math.ceil(df[1][i])%2!=0:\n            df[1][i]=math.ceil(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])+1\n    return df.values.tolist()\n\n@DFF.API(\'Fact取整阶乘转换\',category=\'transformation\')\ndef fact_transformation(dps):\n    \'\'\'\n    返回数字的阶乘。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        product = 1\n        for j in range(math.ceil(df[1][i])):\n            product=product*(j+1)\n        df[1][i]=product\n        # df[1][i]=math.factorial(math.ceil(df[1][i]))\n    return df.values.tolist()\n\n@DFF.API(\'SumSQ平方和转换\',category=\'transformation\')\ndef sumsq_transformation(dps):\n    \'\'\'\n    返回数字的平方和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    df[1]=list(map(lambda x:x**2*2,df[1]))\n    return df.values.tolist()\n\n\n@DFF.API(\'RoundUp绝对值增大取整转换\',category=\'transformation\')\ndef roundup_transformation(dps):\n    \'\'\'\n     向绝对值增大的方向舍入数字。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if df[1][i]<0:\n            df[1][i]=math.floor(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'RoundDown绝对值减小取整转换\',category=\'transformation\')\ndef rounddown_transformation(dps):\n    \'\'\'\n      向绝对值减小的方向舍入数字。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if df[1][i]>0:\n            df[1][i]=math.floor(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Power指数幂转换\',category=\'transformation\')\ndef power_transformation(dps,significance=1):\n    \'\'\'\n       返回给定次幂的结果。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        significance:指定指数\n    \'\'\'\n    try:\n        significance=float(significance)\n        df=pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i]**significance\n        return df.values.tolist()\n    except:\n        # info=traceback.format_exc()\n        info=significance+\' is not a number. \'\n        return info\n\n@DFF.API(\'Ceiling取整转换\',category=\'transformation\')\ndef ceiling_transformation(dps,significance):\n    \'\'\'\n    将数字四舍五入为最接近的整数或最接近的指定基数的倍数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        significance:指定基数的倍数\n    \'\'\'\n    try:\n        significance=float(significance)\n        df=pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            if (df[1][i]/significance)>=1:\n                df[1][i]=round(df[1][i]/significance)*significance\n            else:df[1][i]=(round(df[1][i]/significance)+1)*significance\n        return df.values.tolist()\n    except:\n        # info=traceback.format_exc()\n        info=significance+\' is not a number. \'\n        print(info)\n        return info\n\n@DFF.API(\'AccumuAll累加转换\',category=\'transformation\')\ndef accumuall_transformation(dps):\n    \'\'\'\n    将前n个元素累加求和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    total=0\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        total+=df[1][i]\n        df[1][i]=total\n    return df.values.tolist()\n\n@DFF.API(\'Accumu2求和转换\',category=\'transformation\')\ndef accumu2_transformation(dps):\n    \'\'\'\n    将每相邻2个元素累加求和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    m=df[1].copy()\n    for i in range(0,len(df[1])):\n        if i>0:\n            df[1][i]=m[i-1]+df[1][i]\n    return df.values.tolist()\n\n@DFF.API(\'ACOSH反双曲余弦转换\',category=\'transformation\')\ndef acosh_transformation(dps):\n    \'\'\'\n    返回数字的反双曲余弦值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    #acosh(number),number必须大于/等于1\n    df[1]=df[1].abs()\n    for i in range(0,len(df[1])):\n        if df[1][i]<1:\n            df[1][i]=math.acosh(df[1][i]+1)\n        else:\n             df[1][i]=math.acosh(df[1][i])\n    return df.values.tolist()\n\n\n\ndef test_acosh():\n    dps = [[1576765000100, 20.3], [1576765000200, -33.5], [1576765000300, 25.8]]\n    return acosh_transformation(dps)\n\ndef test_accumu2():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return accumu2_transformation(dps)\n\n\ndef test_abs():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return abs_transformation(dps)\n\ndef test_avg():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return avg_transformation(dps)\n\ndef test_max():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return max_transformation(dps)\n\ndef test_min():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return min_transformation(dps)\n\ndef test_even():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return even_transformation(dps)\n\ndef test_odd():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return odd_transformation(dps)\n\ndef test_fact():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return fact_transformation(dps)\n\ndef test_sumsq():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return sumsq_transformation(dps)\n\ndef test_roundup():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return roundup_transformation(dps)\n\ndef test_rounddown():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return rounddown_transformation(dps)\ndef test_power():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    a=\"2\"\n    return power_transformation(dps,a)\n\ndef test_ceiling():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return ceiling_transformation(dps,2)\n\ndef test_accumall():\n    dps = [[1576765000100, 20.3], [1576765000200, 50], [1576765000300, 25.8]]\n    return accumuall_transformation(dps)\n\n\n\n\n', '3f5018265f67e82fe69dc38628dc6426', '2020-06-29 09:37:40', '2020-06-29 09:41:07'),
('6', 'scpt-ft_tran_str', 'FT转换函数-文本转换', '文本转换用于字符串处理', 'sset-ft_lib', 'ft_tran_str', '1', 'python', 'import pandas as pd\n\n@DFF.API(\'Clean删除不可打印字符\',category=\'transformation\')\ndef clean_transformation(dps):\n    \'\'\'\n    从文本中删除不可打印的字符，去掉了前31个特殊的ASCII字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        word = \'\'\n        for j in df[1][i]:\n            if ord(j)>31:\n                word +=j\n        print(df[1][i])\n        df[1][i]=word\n        print(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Fixed数字格式化\',category=\'transformation\')\ndef fixed_transformation(dps,no_commas=\'True\',decimals=2):\n    \'\'\'\n    将数字格式设置为具有固定小数位数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        decimals:进行四舍五入后小数点后面需要保留的位数,如果是负数，则在小数点左侧进行四舍五入。若此参数                         decimals省略，则默认此参数为2。\n        no_commas:如果是 true，返回的结果不包含逗号千分位。如果是false或省略不写，返回的结果包含逗号千分位。\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        decimals=int(decimals)\n        for i in range(0,len(df[1])):\n            df[1][i]=round(df[1][i],decimals)\n            if no_commas.title()==\'False\':\n                df[1][i]=\'{:,}\'.format(df[1][i])\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'Left截取左边字符\',category=\'transformation\')\ndef left_transformation(dps,num_chars=1):\n    \'\'\'\n    返回文本值中最左边的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        num_chars:需要截取字符的个数。默认截取1个\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][0:num_chars]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'Right截取右边字符\',category=\'transformation\')\ndef right_transformation(dps,num_chars=None):\n    \'\'\'\n    返回文本值中最右边的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        num_chars:需要截取字符的个数。默认截取1个\n    \'\'\'\n    try:\n        if num_chars is None:\n            num_chars = 1\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][-num_chars:]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'LEN字符串长度\',category=\'transformation\')\ndef len_transformation(dps):\n    \'\'\'\n    返回文本字符串中的字符数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        df[1][i]=len(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Lower字符小写转换\',category=\'transformation\')\ndef lower_transformation(dps):\n    \'\'\'\n    将文本转换为小写。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        df[1][i]=df[1][i].lower()\n    return df.values.tolist()\n\n@DFF.API(\'MID截取字符\',category=\'transformation\')\ndef mid_transformation(dps,start_num=1,num_chars=1,):\n    \'\'\'\n     在文本字符串中,从您所指定的位置开始返回特定数量的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        start_num:从左侧第几位开始截取。\n        num_chars:需要截取字符的个数。默认截取1个。\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        start_num=int(start_num)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][start_num-1:start_num-1+num_chars]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'PROPER首字母转大写\',category=\'transformation\')\ndef proper_transformation(dps,sep=None):\n    \'\'\'\n    将字符串的首字母转为大写，其余变为小写规范化输出\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        sep【默认是空格】：要转换的文本的切分符\n    \'\'\'\n    try:\n        if sep is None:\n            sep = \' \'\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            arr = df[1][i].split(sep)\n            df[1][i]=\' \'.join(list(map(normallize,arr)))\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'REPLACE替换目标字符\',category=\'transformation\')\ndef replace_transformation(dps,target,replace):\n    \'\'\'\n    替换字符串中的目标字符\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        target：被替换的字符\n        replace:要替换的字符\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].replace(target,replace)\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'REPT复制目标文本\',category=\'transformation\')\ndef rept_transformation(dps,target,num):\n    \'\'\'\n    复制目标文本追加到文本末尾\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        target：要复制的字符\n        num:复制次数\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i] + target*int(num)\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'SUBSTITUTE替换文本\',category=\'transformation\')\ndef substitute_transformation(dps,replace):\n    \'\'\'\n    在文本字符串中用新文本替换旧文本\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        replace：进行替换的字符\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= replace\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'TRIM去除文本空格\',category=\'transformation\')\ndef trim_transformation(dps):\n    \'\'\'\n    去除文本中空格\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].replace(\' \',\'\')\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'UPPER将文本转为大写\',category=\'transformation\')\ndef upper_transformation(dps):\n    \'\'\'\n    将文本转为大写\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].upper()\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\ndef normallize(name):\n    return name.capitalize()\n\n\ndef test_proper():\n    dps = [[1576765000000, \"sfoaj fajfioaj\"], [1576765001000, \"qeAja afq\"], [1576765002000, \"afakjfa fafa\"], [1576765003000, \"aot ha\"],[1576765004000, \"faq fa\"], [1576765005000, \"bafla ajw\"], [1576765006000, \"afkaj qop\"], [1576765007000, \"fajqq jlj\"],[1576765008000, \"dga afw\"], [1576765009000, \"dfa awo\"], [1576765010000, \"qwk weqb\"], [1576765011000, \"fakljqv bse\"]]\n    # return proper_transformation(dps)\n    # return replace_transformation(dps,\'fa\',\'AA\')\n    # return right_transformation(dps, \'3\')\n    # return rept_transformation(dps,\"AA\",\"2\")\n    # return substitute_transformation(dps,\"aaaa\")\n    # return trim_transformation(dps)\n    return upper_transformation(dps)\n\ndef test_mid():\n    dps = [[1576765000100, \"111DTTTTTSd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return mid_transformation(dps,1,3)\n\ndef test_lower():\n    dps = [[1576765000100, \"111DTTTTTSd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return lower_transformation(dps)\n\ndef test_len():\n    dps = [[1576765000100, \"111Dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return len_transformation(dps)\n\ndef test_left():\n    dps = [[1576765000100, \"111dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss\\t等等\']]\n    return left_transformation(dps,\'3\')\ndef test_clean():\n    dps = [[1576765000100, \"111dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss\\t等等\']]\n    return clean_transformation(dps)\n\ndef test_fixed():\n    dps = [[1576765000100, 438272.7651], [1576765000200, 43318272.7651], [1576765000300, 43823472.51]]\n    return fixed_transformation(dps,\"false\",\"1\")\n\n\n\n\n\n\n', '961549ae168cd3d5933063c87881b918', 'import pandas as pd\n\n@DFF.API(\'Clean删除不可打印字符\',category=\'transformation\')\ndef clean_transformation(dps):\n    \'\'\'\n    从文本中删除不可打印的字符，去掉了前31个特殊的ASCII字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        word = \'\'\n        for j in df[1][i]:\n            if ord(j)>31:\n                word +=j\n        print(df[1][i])\n        df[1][i]=word\n        print(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Fixed数字格式化\',category=\'transformation\')\ndef fixed_transformation(dps,no_commas=\'True\',decimals=2):\n    \'\'\'\n    将数字格式设置为具有固定小数位数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        decimals:进行四舍五入后小数点后面需要保留的位数,如果是负数，则在小数点左侧进行四舍五入。若此参数                         decimals省略，则默认此参数为2。\n        no_commas:如果是 true，返回的结果不包含逗号千分位。如果是false或省略不写，返回的结果包含逗号千分位。\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        decimals=int(decimals)\n        for i in range(0,len(df[1])):\n            df[1][i]=round(df[1][i],decimals)\n            if no_commas.title()==\'False\':\n                df[1][i]=\'{:,}\'.format(df[1][i])\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'Left截取左边字符\',category=\'transformation\')\ndef left_transformation(dps,num_chars=1):\n    \'\'\'\n    返回文本值中最左边的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        num_chars:需要截取字符的个数。默认截取1个\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][0:num_chars]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'Right截取右边字符\',category=\'transformation\')\ndef right_transformation(dps,num_chars=None):\n    \'\'\'\n    返回文本值中最右边的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        num_chars:需要截取字符的个数。默认截取1个\n    \'\'\'\n    try:\n        if num_chars is None:\n            num_chars = 1\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][-num_chars:]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'LEN字符串长度\',category=\'transformation\')\ndef len_transformation(dps):\n    \'\'\'\n    返回文本字符串中的字符数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        df[1][i]=len(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Lower字符小写转换\',category=\'transformation\')\ndef lower_transformation(dps):\n    \'\'\'\n    将文本转换为小写。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        df[1][i]=df[1][i].lower()\n    return df.values.tolist()\n\n@DFF.API(\'MID截取字符\',category=\'transformation\')\ndef mid_transformation(dps,start_num=1,num_chars=1,):\n    \'\'\'\n     在文本字符串中,从您所指定的位置开始返回特定数量的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        start_num:从左侧第几位开始截取。\n        num_chars:需要截取字符的个数。默认截取1个。\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        start_num=int(start_num)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][start_num-1:start_num-1+num_chars]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'PROPER首字母转大写\',category=\'transformation\')\ndef proper_transformation(dps,sep=None):\n    \'\'\'\n    将字符串的首字母转为大写，其余变为小写规范化输出\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        sep【默认是空格】：要转换的文本的切分符\n    \'\'\'\n    try:\n        if sep is None:\n            sep = \' \'\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            arr = df[1][i].split(sep)\n            df[1][i]=\' \'.join(list(map(normallize,arr)))\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'REPLACE替换目标字符\',category=\'transformation\')\ndef replace_transformation(dps,target,replace):\n    \'\'\'\n    替换字符串中的目标字符\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        target：被替换的字符\n        replace:要替换的字符\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].replace(target,replace)\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'REPT复制目标文本\',category=\'transformation\')\ndef rept_transformation(dps,target,num):\n    \'\'\'\n    复制目标文本追加到文本末尾\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        target：要复制的字符\n        num:复制次数\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i] + target*int(num)\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'SUBSTITUTE替换文本\',category=\'transformation\')\ndef substitute_transformation(dps,replace):\n    \'\'\'\n    在文本字符串中用新文本替换旧文本\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        replace：进行替换的字符\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= replace\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'TRIM去除文本空格\',category=\'transformation\')\ndef trim_transformation(dps):\n    \'\'\'\n    去除文本中空格\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].replace(\' \',\'\')\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'UPPER将文本转为大写\',category=\'transformation\')\ndef upper_transformation(dps):\n    \'\'\'\n    将文本转为大写\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].upper()\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\ndef normallize(name):\n    return name.capitalize()\n\n\ndef test_proper():\n    dps = [[1576765000000, \"sfoaj fajfioaj\"], [1576765001000, \"qeAja afq\"], [1576765002000, \"afakjfa fafa\"], [1576765003000, \"aot ha\"],[1576765004000, \"faq fa\"], [1576765005000, \"bafla ajw\"], [1576765006000, \"afkaj qop\"], [1576765007000, \"fajqq jlj\"],[1576765008000, \"dga afw\"], [1576765009000, \"dfa awo\"], [1576765010000, \"qwk weqb\"], [1576765011000, \"fakljqv bse\"]]\n    # return proper_transformation(dps)\n    # return replace_transformation(dps,\'fa\',\'AA\')\n    # return right_transformation(dps, \'3\')\n    # return rept_transformation(dps,\"AA\",\"2\")\n    # return substitute_transformation(dps,\"aaaa\")\n    # return trim_transformation(dps)\n    return upper_transformation(dps)\n\ndef test_mid():\n    dps = [[1576765000100, \"111DTTTTTSd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return mid_transformation(dps,1,3)\n\ndef test_lower():\n    dps = [[1576765000100, \"111DTTTTTSd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return lower_transformation(dps)\n\ndef test_len():\n    dps = [[1576765000100, \"111Dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return len_transformation(dps)\n\ndef test_left():\n    dps = [[1576765000100, \"111dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss\\t等等\']]\n    return left_transformation(dps,\'3\')\ndef test_clean():\n    dps = [[1576765000100, \"111dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss\\t等等\']]\n    return clean_transformation(dps)\n\ndef test_fixed():\n    dps = [[1576765000100, 438272.7651], [1576765000200, 43318272.7651], [1576765000300, 43823472.51]]\n    return fixed_transformation(dps,\"false\",\"1\")\n\n\n\n\n\n\n', '961549ae168cd3d5933063c87881b918', '2020-06-29 09:37:40', '2020-06-29 09:41:07');
INSERT INTO `biz_main_script` (`seq`, `id`, `title`, `description`, `scriptSetId`, `refName`, `publishVersion`, `type`, `code`, `codeMD5`, `codeDraft`, `codeDraftMD5`, `createTime`, `updateTime`) VALUES
('7', 'scpt-level_shift_chk', 'FT水位检测', NULL, 'sset-ft_lib', 'level_shift_chk', '1', 'python', '\"\"\"\n水位检测\n\n注：目前调度是整分钟的01秒调用，根据InfluxQL里的聚合，如果按分钟聚合，最后一个数据点是该分钟聚合这一秒的数据（往往是缺失）\n目前的处理：最后一个数据点为空，则剔除，其他数据点的空值填补成0\n\n2020-06-19  解析参数，验证参数，获取数据，处理数据&异常检测，如果触发事件则利用dataway行协议写入__keyevent\n\n2020-06-28 新增支持 告警功能 & 模板变量 功能\n\"\"\"\n\n\nimport hashlib\nimport re\nimport time\nimport datetime\nimport arrow\nimport simplejson as json\nimport six\nimport pandas as pd\nfrom adtk.data import validate_series\nfrom adtk.detector import PersistAD\n\n_DFF_is_debug = True\nLEVEL_NAME_MAP = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nOK_LEVEL = \'ok\'\nCONDITION_LOGICS = (\'and\', \'or\')\nCONDITION_OPERATORS = \\\n    (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nALL_LEVELS = (\'critical\', \'error\', \'warning\', \'info\')\nACTION_TYPES = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS = (\'GET\', \'POST\')\nHTTP_BODY_TYPES = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\nMOCK_DATA = None\nRAISE_DATAWAY_EXCEPTION = True\n\n# 基础函数\n\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n    return six.ensure_str(s)\n\n\n# 以下函数为 模板变量功能 & 进阶模板功能 所需要的功能函数\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from = from_unit.lower()\n    _search_to = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n                                  carry=1000)\n\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n                                  carry=1024)\n\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n                                  carry=1024)\n\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n                                  carry=1024)\n\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\': __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\': __filter_volumn,\n    \'volumni\': __filter_volumn_i,\n    \'bitrate\': __filter_bit_rate,\n    \'byterate\': __filter_byte_rate,\n    \'upper\': __filter_upper,\n    \'lower\': __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\': __filter_if_true,\n    \'iffalse\': __filter_if_false,\n}\n\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, six.string_types):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result = DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\n\n# 步骤1：参数校验\ndef __param_targets_check(targets):\n    if not isinstance(targets, list):\n        raise Exception(\'`targets` should be a list\')\n\n    for i, t in enumerate(targets):\n        if not isinstance(t, dict):\n            raise Exception(\'`targets[{}]` should be a dict\'.format(i))\n\n        if not isinstance(t.get(\'alias\'), str):\n            raise Exception(\'`targets[{}][\"alias\"]` should be a str\'.format(i))\n\n        if \'influxQL\' in t and not isinstance(t.get(\'influxQL\'), str):\n            raise Exception(\'`targets[{}][\"influxQL\"]` should be a str\'.format(i))\n\n        if \'expression\' in t and not isinstance(t.get(\'expression\'), str):\n            raise Exception(\'`targets[{}][\"expression\"]` should be a str\'.format(i))\n\n\ndef __param_rules_check(rules):\n    if not isinstance(rules, list):\n        raise Exception(\'`rules` should be a list\')\n\n    for i, r in enumerate(rules):\n        if not isinstance(r, dict):\n            raise Exception(\'`rules[{}]` should be a dict\'.format(i))\n\n        if r.get(\'level\') not in ALL_LEVELS:\n            raise Exception(\'`rules[{}][\"level\"]` should be one of {}\'.format(i, \',\'.join(ALL_LEVELS)))\n\n        if not isinstance(r.get(\'triggerCondition\'), dict):\n            raise Exception(\'`rules[{}][\"triggerCondition\"]` should be a dict\'.format(i))\n\n        c = r[\'triggerCondition\']\n        if not isinstance(c.get(\'periodNum\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"periodNum\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'strength\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"strength\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'direction\'), str):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be a str\'.format(i))\n\n        d = c.get(\'direction\')\n        if d not in [\'up\', \'down\', \'both\']:\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be `up` or `down` or `both`\'.format(i))\n\n        if not isinstance(c.get(\'checkCount\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"checkCount\"]` should be an int\'.format(i))\n\n\ndef __param_action_check(actions, parent_path):\n    if not isinstance(actions, list):\n        raise Exception(\'`{}actions` should be a list\'.format(parent_path))\n\n    for i, a in enumerate(actions):\n        if not isinstance(a, dict):\n            raise Exception(\'`{}actions[{}]` should be a dict\'.format(parent_path, i))\n\n        if a.get(\'type\') not in ACTION_TYPES:\n            raise Exception(\n                \'`{}actions[{}][\"type\"]` should be one of {}\'.format(parent_path, i, \',\'.join(ACTION_TYPES)))\n\n        if a[\'type\'] == \'mail\':\n            if not isinstance(a.get(\'to\'), list):\n                raise Exception(\'`{}actions[{}][\"to\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'to\'])):\n                raise Exception(\'Element of `{}actions[{}][\"to\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'dingTalkRobot\':\n            if not isinstance(a.get(\'webhook\'), list):\n                raise Exception(\'`{}actions[{}][\"webhook\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'webhook\'])):\n                raise Exception(\'Element of `{}actions[{}][\"webhook\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'markdown\'), str):\n                raise Exception(\'`{}actions[{}][\"markdown\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'ProfWangAlertHub\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'suggestion\') is not None:\n                if not isinstance(a[\'suggestion\'], str):\n                    raise Exception(\'`{}actions[{}][\"suggestion\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'HTTPRequest\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'method\') not in HTTP_METHODS:\n                raise Exception(\n                    \'`{}actions[{}][\"method\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_METHODS)))\n\n            if a.get(\'headers\') is not None:\n                if not isinstance(a[\'headers\'], dict):\n                    raise Exception(\'`{}actions[{}][\"headers\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].values())):\n                    raise Exception(\'Value of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n            if a[\'method\'] == \'POST\':\n                if a.get(\'bodyType\') not in HTTP_BODY_TYPES:\n                    raise Exception(\'`{}actions[{}][\"bodyType\"]` should be one of {}\'.format(parent_path, i,\n                                                                                             \',\'.join(HTTP_BODY_TYPES)))\n\n                if not isinstance(a.get(\'body\'), str):\n                    raise Exception(\'`{}actions[{}][\"body\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'DataFluxFunc\':\n            if not isinstance(a.get(\'scriptSetRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptSetRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'scriptRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'funcRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"funcRefName\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'kwargs\') is not None:\n                if not isinstance(a[\'kwargs\'], dict):\n                    raise Exception(\'`{}actions[{}][\"kwargs\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'kwargs\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"kwargs\"]` should be a str\'.format(parent_path, i))\n\n\ndef _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    # 1. 检查`trigger_info`\n    if not isinstance(trigger_info, dict):\n        raise Exception(\'`trigger_info` should be a dict\')\n\n    if not isinstance(trigger_info.get(\'uuid\'), str):\n        raise Exception(\'`trigger_info[\"uuid\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'name\'), str):\n        raise Exception(\'`trigger_info[\"name\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'workspaceToken\'), str):\n        raise Exception(\'`trigger_info[\"workspaceToken\"]` should be a str\')\n\n    dimensions = trigger_info.get(\'dimensions\')\n    if dimensions is not None:\n        if not isinstance(dimensions, list):\n            raise Exception(\'`trigger_info[\"dimensions\"]` should be a list\')\n\n        for index, dimension in enumerate(dimensions):\n            if not isinstance(dimension, str):\n                raise Exception(\'`trigger_info[\"dimensions\"][{}]` should be a str\'.format(index))\n\n    # 2. 检查`targets`\n    __param_targets_check(targets)\n\n    # 3. 检查`condition_check_setting`\n    if not isinstance(condition_check_setting, dict):\n        raise Exception(\'`condition_check_setting` should be a dict\')\n\n    __param_rules_check(condition_check_setting.get(\'rules\'))\n    __param_action_check(condition_check_setting.get(\'actions\'), parent_path=\'condition_check_setting.\')\n\n    # 4. 检查`all_time`\n    if not isinstance(all_time, dict):\n        raise Exception(\'`all_time` should be a dict\')\n\n    if not isinstance(all_time.get(\'every_time\'), str):\n        raise Exception(\'`all_time[\"every_time\"]` should be a str\')\n\n    if not isinstance(all_time.get(\'group_time\'), str):\n        raise Exception(\'`all_time[\"group_time\"]` should be a str\')\n\n    # 正则表达式 匹配 *s, *m, *h, *d, *w, *M (秒，分，时，天，周，月)\n    match_every_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'every_time\'))\n    if not (match_every_time is not None and match_every_time.span(0)[1] == len(all_time.get(\'every_time\'))):\n        raise Exception(\'`all_time[\"every_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    match_group_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'group_time\'))\n    if not (match_group_time is not None and match_group_time.span(0)[1] == len(all_time.get(\'group_time\'))):\n        raise Exception(\'`all_time[\"group_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    # 5. 检查 `extra_alert_tags`\n    if extra_alert_tags is not None:\n        if not isinstance(extra_alert_tags, dict):\n            raise Exception(\'`extra_alert_tags` should be an dict\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.keys())):\n            raise Exception(\'Key of `extra_alert_tags` should be a str\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.values())):\n            raise Exception(\'Value of `extra_alert_tags` should be a str\')\n\n\n# 步骤2：获取数据\ndef _to_iso_time(d=None):\n    # 将unix时间戳转化成iso8601时间格式\n    return arrow.get(d).isoformat()\n\n\ndef _get_data(targets):\n\n    # 根据targets里的InfluxQL语句，获取数据，targets是个列表，目前仅支持单个target，所以取第一个\n    target = targets[0]\n    db_res = None\n    if MOCK_DATA:\n        db_res = MOCK_DATA[target[\'alias\']]\n    else:\n        bind_params = {}\n        # 转成真实触发时间\n        if \'$now\' in target[\'influxQL\']:\n            bind_params[\'now\'] = _to_iso_time(int(time.time()))\n\n        db_res = DFF.SRC(\'df_influxdb\').query(target[\'influxQL\'], bind_params=bind_params)\n\n    # 最终返回的valid_series_data是 由多个dict组成的list\n    valid_series_data = []\n    if \'series\' in db_res:\n        for s in db_res[\'series\']:\n            if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                continue\n\n            valid_series_data.append(s)\n\n    return valid_series_data\n\n\n# 步骤3：异常检测\n# 第二步返回的数据结果是 list of dict，第3步将遍历每一个dict（每个dict就是一条时间序列），进行单时间序列的异常检测\n\n\ndef _preprocessing(data):\n    \"\"\"\n    data为dict类型数据，处理成pd.Series\n    如果最后一个数据点为空，则剔除，其余为空则补0\n    \"\"\"\n    print(data)\n    df = pd.DataFrame(data[\'values\'])\n    df.index = df[0].apply(lambda x: x.replace(\'T\', \' \').replace(\'Z\', \'\'))\n    df = df[1]\n    df.index = pd.to_datetime(df.index)\n    df = df.astype(float)\n    if df.isnull().iloc[-1]:\n        df = df.iloc[:-1]\n    df.fillna(0, inplace=True)\n    print(df)\n    return df\n\n\ndef _level_shift_check(df, trigger_condition):\n    \"\"\"\n    水位检测，用于检测连续频繁的异常点，对于偶发的突变点不敏感\n    注：目前是 使用历史 30 个点，计算四分位距，中位数median，\n\n    :param df: pd.Series. 要检测的时间序列数据 index: datetime  value: float\n    :param trigger_condition: dict. 异常检测器的参数设置\n\n    :return: None or dict.\n    \"\"\"\n\n    # validate_series 是 adtk包的函数，可以对时序pandas数据进行预处理\n    df = validate_series(df)\n    # 检测强度，强度越高，对于异常值的检测越苛刻\n    c_dict = {1: 1, 2: 2, 3: 3}\n    c = c_dict[trigger_condition[\'strength\']]\n    if trigger_condition[\'direction\'] == \'up\':\n        side = \'positive\'\n    elif trigger_condition[\'direction\'] == \'down\':\n        side = \'negative\'\n    else:\n        side = \'both\'\n    check_count = trigger_condition[\'checkCount\']\n    window = min(100, max(10, check_count * 5))\n    ad = PersistAD(window=window, c=c, side=side)\n    anomalies = ad.fit_detect(df)\n\n    # 只检测最近的 point_num 个点\n    point_num = int(trigger_condition[\'periodNum\'])\n    anomalies = anomalies.iloc[-point_num:]\n\n    # 只有 连续的 anomalies个数 达到 checkCount时，才当做关键事件\n    # 刚好触达 checkCount的那个异常点所在的时刻 定义为 关键事件的时刻\n    # 持续时间：单位为微秒\n    temp = 0\n    dps = []\n    ts = None\n    for k, v in anomalies.items():\n        if temp == check_count:\n            ts = dps[-1][0]\n            break\n        if v != 1:\n            temp = 0\n            dps = []\n        else:\n            dps.append([int(k.timestamp()), df[k]])\n            temp += 1\n\n    if ts is None:\n        res = None\n    else:\n        res = {\n            \'timestamp\': ts,\n            \'anomalies_dps\': dps,\n            \'duration\': int((dps[-1][0] - dps[0][0]) * 1000 * 1000),\n            \'anomaly\': dps[-1][1]\n        }\n        print(res)\n    return res\n\n\n# 步骤4：生成关键事件，和告警内容；存入关键事件，实施告警\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\n\ndef _get_event_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    return _get_md5(str_to_md5)\n\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, event_item_tags, event_id, duration, check_value_map=None,\n                     prev_level=None):\n    \"\"\"\n    创建变量字典\n    相关变量定义参考 模板变量文档\n    https://help.dataflux.cn/doc/1ee7e06f0f06162dd3ee3bd834611b6f0c3100d8\n\n    注：目前prev_level默认缺失，即默认为 OK\n    \"\"\"\n\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\': level,\n        \'status\': level,\n        \'prevLevel\': prev_level,\n        \'prevStatus\': prev_level,\n        \'levelName\': LEVEL_NAME_MAP[level],\n        \'statusName\': LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'prevStatusName\': prev_level_name,\n        \'tags\': _json_dumps(event_item_tags),\n        \'tagsEscaped\': None,\n        \'fields\': None,\n        \'fieldsEscaped\': None,\n        \'duration\': duration * 1000 if duration is not None else None,\n        \'durationHuman\': None,\n        \'ruleId\': trigger_uuid,\n        \'ruleName\': trigger_name,\n        \'alertId\': event_id,\n        \'eventId\': event_id\n    }\n\n    # 补全tags.KEY\n    for k, v in event_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全 fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全 fieldsEscaped、tagsEscaped\n    # 在json结构的string类型值里面再放tags, fields 的 json, 需要转义\n    # 如果人家填写的整个body是 {\"tags\": \"#{tags}\"} ，最终结果会变成\n    # {\"tags\": \"{\"a\": 1}\"}，这个是错误的json结构\n    # 因此提供了转义后的json字符串，保证上述例子输出为 {\"tags\": \"{\\\"a\\\": 1}\"}\n    # 如果人家body填的是 {\"tags\":  # {tags} } 就没事，最终结果为 {\"tags\": {\"a\": 1}}\n\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    if duration is not None:\n        _m, _s = divmod(duration, 60)\n        _h, _m = divmod(_m, 60)\n        _d, _h = divmod(_h, 24)\n        _human = \'{}秒\'.format(_s)\n        if _m > 0:\n            _human = \'{}分钟\'.format(_m) + _human\n        if _h > 0:\n            _human = \'{}小时\'.format(_h) + _human\n        if _d > 0:\n            _human = \'{}天\'.format(_d) + _human\n\n        var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to = action[\'to\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\': to,\n            \'title\': title,\n            \'content\': {\'text\': content},\n            \'sender\': \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook = action[\'webhook\']\n    title = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\': title,\n                \'text\': markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url = action[\'url\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\': var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\': profwang_level,\n            \'type\': \'alarm\',\n            \'title\': title,\n            \'content\': content,\n            \'suggestion\': suggestion,\n            \'origin\': \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': \'POST\',\n                \'url\': _url,\n                \'body\': _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    url = action[\'url\']\n    method = action[\'method\'].upper()\n    headers = action.get(\'headers\')\n    body = action.get(\'body\') or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': method,\n                \'url\': _url,\n                \'headers\': headers,\n                \'query\': None,\n                \'body\': body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'url\': url,\n            \'method\': method,\n            \'headers\': headers,\n            \'body\': body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name = action[\'scriptRefName\']\n    func_ref_name = action[\'funcRefName\']\n    kwargs = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\': script_ref_name,\n            \'funcRefName\': func_ref_name,\n            \'kwargs\': kwargs,\n        }),\n    }\n    return action_content, None\n\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\': _create_action_debug,\n    \'mail\': _create_action_mail,\n    \'dingTalkRobot\': _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\': _create_action_http_request,\n    \'DataFluxFunc\': _create_action_dataflux_func,\n}\n\n\ndef _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level, event_item_tags,\n                                  event_id, duration, check_value_map, extra_alert_tags=None, is_no_data=False):\n    trigger_uuid = trigger_info[\'uuid\']\n    trigger_name = trigger_info[\'name\']\n    # dimensions = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    var_dict = _create_var_dict(\n        trigger_uuid=trigger_uuid,\n        trigger_name=trigger_name,\n        level=level,\n        prev_level=prev_level,\n        event_item_tags=event_item_tags,\n        event_id=event_id,\n        duration=duration,\n        check_value_map=check_value_map)\n\n    for action in actions:\n        action_type = action[\'type\']\n\n        # 记录执行动作\n        _action = _json_copy(action)\n        action_content = None\n        action_task = None\n        try:\n            action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n        except Exception as e:\n            print(e)\n        else:\n            if action_task:\n                if isinstance(action_task, list):\n                    actions_to_perform.extend(action_task)\n                else:\n                    actions_to_perform.append(action_task)\n\n        # 记录写入__keyevent\n        print(action_content)\n        _title = action_content[\'title\']\n        _content = action_content.get(\'content\') or None\n        # _suggestion = action_content.get(\'suggestion\') or None\n        # _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n        # _keyevent_type = None\n        # if is_no_data:\n        #     _keyevent_type = \'noData\'\n\n        tags = {\n            \'__status\': level,\n            \'__eventId\': event_id,\n            \'__source\': \'datafluxTrigger\',\n            \'__ruleId\': trigger_uuid,\n            \'__ruleName\': trigger_name,\n            \'__actionType\': action_type\n        }\n        if extra_alert_tags:\n            tags.update(extra_alert_tags)\n        tags.update(event_item_tags)\n\n        __targets = [{\n            \'qtype\': targets[0][\'qtype\'],\n            \'query\': targets[0][\'query\']\n        }]\n\n        __outlierDps = [\n            {\'dps\': anomalies[\'anomalies_dps\']\n             }\n        ]\n\n        fields = {\n            \'__title\': _title,\n            \'__content\': _content,\n            \'__dimensions\': json.dumps(trigger_info[\'dimensions\'], ensure_ascii=False, separators=(\',\', \':\')),\n            \'__targets\': json.dumps(__targets, ensure_ascii=False, separators=(\',\', \':\')),\n            \'__outlierDps\': json.dumps(__outlierDps, ensure_ascii=False, separators=(\',\', \':\'))\n        }\n\n        key_event = {\n            \'measurement\': \'__keyevent\',\n            \'tags\': tags,\n            \'fields\': fields,\n            \'timestamp\': anomalies[\'timestamp\']\n        }\n\n        keyevents_to_write.append(key_event)\n\n    return keyevents_to_write, actions_to_perform\n\n\ndef _perform_actions(actions_to_perform):\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n\n\ndef _write_keyevents(keyevents_to_write, trigger_info):\n    # 批量写入关键事件\n    # 实例化dataway对象，用于写入数据（行协议）\n    dataway = DFF.SRC(\'df_dataway\', token=trigger_info[\'workspaceToken\'])\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    # 利用行协议方法写入dataway，接口返回200表示写入成功\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.post_line_protocol(points=keyevents_to_write, path=\'/v1/write/keyevent\')\n        print(\'DataWay返回: {}\'.format(_resp))\n    else:\n        print(\'没有key event需要写入\')\n\n\n@DFF.API(\'水位检测\', category=\'builtinCheck\', is_hidden=True)\ndef level_shift_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    \"\"\"\n    水位检测，用于检测连续频繁的异常点，对于偶发的突变点不敏感\n    参数如下：\n        1. trigger_info {dict} 异常检测规则（以前叫触发器）的基本信息，结构如下：\n            {\n              \"uuid\"          : \"str, <UUID> 即规则ID\",\n              \"name\"          : \"str, <名称> 即规则名称\",\n              \"workspaceToken\": \"str, <工作空间令牌> 用于判断出事件之后写入__keyevent\",\n              \"dimensions\"    : \"<触发维度> list类型，元素为str\"\n            }\n\n        2. targets {list} 检测的目标，结构如下：\n            [\n              {\n                \"alias\"     : \"str, <别名> 默认即M1, M2 ...，用户可以改为其他内容，用于各类告警文案、数据填充\",\n                \"influxQL\"  : \"str, <InfluxDB 查询语句>\", 注：实际查询时需 替换占位符（$now) 为当前的时刻;\n                                注：influxQL里，where的时间窗里面，取 $now - （聚合周期个数+100）* 聚合周期 到 $now 的数据\",\n                \"qtype\"     : \"str, 如http，注：该字段内容算法用不上，只是当写入keyevennt时原封不动一并传入\",\n                \"query\"     : {\"注：该字段内容算法用不上，只是当写入keyevennt时一并传入，目的是使前端可以拉取keyevent同时也能根                                        据query获取原始的时序数据。该字段结构由后端和前端一起定义\"}\n              }\n            ]\n\n        3. condition_check_setting {dict} 异常检测的触发条件配置，结构如下，分为检测规则和告警动作：\n            {\n              \"rules\": [\n              {\n                \"triggerCondition\": {\n                    \"periodNum\": \"int, 检测几个聚合周期，实际就是检测最近的多少个点\",\n                    \"strength\": \"int, 检测的强度，目前页面上可选强，中，弱，传过来是数字3，2，1，强度越高代表异常的判定越苛刻\",\n                    \"direction\": \"str, 检测的方向, 页面上三个单选项对应的是 up, down, both\",\n                    \"checkCount\": \"int, 当连续的异常点数>=该值时，触发事件\"\n                },\n                \"level\": \"str, 异常级别，目前默认 warning，写死\"\n            }],\n              \"actions\": [\n                {\n                  \"type\"   : \"mail\",\n                  \"to\"     : [\"邮件地址1\", \"邮件地址2\"],\n                  \"title\"  : \"<邮件标题>\",\n                  \"content\": \"<邮件内容>\"\n                },\n                {\n                  \"type\"    : \"dingTalkRobot\",\n                  \"webhook\" : [\"钉钉机器人调用地址1\", \"钉钉机器人调用地址2\"],\n                  \"title\"   : \"<通知标题>\",\n                  \"markdown\": \"<MarkDown格式通知内容>\"\n                },\n                {\n                  \"type\"   : \"ProfWangAlertHub\",\n                  \"url\"    : [\"王教授AlertHub调用地址1\", \"王教授AlertHub调用地址2\"],\n                  \"title\"  : \"<情报标题>\",\n                  \"content\": \"<情报内容>\"\n                },\n                {\n                  \"type\"    : \"HTTPRequest\",\n                  \"url\"     : [\"请求地址1\", \"请求地址2\"],\n                  \"method\"  : \"<请求方式> GET|POST\",\n                  \"headers\" : {\"Header1\": \"HeaderValue1\", \"Header2\": \"HeaderValue2\"},\n                  \"body\"    : \"<method=POST 请求体>\",\n                  \"bodyType\": \"<method=POST 请求体类型> text|json|form\"\n                },\n                {\n                  \"type\"            : \"DataFluxFunc\",\n                  \"scriptSetRefName\": \"<脚本集引用名>\",\n                  \"scriptRefName\"   : \"<脚本引用名>\",\n                  \"funcRefName\"     : \"<函数名>\",\n                  \"kwargs\"          : { \"<调用参数名>\": \"<调用参数值>\" }\n                }\n              ]\n            }\n\n        4. all_time:  \"dict, 检测频率，数据聚合的周期\"\n            {\n                \"every_time\": \"str, 每隔多久检测一次\",\n                \"group_time\": \"str, 时间聚合周期\",\n            }\n\n        5. extra_alert_tags: {dict} 额外添加的告警数据标签\n            {\n                \"extra_alert_tags\": \"str, 异常\"\n            }\n    \"\"\"\n    _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags)\n\n    db_res = _get_data(targets)\n\n    trigger_condition = condition_check_setting[\'rules\'][0][\'triggerCondition\']\n\n    for data in db_res:\n        print(data[\'tags\'])\n        df = _preprocessing(data)\n        anomalies = _level_shift_check(df, trigger_condition)\n        if anomalies is not None:\n            print(\'异常时刻: \', anomalies[\'timestamp\'])\n            print(\'异常时间: \', datetime.datetime.fromtimestamp(anomalies[\'timestamp\'] + 28800))\n\n            actions = condition_check_setting[\'actions\']\n\n            level = condition_check_setting[\'rules\'][0][\'level\']\n            prev_level = None\n\n            event_item_tags = data[\'tags\']\n\n            # 得到event_id，异常检测器ID + 这条时序数据的tags 得到唯一的event_id\n            # 字典对象序列化，变成字符串\n            event_item_key = json.dumps(event_item_tags, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n            event_id = _get_event_id(trigger_info[\'uuid\'], event_item_key)\n\n            duration = anomalies[\'duration\'] / 1000 / 1000\n\n            field_alias = data[\'columns\'][1]\n            check_value_map = {field_alias: anomalies[\'anomaly\']}\n\n            keyevents_to_write, actions_to_perform = \\\n                _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level,\n                                              event_item_tags, event_id, duration, check_value_map,\n                                              extra_alert_tags=extra_alert_tags)\n            print(keyevents_to_write)\n            print(\'-------------------------------------------------\')\n            print(actions_to_perform)\n            _write_keyevents(keyevents_to_write, trigger_info)\n            _perform_actions(actions_to_perform)\n        else:\n            print(\'没有异常\')\n\n    return 1\n\n\ndef test():\n\n        # 模拟一个完整参数\n    kwargs = {\n        \"targets\": [\n          {\n            \"alias\": \"M1\",\n            \"qtype\": \"http\",\n            \"query\": {\n              \"fields\": [\n                {\n                  \"args\": [\n                    {\n                      \"name\": \"人数\"\n                    }\n                  ],\n                  \"name\": \"人数\",\n                  \"alias\": \"M1\",\n                  \"content\": \"\",\n                  \"funcName\": \"max\",\n                  \"fieldType\": \"integer\"\n                }\n              ],\n              \"filter\": {\n                \"tags\": [\n                  {\n                    \"name\": \"交通方式\",\n                    \"value\": [\n                      \"地铁\"\n                    ],\n                    \"condition\": \"\",\n                    \"operation\": \"=\"\n                  }\n                ]\n              },\n              \"period\": \"1m\",\n              \"groupBy\": [\n                \"交通方式\",\n                \"time(1m)\"\n              ],\n              \"measurements\": [\n                \"水位检测\"\n              ]\n            },\n            \"influxQL\": \"select (max(\\\"人数\\\")) as \\\"M1\\\" from \\\"biz_7ee6760f78ae4f2db672dfcd72be1aa8\\\".\\\"rp5\\\".\\\"水位检测\\\" where (\\\"time\\\" >=  $now - 103m and \\\"time\\\" <  $now ) and (\\\"交通方式\\\" = \'地铁\') group by \\\"交通方式\\\", time(1m)\"\n          }\n        ],\n        \"all_time\": {\n          \"every_time\": \"1m\",\n          \"group_time\": \"1m\"\n        },\n        \"trigger_info\": {\n          \"name\": \"水位检测\",\n          \"uuid\": \"rul_367ee926b52b11ea81dc8a480da894f2\",\n          \"dimensions\": [\n            \"交通方式\"\n          ],\n          \"workspaceToken\": \"tkn_69c090f66e14462787327653ba251e60\"\n        },\n        \"extra_alert_tags\": {\n          \"name\": \"水位检测\"\n        },\n        \"condition_check_setting\": {\n          \"rules\": [\n            {\n              \"level\": \"warning\",\n              \"triggerCondition\": {\n                \"strength\": 3,\n                \"direction\": \"both\",\n                \"periodNum\": 33,\n                \"checkCount\": 2\n              }\n            }\n          ],\n          \"actions\": [\n            {\n              \"to\": [\n                \"chenyiyun@jiagouyun.com\"\n              ],\n              \"type\": \"mail\",\n              \"title\": \"水位检测 suanfa\",\n              \"content\": \"您的水位检测存在异常，请及时处理！suanfa\"\n            }\n          ]\n        }\n      }\n\n    level_shift_check(**kwargs)\n', 'cc95bf21452c1db10ef7b06be5398101', '\"\"\"\n水位检测\n\n注：目前调度是整分钟的01秒调用，根据InfluxQL里的聚合，如果按分钟聚合，最后一个数据点是该分钟聚合这一秒的数据（往往是缺失）\n目前的处理：最后一个数据点为空，则剔除，其他数据点的空值填补成0\n\n2020-06-19  解析参数，验证参数，获取数据，处理数据&异常检测，如果触发事件则利用dataway行协议写入__keyevent\n\n2020-06-28 新增支持 告警功能 & 模板变量 功能\n\"\"\"\n\n\nimport hashlib\nimport re\nimport time\nimport datetime\nimport arrow\nimport simplejson as json\nimport six\nimport pandas as pd\nfrom adtk.data import validate_series\nfrom adtk.detector import PersistAD\n\n_DFF_is_debug = True\nLEVEL_NAME_MAP = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nOK_LEVEL = \'ok\'\nCONDITION_LOGICS = (\'and\', \'or\')\nCONDITION_OPERATORS = \\\n    (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nALL_LEVELS = (\'critical\', \'error\', \'warning\', \'info\')\nACTION_TYPES = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS = (\'GET\', \'POST\')\nHTTP_BODY_TYPES = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\nMOCK_DATA = None\nRAISE_DATAWAY_EXCEPTION = True\n\n# 基础函数\n\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n    return six.ensure_str(s)\n\n\n# 以下函数为 模板变量功能 & 进阶模板功能 所需要的功能函数\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from = from_unit.lower()\n    _search_to = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n                                  carry=1000)\n\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n                                  carry=1024)\n\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n                                  carry=1024)\n\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n                                  carry=1024)\n\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\': __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\': __filter_volumn,\n    \'volumni\': __filter_volumn_i,\n    \'bitrate\': __filter_bit_rate,\n    \'byterate\': __filter_byte_rate,\n    \'upper\': __filter_upper,\n    \'lower\': __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\': __filter_if_true,\n    \'iffalse\': __filter_if_false,\n}\n\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, six.string_types):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result = DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\n\n# 步骤1：参数校验\ndef __param_targets_check(targets):\n    if not isinstance(targets, list):\n        raise Exception(\'`targets` should be a list\')\n\n    for i, t in enumerate(targets):\n        if not isinstance(t, dict):\n            raise Exception(\'`targets[{}]` should be a dict\'.format(i))\n\n        if not isinstance(t.get(\'alias\'), str):\n            raise Exception(\'`targets[{}][\"alias\"]` should be a str\'.format(i))\n\n        if \'influxQL\' in t and not isinstance(t.get(\'influxQL\'), str):\n            raise Exception(\'`targets[{}][\"influxQL\"]` should be a str\'.format(i))\n\n        if \'expression\' in t and not isinstance(t.get(\'expression\'), str):\n            raise Exception(\'`targets[{}][\"expression\"]` should be a str\'.format(i))\n\n\ndef __param_rules_check(rules):\n    if not isinstance(rules, list):\n        raise Exception(\'`rules` should be a list\')\n\n    for i, r in enumerate(rules):\n        if not isinstance(r, dict):\n            raise Exception(\'`rules[{}]` should be a dict\'.format(i))\n\n        if r.get(\'level\') not in ALL_LEVELS:\n            raise Exception(\'`rules[{}][\"level\"]` should be one of {}\'.format(i, \',\'.join(ALL_LEVELS)))\n\n        if not isinstance(r.get(\'triggerCondition\'), dict):\n            raise Exception(\'`rules[{}][\"triggerCondition\"]` should be a dict\'.format(i))\n\n        c = r[\'triggerCondition\']\n        if not isinstance(c.get(\'periodNum\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"periodNum\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'strength\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"strength\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'direction\'), str):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be a str\'.format(i))\n\n        d = c.get(\'direction\')\n        if d not in [\'up\', \'down\', \'both\']:\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be `up` or `down` or `both`\'.format(i))\n\n        if not isinstance(c.get(\'checkCount\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"checkCount\"]` should be an int\'.format(i))\n\n\ndef __param_action_check(actions, parent_path):\n    if not isinstance(actions, list):\n        raise Exception(\'`{}actions` should be a list\'.format(parent_path))\n\n    for i, a in enumerate(actions):\n        if not isinstance(a, dict):\n            raise Exception(\'`{}actions[{}]` should be a dict\'.format(parent_path, i))\n\n        if a.get(\'type\') not in ACTION_TYPES:\n            raise Exception(\n                \'`{}actions[{}][\"type\"]` should be one of {}\'.format(parent_path, i, \',\'.join(ACTION_TYPES)))\n\n        if a[\'type\'] == \'mail\':\n            if not isinstance(a.get(\'to\'), list):\n                raise Exception(\'`{}actions[{}][\"to\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'to\'])):\n                raise Exception(\'Element of `{}actions[{}][\"to\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'dingTalkRobot\':\n            if not isinstance(a.get(\'webhook\'), list):\n                raise Exception(\'`{}actions[{}][\"webhook\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'webhook\'])):\n                raise Exception(\'Element of `{}actions[{}][\"webhook\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'markdown\'), str):\n                raise Exception(\'`{}actions[{}][\"markdown\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'ProfWangAlertHub\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'suggestion\') is not None:\n                if not isinstance(a[\'suggestion\'], str):\n                    raise Exception(\'`{}actions[{}][\"suggestion\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'HTTPRequest\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'method\') not in HTTP_METHODS:\n                raise Exception(\n                    \'`{}actions[{}][\"method\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_METHODS)))\n\n            if a.get(\'headers\') is not None:\n                if not isinstance(a[\'headers\'], dict):\n                    raise Exception(\'`{}actions[{}][\"headers\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].values())):\n                    raise Exception(\'Value of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n            if a[\'method\'] == \'POST\':\n                if a.get(\'bodyType\') not in HTTP_BODY_TYPES:\n                    raise Exception(\'`{}actions[{}][\"bodyType\"]` should be one of {}\'.format(parent_path, i,\n                                                                                             \',\'.join(HTTP_BODY_TYPES)))\n\n                if not isinstance(a.get(\'body\'), str):\n                    raise Exception(\'`{}actions[{}][\"body\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'DataFluxFunc\':\n            if not isinstance(a.get(\'scriptSetRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptSetRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'scriptRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'funcRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"funcRefName\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'kwargs\') is not None:\n                if not isinstance(a[\'kwargs\'], dict):\n                    raise Exception(\'`{}actions[{}][\"kwargs\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'kwargs\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"kwargs\"]` should be a str\'.format(parent_path, i))\n\n\ndef _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    # 1. 检查`trigger_info`\n    if not isinstance(trigger_info, dict):\n        raise Exception(\'`trigger_info` should be a dict\')\n\n    if not isinstance(trigger_info.get(\'uuid\'), str):\n        raise Exception(\'`trigger_info[\"uuid\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'name\'), str):\n        raise Exception(\'`trigger_info[\"name\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'workspaceToken\'), str):\n        raise Exception(\'`trigger_info[\"workspaceToken\"]` should be a str\')\n\n    dimensions = trigger_info.get(\'dimensions\')\n    if dimensions is not None:\n        if not isinstance(dimensions, list):\n            raise Exception(\'`trigger_info[\"dimensions\"]` should be a list\')\n\n        for index, dimension in enumerate(dimensions):\n            if not isinstance(dimension, str):\n                raise Exception(\'`trigger_info[\"dimensions\"][{}]` should be a str\'.format(index))\n\n    # 2. 检查`targets`\n    __param_targets_check(targets)\n\n    # 3. 检查`condition_check_setting`\n    if not isinstance(condition_check_setting, dict):\n        raise Exception(\'`condition_check_setting` should be a dict\')\n\n    __param_rules_check(condition_check_setting.get(\'rules\'))\n    __param_action_check(condition_check_setting.get(\'actions\'), parent_path=\'condition_check_setting.\')\n\n    # 4. 检查`all_time`\n    if not isinstance(all_time, dict):\n        raise Exception(\'`all_time` should be a dict\')\n\n    if not isinstance(all_time.get(\'every_time\'), str):\n        raise Exception(\'`all_time[\"every_time\"]` should be a str\')\n\n    if not isinstance(all_time.get(\'group_time\'), str):\n        raise Exception(\'`all_time[\"group_time\"]` should be a str\')\n\n    # 正则表达式 匹配 *s, *m, *h, *d, *w, *M (秒，分，时，天，周，月)\n    match_every_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'every_time\'))\n    if not (match_every_time is not None and match_every_time.span(0)[1] == len(all_time.get(\'every_time\'))):\n        raise Exception(\'`all_time[\"every_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    match_group_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'group_time\'))\n    if not (match_group_time is not None and match_group_time.span(0)[1] == len(all_time.get(\'group_time\'))):\n        raise Exception(\'`all_time[\"group_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    # 5. 检查 `extra_alert_tags`\n    if extra_alert_tags is not None:\n        if not isinstance(extra_alert_tags, dict):\n            raise Exception(\'`extra_alert_tags` should be an dict\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.keys())):\n            raise Exception(\'Key of `extra_alert_tags` should be a str\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.values())):\n            raise Exception(\'Value of `extra_alert_tags` should be a str\')\n\n\n# 步骤2：获取数据\ndef _to_iso_time(d=None):\n    # 将unix时间戳转化成iso8601时间格式\n    return arrow.get(d).isoformat()\n\n\ndef _get_data(targets):\n\n    # 根据targets里的InfluxQL语句，获取数据，targets是个列表，目前仅支持单个target，所以取第一个\n    target = targets[0]\n    db_res = None\n    if MOCK_DATA:\n        db_res = MOCK_DATA[target[\'alias\']]\n    else:\n        bind_params = {}\n        # 转成真实触发时间\n        if \'$now\' in target[\'influxQL\']:\n            bind_params[\'now\'] = _to_iso_time(int(time.time()))\n\n        db_res = DFF.SRC(\'df_influxdb\').query(target[\'influxQL\'], bind_params=bind_params)\n\n    # 最终返回的valid_series_data是 由多个dict组成的list\n    valid_series_data = []\n    if \'series\' in db_res:\n        for s in db_res[\'series\']:\n            if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                continue\n\n            valid_series_data.append(s)\n\n    return valid_series_data\n\n\n# 步骤3：异常检测\n# 第二步返回的数据结果是 list of dict，第3步将遍历每一个dict（每个dict就是一条时间序列），进行单时间序列的异常检测\n\n\ndef _preprocessing(data):\n    \"\"\"\n    data为dict类型数据，处理成pd.Series\n    如果最后一个数据点为空，则剔除，其余为空则补0\n    \"\"\"\n    print(data)\n    df = pd.DataFrame(data[\'values\'])\n    df.index = df[0].apply(lambda x: x.replace(\'T\', \' \').replace(\'Z\', \'\'))\n    df = df[1]\n    df.index = pd.to_datetime(df.index)\n    df = df.astype(float)\n    if df.isnull().iloc[-1]:\n        df = df.iloc[:-1]\n    df.fillna(0, inplace=True)\n    print(df)\n    return df\n\n\ndef _level_shift_check(df, trigger_condition):\n    \"\"\"\n    水位检测，用于检测连续频繁的异常点，对于偶发的突变点不敏感\n    注：目前是 使用历史 30 个点，计算四分位距，中位数median，\n\n    :param df: pd.Series. 要检测的时间序列数据 index: datetime  value: float\n    :param trigger_condition: dict. 异常检测器的参数设置\n\n    :return: None or dict.\n    \"\"\"\n\n    # validate_series 是 adtk包的函数，可以对时序pandas数据进行预处理\n    df = validate_series(df)\n    # 检测强度，强度越高，对于异常值的检测越苛刻\n    c_dict = {1: 1, 2: 2, 3: 3}\n    c = c_dict[trigger_condition[\'strength\']]\n    if trigger_condition[\'direction\'] == \'up\':\n        side = \'positive\'\n    elif trigger_condition[\'direction\'] == \'down\':\n        side = \'negative\'\n    else:\n        side = \'both\'\n    check_count = trigger_condition[\'checkCount\']\n    window = min(100, max(10, check_count * 5))\n    ad = PersistAD(window=window, c=c, side=side)\n    anomalies = ad.fit_detect(df)\n\n    # 只检测最近的 point_num 个点\n    point_num = int(trigger_condition[\'periodNum\'])\n    anomalies = anomalies.iloc[-point_num:]\n\n    # 只有 连续的 anomalies个数 达到 checkCount时，才当做关键事件\n    # 刚好触达 checkCount的那个异常点所在的时刻 定义为 关键事件的时刻\n    # 持续时间：单位为微秒\n    temp = 0\n    dps = []\n    ts = None\n    for k, v in anomalies.items():\n        if temp == check_count:\n            ts = dps[-1][0]\n            break\n        if v != 1:\n            temp = 0\n            dps = []\n        else:\n            dps.append([int(k.timestamp()), df[k]])\n            temp += 1\n\n    if ts is None:\n        res = None\n    else:\n        res = {\n            \'timestamp\': ts,\n            \'anomalies_dps\': dps,\n            \'duration\': int((dps[-1][0] - dps[0][0]) * 1000 * 1000),\n            \'anomaly\': dps[-1][1]\n        }\n        print(res)\n    return res\n\n\n# 步骤4：生成关键事件，和告警内容；存入关键事件，实施告警\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\n\ndef _get_event_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    return _get_md5(str_to_md5)\n\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, event_item_tags, event_id, duration, check_value_map=None,\n                     prev_level=None):\n    \"\"\"\n    创建变量字典\n    相关变量定义参考 模板变量文档\n    https://help.dataflux.cn/doc/1ee7e06f0f06162dd3ee3bd834611b6f0c3100d8\n\n    注：目前prev_level默认缺失，即默认为 OK\n    \"\"\"\n\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\': level,\n        \'status\': level,\n        \'prevLevel\': prev_level,\n        \'prevStatus\': prev_level,\n        \'levelName\': LEVEL_NAME_MAP[level],\n        \'statusName\': LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'prevStatusName\': prev_level_name,\n        \'tags\': _json_dumps(event_item_tags),\n        \'tagsEscaped\': None,\n        \'fields\': None,\n        \'fieldsEscaped\': None,\n        \'duration\': duration * 1000 if duration is not None else None,\n        \'durationHuman\': None,\n        \'ruleId\': trigger_uuid,\n        \'ruleName\': trigger_name,\n        \'alertId\': event_id,\n        \'eventId\': event_id\n    }\n\n    # 补全tags.KEY\n    for k, v in event_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全 fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全 fieldsEscaped、tagsEscaped\n    # 在json结构的string类型值里面再放tags, fields 的 json, 需要转义\n    # 如果人家填写的整个body是 {\"tags\": \"#{tags}\"} ，最终结果会变成\n    # {\"tags\": \"{\"a\": 1}\"}，这个是错误的json结构\n    # 因此提供了转义后的json字符串，保证上述例子输出为 {\"tags\": \"{\\\"a\\\": 1}\"}\n    # 如果人家body填的是 {\"tags\":  # {tags} } 就没事，最终结果为 {\"tags\": {\"a\": 1}}\n\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    if duration is not None:\n        _m, _s = divmod(duration, 60)\n        _h, _m = divmod(_m, 60)\n        _d, _h = divmod(_h, 24)\n        _human = \'{}秒\'.format(_s)\n        if _m > 0:\n            _human = \'{}分钟\'.format(_m) + _human\n        if _h > 0:\n            _human = \'{}小时\'.format(_h) + _human\n        if _d > 0:\n            _human = \'{}天\'.format(_d) + _human\n\n        var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to = action[\'to\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\': to,\n            \'title\': title,\n            \'content\': {\'text\': content},\n            \'sender\': \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook = action[\'webhook\']\n    title = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\': title,\n                \'text\': markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url = action[\'url\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\': var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\': profwang_level,\n            \'type\': \'alarm\',\n            \'title\': title,\n            \'content\': content,\n            \'suggestion\': suggestion,\n            \'origin\': \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': \'POST\',\n                \'url\': _url,\n                \'body\': _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    url = action[\'url\']\n    method = action[\'method\'].upper()\n    headers = action.get(\'headers\')\n    body = action.get(\'body\') or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': method,\n                \'url\': _url,\n                \'headers\': headers,\n                \'query\': None,\n                \'body\': body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'url\': url,\n            \'method\': method,\n            \'headers\': headers,\n            \'body\': body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name = action[\'scriptRefName\']\n    func_ref_name = action[\'funcRefName\']\n    kwargs = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\': script_ref_name,\n            \'funcRefName\': func_ref_name,\n            \'kwargs\': kwargs,\n        }),\n    }\n    return action_content, None\n\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\': _create_action_debug,\n    \'mail\': _create_action_mail,\n    \'dingTalkRobot\': _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\': _create_action_http_request,\n    \'DataFluxFunc\': _create_action_dataflux_func,\n}\n\n\ndef _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level, event_item_tags,\n                                  event_id, duration, check_value_map, extra_alert_tags=None, is_no_data=False):\n    trigger_uuid = trigger_info[\'uuid\']\n    trigger_name = trigger_info[\'name\']\n    # dimensions = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    var_dict = _create_var_dict(\n        trigger_uuid=trigger_uuid,\n        trigger_name=trigger_name,\n        level=level,\n        prev_level=prev_level,\n        event_item_tags=event_item_tags,\n        event_id=event_id,\n        duration=duration,\n        check_value_map=check_value_map)\n\n    for action in actions:\n        action_type = action[\'type\']\n\n        # 记录执行动作\n        _action = _json_copy(action)\n        action_content = None\n        action_task = None\n        try:\n            action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n        except Exception as e:\n            print(e)\n        else:\n            if action_task:\n                if isinstance(action_task, list):\n                    actions_to_perform.extend(action_task)\n                else:\n                    actions_to_perform.append(action_task)\n\n        # 记录写入__keyevent\n        print(action_content)\n        _title = action_content[\'title\']\n        _content = action_content.get(\'content\') or None\n        # _suggestion = action_content.get(\'suggestion\') or None\n        # _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n        # _keyevent_type = None\n        # if is_no_data:\n        #     _keyevent_type = \'noData\'\n\n        tags = {\n            \'__status\': level,\n            \'__eventId\': event_id,\n            \'__source\': \'datafluxTrigger\',\n            \'__ruleId\': trigger_uuid,\n            \'__ruleName\': trigger_name,\n            \'__actionType\': action_type\n        }\n        if extra_alert_tags:\n            tags.update(extra_alert_tags)\n        tags.update(event_item_tags)\n\n        __targets = [{\n            \'qtype\': targets[0][\'qtype\'],\n            \'query\': targets[0][\'query\']\n        }]\n\n        __outlierDps = [\n            {\'dps\': anomalies[\'anomalies_dps\']\n             }\n        ]\n\n        fields = {\n            \'__title\': _title,\n            \'__content\': _content,\n            \'__dimensions\': json.dumps(trigger_info[\'dimensions\'], ensure_ascii=False, separators=(\',\', \':\')),\n            \'__targets\': json.dumps(__targets, ensure_ascii=False, separators=(\',\', \':\')),\n            \'__outlierDps\': json.dumps(__outlierDps, ensure_ascii=False, separators=(\',\', \':\'))\n        }\n\n        key_event = {\n            \'measurement\': \'__keyevent\',\n            \'tags\': tags,\n            \'fields\': fields,\n            \'timestamp\': anomalies[\'timestamp\']\n        }\n\n        keyevents_to_write.append(key_event)\n\n    return keyevents_to_write, actions_to_perform\n\n\ndef _perform_actions(actions_to_perform):\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n\n\ndef _write_keyevents(keyevents_to_write, trigger_info):\n    # 批量写入关键事件\n    # 实例化dataway对象，用于写入数据（行协议）\n    dataway = DFF.SRC(\'df_dataway\', token=trigger_info[\'workspaceToken\'])\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    # 利用行协议方法写入dataway，接口返回200表示写入成功\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.post_line_protocol(points=keyevents_to_write, path=\'/v1/write/keyevent\')\n        print(\'DataWay返回: {}\'.format(_resp))\n    else:\n        print(\'没有key event需要写入\')\n\n\n@DFF.API(\'水位检测\', category=\'builtinCheck\', is_hidden=True)\ndef level_shift_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    \"\"\"\n    水位检测，用于检测连续频繁的异常点，对于偶发的突变点不敏感\n    参数如下：\n        1. trigger_info {dict} 异常检测规则（以前叫触发器）的基本信息，结构如下：\n            {\n              \"uuid\"          : \"str, <UUID> 即规则ID\",\n              \"name\"          : \"str, <名称> 即规则名称\",\n              \"workspaceToken\": \"str, <工作空间令牌> 用于判断出事件之后写入__keyevent\",\n              \"dimensions\"    : \"<触发维度> list类型，元素为str\"\n            }\n\n        2. targets {list} 检测的目标，结构如下：\n            [\n              {\n                \"alias\"     : \"str, <别名> 默认即M1, M2 ...，用户可以改为其他内容，用于各类告警文案、数据填充\",\n                \"influxQL\"  : \"str, <InfluxDB 查询语句>\", 注：实际查询时需 替换占位符（$now) 为当前的时刻;\n                                注：influxQL里，where的时间窗里面，取 $now - （聚合周期个数+100）* 聚合周期 到 $now 的数据\",\n                \"qtype\"     : \"str, 如http，注：该字段内容算法用不上，只是当写入keyevennt时原封不动一并传入\",\n                \"query\"     : {\"注：该字段内容算法用不上，只是当写入keyevennt时一并传入，目的是使前端可以拉取keyevent同时也能根                                        据query获取原始的时序数据。该字段结构由后端和前端一起定义\"}\n              }\n            ]\n\n        3. condition_check_setting {dict} 异常检测的触发条件配置，结构如下，分为检测规则和告警动作：\n            {\n              \"rules\": [\n              {\n                \"triggerCondition\": {\n                    \"periodNum\": \"int, 检测几个聚合周期，实际就是检测最近的多少个点\",\n                    \"strength\": \"int, 检测的强度，目前页面上可选强，中，弱，传过来是数字3，2，1，强度越高代表异常的判定越苛刻\",\n                    \"direction\": \"str, 检测的方向, 页面上三个单选项对应的是 up, down, both\",\n                    \"checkCount\": \"int, 当连续的异常点数>=该值时，触发事件\"\n                },\n                \"level\": \"str, 异常级别，目前默认 warning，写死\"\n            }],\n              \"actions\": [\n                {\n                  \"type\"   : \"mail\",\n                  \"to\"     : [\"邮件地址1\", \"邮件地址2\"],\n                  \"title\"  : \"<邮件标题>\",\n                  \"content\": \"<邮件内容>\"\n                },\n                {\n                  \"type\"    : \"dingTalkRobot\",\n                  \"webhook\" : [\"钉钉机器人调用地址1\", \"钉钉机器人调用地址2\"],\n                  \"title\"   : \"<通知标题>\",\n                  \"markdown\": \"<MarkDown格式通知内容>\"\n                },\n                {\n                  \"type\"   : \"ProfWangAlertHub\",\n                  \"url\"    : [\"王教授AlertHub调用地址1\", \"王教授AlertHub调用地址2\"],\n                  \"title\"  : \"<情报标题>\",\n                  \"content\": \"<情报内容>\"\n                },\n                {\n                  \"type\"    : \"HTTPRequest\",\n                  \"url\"     : [\"请求地址1\", \"请求地址2\"],\n                  \"method\"  : \"<请求方式> GET|POST\",\n                  \"headers\" : {\"Header1\": \"HeaderValue1\", \"Header2\": \"HeaderValue2\"},\n                  \"body\"    : \"<method=POST 请求体>\",\n                  \"bodyType\": \"<method=POST 请求体类型> text|json|form\"\n                },\n                {\n                  \"type\"            : \"DataFluxFunc\",\n                  \"scriptSetRefName\": \"<脚本集引用名>\",\n                  \"scriptRefName\"   : \"<脚本引用名>\",\n                  \"funcRefName\"     : \"<函数名>\",\n                  \"kwargs\"          : { \"<调用参数名>\": \"<调用参数值>\" }\n                }\n              ]\n            }\n\n        4. all_time:  \"dict, 检测频率，数据聚合的周期\"\n            {\n                \"every_time\": \"str, 每隔多久检测一次\",\n                \"group_time\": \"str, 时间聚合周期\",\n            }\n\n        5. extra_alert_tags: {dict} 额外添加的告警数据标签\n            {\n                \"extra_alert_tags\": \"str, 异常\"\n            }\n    \"\"\"\n    _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags)\n\n    db_res = _get_data(targets)\n\n    trigger_condition = condition_check_setting[\'rules\'][0][\'triggerCondition\']\n\n    for data in db_res:\n        print(data[\'tags\'])\n        df = _preprocessing(data)\n        anomalies = _level_shift_check(df, trigger_condition)\n        if anomalies is not None:\n            print(\'异常时刻: \', anomalies[\'timestamp\'])\n            print(\'异常时间: \', datetime.datetime.fromtimestamp(anomalies[\'timestamp\'] + 28800))\n\n            actions = condition_check_setting[\'actions\']\n\n            level = condition_check_setting[\'rules\'][0][\'level\']\n            prev_level = None\n\n            event_item_tags = data[\'tags\']\n\n            # 得到event_id，异常检测器ID + 这条时序数据的tags 得到唯一的event_id\n            # 字典对象序列化，变成字符串\n            event_item_key = json.dumps(event_item_tags, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n            event_id = _get_event_id(trigger_info[\'uuid\'], event_item_key)\n\n            duration = anomalies[\'duration\'] / 1000 / 1000\n\n            field_alias = data[\'columns\'][1]\n            check_value_map = {field_alias: anomalies[\'anomaly\']}\n\n            keyevents_to_write, actions_to_perform = \\\n                _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level,\n                                              event_item_tags, event_id, duration, check_value_map,\n                                              extra_alert_tags=extra_alert_tags)\n            print(keyevents_to_write)\n            print(\'-------------------------------------------------\')\n            print(actions_to_perform)\n            _write_keyevents(keyevents_to_write, trigger_info)\n            _perform_actions(actions_to_perform)\n        else:\n            print(\'没有异常\')\n\n    return 1\n\n\ndef test():\n\n        # 模拟一个完整参数\n    kwargs = {\n        \"targets\": [\n          {\n            \"alias\": \"M1\",\n            \"qtype\": \"http\",\n            \"query\": {\n              \"fields\": [\n                {\n                  \"args\": [\n                    {\n                      \"name\": \"人数\"\n                    }\n                  ],\n                  \"name\": \"人数\",\n                  \"alias\": \"M1\",\n                  \"content\": \"\",\n                  \"funcName\": \"max\",\n                  \"fieldType\": \"integer\"\n                }\n              ],\n              \"filter\": {\n                \"tags\": [\n                  {\n                    \"name\": \"交通方式\",\n                    \"value\": [\n                      \"地铁\"\n                    ],\n                    \"condition\": \"\",\n                    \"operation\": \"=\"\n                  }\n                ]\n              },\n              \"period\": \"1m\",\n              \"groupBy\": [\n                \"交通方式\",\n                \"time(1m)\"\n              ],\n              \"measurements\": [\n                \"水位检测\"\n              ]\n            },\n            \"influxQL\": \"select (max(\\\"人数\\\")) as \\\"M1\\\" from \\\"biz_7ee6760f78ae4f2db672dfcd72be1aa8\\\".\\\"rp5\\\".\\\"水位检测\\\" where (\\\"time\\\" >=  $now - 103m and \\\"time\\\" <  $now ) and (\\\"交通方式\\\" = \'地铁\') group by \\\"交通方式\\\", time(1m)\"\n          }\n        ],\n        \"all_time\": {\n          \"every_time\": \"1m\",\n          \"group_time\": \"1m\"\n        },\n        \"trigger_info\": {\n          \"name\": \"水位检测\",\n          \"uuid\": \"rul_367ee926b52b11ea81dc8a480da894f2\",\n          \"dimensions\": [\n            \"交通方式\"\n          ],\n          \"workspaceToken\": \"tkn_69c090f66e14462787327653ba251e60\"\n        },\n        \"extra_alert_tags\": {\n          \"name\": \"水位检测\"\n        },\n        \"condition_check_setting\": {\n          \"rules\": [\n            {\n              \"level\": \"warning\",\n              \"triggerCondition\": {\n                \"strength\": 3,\n                \"direction\": \"both\",\n                \"periodNum\": 33,\n                \"checkCount\": 2\n              }\n            }\n          ],\n          \"actions\": [\n            {\n              \"to\": [\n                \"chenyiyun@jiagouyun.com\"\n              ],\n              \"type\": \"mail\",\n              \"title\": \"水位检测 suanfa\",\n              \"content\": \"您的水位检测存在异常，请及时处理！suanfa\"\n            }\n          ]\n        }\n      }\n\n    level_shift_check(**kwargs)\n', 'cc95bf21452c1db10ef7b06be5398101', '2020-06-29 09:37:40', '2020-06-29 09:41:07'),
('8', 'scpt-range_chk', 'FT区间检测', NULL, 'sset-ft_lib', 'range_chk', '1', 'python', '\"\"\"\n区间检测\n\n注：目前调度是整分钟的01秒调用，根据InfluxQL里的聚合，如果按分钟聚合，最后一个数据点是该分钟聚合这一秒的数据（往往是缺失）\n目前的处理：最后一个数据点为空，则剔除，其他数据点的空值填补成0\n\n2020-06-18  解析参数，验证参数，获取数据，处理数据&异常检测，如果触发事件则利用dataway行协议写入__keyevent\n\n2020-06-28 新增支持 告警功能 & 模板变量 功能\n\"\"\"\n\n\nimport hashlib\nimport re\nimport time\nimport datetime\nimport arrow\nimport simplejson as json\nimport six\nimport pandas as pd\nimport numpy as np\nfrom adtk.data import validate_series\n\n_DFF_is_debug = True\nLEVEL_NAME_MAP = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nOK_LEVEL = \'ok\'\nCONDITION_LOGICS = (\'and\', \'or\')\nCONDITION_OPERATORS = \\\n    (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nALL_LEVELS = (\'critical\', \'error\', \'warning\', \'info\')\nACTION_TYPES = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS = (\'GET\', \'POST\')\nHTTP_BODY_TYPES = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\nMOCK_DATA = None\nRAISE_DATAWAY_EXCEPTION = True\n\n# 基础函数\n\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n    return six.ensure_str(s)\n\n\n# 以下函数为 模板变量功能 & 进阶模板功能 所需要的功能函数\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from = from_unit.lower()\n    _search_to = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n                                  carry=1000)\n\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n                                  carry=1024)\n\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n                                  carry=1024)\n\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n                                  carry=1024)\n\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\': __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\': __filter_volumn,\n    \'volumni\': __filter_volumn_i,\n    \'bitrate\': __filter_bit_rate,\n    \'byterate\': __filter_byte_rate,\n    \'upper\': __filter_upper,\n    \'lower\': __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\': __filter_if_true,\n    \'iffalse\': __filter_if_false,\n}\n\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, six.string_types):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result = DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\n\n# 步骤1：参数校验\ndef __param_targets_check(targets):\n    if not isinstance(targets, list):\n        raise Exception(\'`targets` should be a list\')\n\n    for i, t in enumerate(targets):\n        if not isinstance(t, dict):\n            raise Exception(\'`targets[{}]` should be a dict\'.format(i))\n\n        if not isinstance(t.get(\'alias\'), str):\n            raise Exception(\'`targets[{}][\"alias\"]` should be a str\'.format(i))\n\n        if \'influxQL\' in t and not isinstance(t.get(\'influxQL\'), str):\n            raise Exception(\'`targets[{}][\"influxQL\"]` should be a str\'.format(i))\n\n        if \'expression\' in t and not isinstance(t.get(\'expression\'), str):\n            raise Exception(\'`targets[{}][\"expression\"]` should be a str\'.format(i))\n\n\ndef __param_rules_check(rules):\n    if not isinstance(rules, list):\n        raise Exception(\'`rules` should be a list\')\n\n    for i, r in enumerate(rules):\n        if not isinstance(r, dict):\n            raise Exception(\'`rules[{}]` should be a dict\'.format(i))\n\n        if r.get(\'level\') not in ALL_LEVELS:\n            raise Exception(\'`rules[{}][\"level\"]` should be one of {}\'.format(i, \',\'.join(ALL_LEVELS)))\n\n        if not isinstance(r.get(\'triggerCondition\'), dict):\n            raise Exception(\'`rules[{}][\"triggerCondition\"]` should be a dict\'.format(i))\n\n        c = r[\'triggerCondition\']\n        if not isinstance(c.get(\'periodNum\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"periodNum\"]` should be an int\'.format(i))\n\n        # if not isinstance(c.get(\'strength\'), int):\n        #     raise Exception(\'`rules[{}][\"triggerCondition\"][\"strength\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'direction\'), str):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be a str\'.format(i))\n\n        d = c.get(\'direction\')\n        if d not in [\'up\', \'down\', \'both\']:\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be `up` or `down` or `both`\'.format(i))\n\n        if not isinstance(c.get(\'checkPercent\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"checkPercent\"]` should be an int\'.format(i))\n\n\ndef __param_action_check(actions, parent_path):\n    if not isinstance(actions, list):\n        raise Exception(\'`{}actions` should be a list\'.format(parent_path))\n\n    for i, a in enumerate(actions):\n        if not isinstance(a, dict):\n            raise Exception(\'`{}actions[{}]` should be a dict\'.format(parent_path, i))\n\n        if a.get(\'type\') not in ACTION_TYPES:\n            raise Exception(\n                \'`{}actions[{}][\"type\"]` should be one of {}\'.format(parent_path, i, \',\'.join(ACTION_TYPES)))\n\n        if a[\'type\'] == \'mail\':\n            if not isinstance(a.get(\'to\'), list):\n                raise Exception(\'`{}actions[{}][\"to\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'to\'])):\n                raise Exception(\'Element of `{}actions[{}][\"to\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'dingTalkRobot\':\n            if not isinstance(a.get(\'webhook\'), list):\n                raise Exception(\'`{}actions[{}][\"webhook\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'webhook\'])):\n                raise Exception(\'Element of `{}actions[{}][\"webhook\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'markdown\'), str):\n                raise Exception(\'`{}actions[{}][\"markdown\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'ProfWangAlertHub\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'suggestion\') is not None:\n                if not isinstance(a[\'suggestion\'], str):\n                    raise Exception(\'`{}actions[{}][\"suggestion\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'HTTPRequest\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'method\') not in HTTP_METHODS:\n                raise Exception(\n                    \'`{}actions[{}][\"method\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_METHODS)))\n\n            if a.get(\'headers\') is not None:\n                if not isinstance(a[\'headers\'], dict):\n                    raise Exception(\'`{}actions[{}][\"headers\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].values())):\n                    raise Exception(\'Value of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n            if a[\'method\'] == \'POST\':\n                if a.get(\'bodyType\') not in HTTP_BODY_TYPES:\n                    raise Exception(\'`{}actions[{}][\"bodyType\"]` should be one of {}\'.format(parent_path, i,\n                                                                                             \',\'.join(HTTP_BODY_TYPES)))\n\n                if not isinstance(a.get(\'body\'), str):\n                    raise Exception(\'`{}actions[{}][\"body\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'DataFluxFunc\':\n            if not isinstance(a.get(\'scriptSetRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptSetRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'scriptRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'funcRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"funcRefName\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'kwargs\') is not None:\n                if not isinstance(a[\'kwargs\'], dict):\n                    raise Exception(\'`{}actions[{}][\"kwargs\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'kwargs\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"kwargs\"]` should be a str\'.format(parent_path, i))\n\n\ndef _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    # 1. 检查`trigger_info`\n    if not isinstance(trigger_info, dict):\n        raise Exception(\'`trigger_info` should be a dict\')\n\n    if not isinstance(trigger_info.get(\'uuid\'), str):\n        raise Exception(\'`trigger_info[\"uuid\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'name\'), str):\n        raise Exception(\'`trigger_info[\"name\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'workspaceToken\'), str):\n        raise Exception(\'`trigger_info[\"workspaceToken\"]` should be a str\')\n\n    dimensions = trigger_info.get(\'dimensions\')\n    if dimensions is not None:\n        if not isinstance(dimensions, list):\n            raise Exception(\'`trigger_info[\"dimensions\"]` should be a list\')\n\n        for index, dimension in enumerate(dimensions):\n            if not isinstance(dimension, str):\n                raise Exception(\'`trigger_info[\"dimensions\"][{}]` should be a str\'.format(index))\n\n    # 2. 检查`targets`\n    __param_targets_check(targets)\n\n    # 3. 检查`condition_check_setting`\n    if not isinstance(condition_check_setting, dict):\n        raise Exception(\'`condition_check_setting` should be a dict\')\n\n    __param_rules_check(condition_check_setting.get(\'rules\'))\n    __param_action_check(condition_check_setting.get(\'actions\'), parent_path=\'condition_check_setting.\')\n\n    # 4. 检查`all_time`\n    if not isinstance(all_time, dict):\n        raise Exception(\'`all_time` should be a dict\')\n\n    if not isinstance(all_time.get(\'every_time\'), str):\n        raise Exception(\'`all_time[\"every_time\"]` should be a str\')\n\n    if not isinstance(all_time.get(\'group_time\'), str):\n        raise Exception(\'`all_time[\"group_time\"]` should be a str\')\n\n    # 正则表达式 匹配 *s, *m, *h, *d, *w, *M (秒，分，时，天，周，月)\n    match_every_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'every_time\'))\n    if not (match_every_time is not None and match_every_time.span(0)[1] == len(all_time.get(\'every_time\'))):\n        raise Exception(\'`all_time[\"every_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    match_group_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'group_time\'))\n    if not (match_group_time is not None and match_group_time.span(0)[1] == len(all_time.get(\'group_time\'))):\n        raise Exception(\'`all_time[\"group_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    # 5. 检查 `extra_alert_tags`\n    if extra_alert_tags is not None:\n        if not isinstance(extra_alert_tags, dict):\n            raise Exception(\'`extra_alert_tags` should be an dict\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.keys())):\n            raise Exception(\'Key of `extra_alert_tags` should be a str\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.values())):\n            raise Exception(\'Value of `extra_alert_tags` should be a str\')\n\n\n# 步骤2：获取数据\ndef _to_iso_time(d=None):\n    # 将unix时间戳转化成iso8601时间格式\n    return arrow.get(d).isoformat()\n\n\ndef _get_data(targets):\n\n    # 根据targets里的InfluxQL语句，获取数据，targets是个列表，目前仅支持单个target，所以取第一个\n    target = targets[0]\n    db_res = None\n    if MOCK_DATA:\n        db_res = MOCK_DATA[target[\'alias\']]\n    else:\n        bind_params = {}\n        # 转成真实触发时间\n        if \'$now\' in target[\'influxQL\']:\n            bind_params[\'now\'] = _to_iso_time(int(time.time()))\n\n        db_res = DFF.SRC(\'df_influxdb\').query(target[\'influxQL\'], bind_params=bind_params)\n\n    # 最终返回的valid_series_data是 由多个dict组成的list\n    valid_series_data = []\n    if \'series\' in db_res:\n        for s in db_res[\'series\']:\n            if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                continue\n\n            valid_series_data.append(s)\n\n    return valid_series_data\n\n\n# 步骤3：异常检测\n# 第二步返回的数据结果是 list of dict，第3步将遍历每一个dict（每个dict就是一条时间序列），进行单时间序列的异常检测\n\n\ndef _preprocessing(data):\n    \"\"\"\n    data为dict类型数据，处理成pd.Series\n    如果最后一个数据点为空，则剔除，其余为空则补0\n    \"\"\"\n    print(data)\n    df = pd.DataFrame(data[\'values\'])\n    df.index = df[0].apply(lambda x: x.replace(\'T\', \' \').replace(\'Z\', \'\'))\n    df = df[1]\n    df.index = pd.to_datetime(df.index)\n    df = df.astype(float)\n    if df.isnull().iloc[-1]:\n        df = df.iloc[:-1]\n    df.fillna(0, inplace=True)\n    print(df)\n    return df\n\n\ndef _range_check(df, trigger_condition, window=30, c=3.0):\n    \"\"\"\n    区间检测，对于要检测的每个点，根据这个点之前的历史n个点形成上区间和下区间，判定其是否异常点\n    注：目前是 使用历史 30 个点，计算四分位距，中位数median， 上区间：median + 3*四分位距 下区间：median -3*四分位距 .\n\n    :param df:  pd.Series. 要检测的时间序列数据 index: datetime  value: float\n    :param trigger_condition: dict. 异常检测器的参数设置\n    :param window: int. 移动时间窗口点数\n    :param c: float. Factor used to determine the bound of normal range (betweeen median-c*IQR and median+c*IQR).\n\n    :return: None or dict.\n    \"\"\"\n\n    # validate_series 是 adtk包的函数，可以对时序pandas数据进行预处理\n    df = validate_series(df)\n\n    # 滚动判断每个点是否异常\n    new_df = pd.DataFrame(df.values, index=df.index)\n    new_df[\'rolling_median\'] = new_df[0].rolling(window).median()\n    new_df[\'rolling_iqr\'] = new_df[0].rolling(window).quantile(0.75) - new_df[0].rolling(window).quantile(0.25)\n    new_df[\'up\'] = new_df[\'rolling_median\'] + c * new_df[\'rolling_iqr\']\n    new_df[\'down\'] = new_df[\'rolling_median\'] - c * new_df[\'rolling_iqr\']\n    range_df = new_df[[\'up\', \'down\']].copy()\n\n    if trigger_condition[\'direction\'] == \'up\':\n        range_df[\'down\'] = -float(\'inf\')\n    elif trigger_condition[\'direction\'] == \'down\':\n        range_df[\'up\'] = float(\'inf\')\n\n    anomalies = (new_df[0] > range_df[\'up\']) | (new_df[0] < range_df[\'down\'])\n    anomalies = anomalies.astype(int)\n\n    # 只检测最近的 point_num 个点\n    point_num = int(trigger_condition[\'periodNum\'])\n\n    anomalies = anomalies.iloc[-point_num:]\n    anomalies = anomalies[anomalies == 1]\n    anomalies = df[anomalies.index]\n\n    # 只有anomalies比例 超过 checkPercent时，才当做关键事件\n    check_count = int(np.ceil(trigger_condition[\'checkPercent\'] / 100 * point_num))\n    # 刚好触达 checkCount的那个异常点所在的时刻 定义为 关键事件的时刻\n    if anomalies.shape[0] < check_count:\n        res = None\n    else:\n        dps = [[int(k.timestamp()), v] for k, v in anomalies.items()]\n        new_df = new_df[-point_num:]\n        up_dps = [[int(k.timestamp()), v] for k, v in new_df[\'up\'].items()]\n        down_dps = [[int(k.timestamp()), v] for k, v in new_df[\'down\'].items()]\n        res = {\n            \'timestamp\': int(anomalies.index[check_count - 1].timestamp()),\n            \'anomalies_dps\': dps,\n            \'up_dps\': up_dps,\n            \'down_dps\': down_dps,\n            \'anomaly\': anomalies.iloc[check_count - 1]\n        }\n        print(res)\n    return res\n\n\n# 步骤4：生成关键事件，和告警内容；存入关键事件，实施告警\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\n\ndef _get_event_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    return _get_md5(str_to_md5)\n\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, event_item_tags, event_id, duration, check_value_map=None,\n                     prev_level=None):\n    \"\"\"\n    创建变量字典\n    相关变量定义参考 模板变量文档\n    https://help.dataflux.cn/doc/1ee7e06f0f06162dd3ee3bd834611b6f0c3100d8\n\n    注：目前prev_level默认缺失，即默认为 OK\n    \"\"\"\n\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\': level,\n        \'status\': level,\n        \'prevLevel\': prev_level,\n        \'prevStatus\': prev_level,\n        \'levelName\': LEVEL_NAME_MAP[level],\n        \'statusName\': LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'prevStatusName\': prev_level_name,\n        \'tags\': _json_dumps(event_item_tags),\n        \'tagsEscaped\': None,\n        \'fields\': None,\n        \'fieldsEscaped\': None,\n        \'duration\': duration * 1000 if duration is not None else None,\n        \'durationHuman\': None,\n        \'ruleId\': trigger_uuid,\n        \'ruleName\': trigger_name,\n        \'alertId\': event_id,\n        \'eventId\': event_id\n    }\n\n    # 补全tags.KEY\n    for k, v in event_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全 fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全 fieldsEscaped、tagsEscaped\n    # 在json结构的string类型值里面再放tags, fields 的 json, 需要转义\n    # 如果人家填写的整个body是 {\"tags\": \"#{tags}\"} ，最终结果会变成\n    # {\"tags\": \"{\"a\": 1}\"}，这个是错误的json结构\n    # 因此提供了转义后的json字符串，保证上述例子输出为 {\"tags\": \"{\\\"a\\\": 1}\"}\n    # 如果人家body填的是 {\"tags\":  # {tags} } 就没事，最终结果为 {\"tags\": {\"a\": 1}}\n\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    if duration is not None:\n        _m, _s = divmod(duration, 60)\n        _h, _m = divmod(_m, 60)\n        _d, _h = divmod(_h, 24)\n        _human = \'{}秒\'.format(_s)\n        if _m > 0:\n            _human = \'{}分钟\'.format(_m) + _human\n        if _h > 0:\n            _human = \'{}小时\'.format(_h) + _human\n        if _d > 0:\n            _human = \'{}天\'.format(_d) + _human\n\n        var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to = action[\'to\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\': to,\n            \'title\': title,\n            \'content\': {\'text\': content},\n            \'sender\': \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook = action[\'webhook\']\n    title = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\': title,\n                \'text\': markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url = action[\'url\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\': var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\': profwang_level,\n            \'type\': \'alarm\',\n            \'title\': title,\n            \'content\': content,\n            \'suggestion\': suggestion,\n            \'origin\': \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': \'POST\',\n                \'url\': _url,\n                \'body\': _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    url = action[\'url\']\n    method = action[\'method\'].upper()\n    headers = action.get(\'headers\')\n    body = action.get(\'body\') or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': method,\n                \'url\': _url,\n                \'headers\': headers,\n                \'query\': None,\n                \'body\': body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'url\': url,\n            \'method\': method,\n            \'headers\': headers,\n            \'body\': body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name = action[\'scriptRefName\']\n    func_ref_name = action[\'funcRefName\']\n    kwargs = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\': script_ref_name,\n            \'funcRefName\': func_ref_name,\n            \'kwargs\': kwargs,\n        }),\n    }\n    return action_content, None\n\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\': _create_action_debug,\n    \'mail\': _create_action_mail,\n    \'dingTalkRobot\': _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\': _create_action_http_request,\n    \'DataFluxFunc\': _create_action_dataflux_func,\n}\n\n\ndef _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level, event_item_tags,\n                                  event_id, duration, check_value_map, extra_alert_tags=None, is_no_data=False):\n    trigger_uuid = trigger_info[\'uuid\']\n    trigger_name = trigger_info[\'name\']\n    # dimensions = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    var_dict = _create_var_dict(\n        trigger_uuid=trigger_uuid,\n        trigger_name=trigger_name,\n        level=level,\n        prev_level=prev_level,\n        event_item_tags=event_item_tags,\n        event_id=event_id,\n        duration=duration,\n        check_value_map=check_value_map)\n\n    for action in actions:\n        action_type = action[\'type\']\n\n        # 记录执行动作\n        _action = _json_copy(action)\n        action_content = None\n        action_task = None\n        try:\n            action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n        except Exception as e:\n            print(e)\n        else:\n            if action_task:\n                if isinstance(action_task, list):\n                    actions_to_perform.extend(action_task)\n                else:\n                    actions_to_perform.append(action_task)\n\n        # 记录写入__keyevent\n        print(action_content)\n        _title = action_content[\'title\']\n        _content = action_content.get(\'content\') or None\n        # _suggestion = action_content.get(\'suggestion\') or None\n        # _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n        # _keyevent_type = None\n        # if is_no_data:\n        #     _keyevent_type = \'noData\'\n\n        tags = {\n            \'__status\': level,\n            \'__eventId\': event_id,\n            \'__source\': \'datafluxTrigger\',\n            \'__ruleId\': trigger_uuid,\n            \'__ruleName\': trigger_name,\n            \'__actionType\': action_type\n        }\n        if extra_alert_tags:\n            tags.update(extra_alert_tags)\n        tags.update(event_item_tags)\n\n        __targets = [{\n            \'qtype\': targets[0][\'qtype\'],\n            \'query\': targets[0][\'query\']\n        }]\n\n        __outlierDps = [\n            {\'dps\': anomalies[\'anomalies_dps\']\n             }\n        ]\n\n        __extendDps = [\n            {\n                \'dps\': anomalies[\'up_dps\']\n            },\n            {\n                \'dps\': anomalies[\'down_dps\']\n            }\n        ]\n\n        fields = {\n            \'__title\': _title,\n            \'__content\': _content,\n            \'__dimensions\': json.dumps(trigger_info[\'dimensions\'], ensure_ascii=False, separators=(\',\', \':\')),\n            \'__targets\': json.dumps(__targets, ensure_ascii=False, separators=(\',\', \':\')),\n            \'__outlierDps\': json.dumps(__outlierDps, ensure_ascii=False, separators=(\',\', \':\')),\n            \'__extendDps\': json.dumps(__extendDps, ensure_ascii=False, separators=(\',\', \':\'))\n        }\n\n        key_event = {\n            \'measurement\': \'__keyevent\',\n            \'tags\': tags,\n            \'fields\': fields,\n            \'timestamp\': anomalies[\'timestamp\']\n        }\n\n        keyevents_to_write.append(key_event)\n\n    return keyevents_to_write, actions_to_perform\n\n\ndef _perform_actions(actions_to_perform):\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n\n\ndef _write_keyevents(keyevents_to_write, trigger_info):\n    # 批量写入关键事件\n    # 实例化dataway对象，用于写入数据（行协议）\n    dataway = DFF.SRC(\'df_dataway\', token=trigger_info[\'workspaceToken\'])\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    # 利用行协议方法写入dataway，接口返回200表示写入成功\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.post_line_protocol(points=keyevents_to_write, path=\'/v1/write/keyevent\')\n        print(\'DataWay返回: {}\'.format(_resp))\n    else:\n        print(\'没有key event需要写入\')\n\n\n@DFF.API(\'区间检测\', category=\'builtinCheck\', is_hidden=True)\ndef range_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    \"\"\"\n    区间检测，利用历史数据预测出数据点的上下区间，超出区间外的为异常点\n    参数如下：\n        1. trigger_info {dict} 异常检测规则（以前叫触发器）的基本信息，结构如下：\n            {\n              \"uuid\"          : \"str, <UUID> 即规则ID\",\n              \"name\"          : \"str, <名称> 即规则名称\",\n              \"workspaceToken\": \"str, <工作空间令牌> 用于判断出事件之后写入__keyevent\",\n              \"dimensions\"    : \"<触发维度> list类型，元素为str\"\n            }\n\n        2. targets {list} 检测的目标，结构如下：\n            [\n              {\n                \"alias\"     : \"str, <别名> 默认即M1, M2 ...，用户可以改为其他内容，用于各类告警文案、数据填充\",\n                \"influxQL\"  : \"str, <InfluxDB 查询语句>\", 注：实际查询时需 替换占位符（$now) 为当前的时刻;\n                                注：influxQL里，where的时间窗里面，取 $now - （聚合周期个数+100）* 聚合周期 到 $now 的数据\",\n                \"qtype\"     : \"str, 如http，注：该字段内容算法用不上，只是当写入keyevennt时原封不动一并传入\",\n                \"query\"     : {\"注：该字段内容算法用不上，只是当写入keyevennt时一并传入，目的是使前端可以拉取keyevent同时也能根                                        据query获取原始的时序数据。该字段结构由后端和前端一起定义\"}\n              }\n            ]\n\n        3. condition_check_setting {dict} 异常检测的触发条件配置，结构如下，分为检测规则和告警动作：\n            {\n              \"rules\": [\n              {\n                \"triggerCondition\": {\n                    \"periodNum\": \"int, 检测几个聚合周期\",\n                    \"strength\": \"int, 检测的强度，目前页面上可选强，中，弱，传过来是数字3，2，1，强度越高代表异常的判定越苛刻\",\n                    \"direction\": \"str, 检测的方向, 页面上三个单选项对应的是 up, down, both\",\n                    \"checkPercent\": \"int，1-100，当异常点数百分比 >= 该值时，触发事件\"\n                },\n                \"level\": \"str, 异常级别，目前默认 warning，写死\"\n            }],\n              \"actions\": [\n                {\n                  \"type\"   : \"mail\",\n                  \"to\"     : [\"邮件地址1\", \"邮件地址2\"],\n                  \"title\"  : \"<邮件标题>\",\n                  \"content\": \"<邮件内容>\"\n                },\n                {\n                  \"type\"    : \"dingTalkRobot\",\n                  \"webhook\" : [\"钉钉机器人调用地址1\", \"钉钉机器人调用地址2\"],\n                  \"title\"   : \"<通知标题>\",\n                  \"markdown\": \"<MarkDown格式通知内容>\"\n                },\n                {\n                  \"type\"   : \"ProfWangAlertHub\",\n                  \"url\"    : [\"王教授AlertHub调用地址1\", \"王教授AlertHub调用地址2\"],\n                  \"title\"  : \"<情报标题>\",\n                  \"content\": \"<情报内容>\"\n                },\n                {\n                  \"type\"    : \"HTTPRequest\",\n                  \"url\"     : [\"请求地址1\", \"请求地址2\"],\n                  \"method\"  : \"<请求方式> GET|POST\",\n                  \"headers\" : {\"Header1\": \"HeaderValue1\", \"Header2\": \"HeaderValue2\"},\n                  \"body\"    : \"<method=POST 请求体>\",\n                  \"bodyType\": \"<method=POST 请求体类型> text|json|form\"\n                },\n                {\n                  \"type\"            : \"DataFluxFunc\",\n                  \"scriptSetRefName\": \"<脚本集引用名>\",\n                  \"scriptRefName\"   : \"<脚本引用名>\",\n                  \"funcRefName\"     : \"<函数名>\",\n                  \"kwargs\"          : { \"<调用参数名>\": \"<调用参数值>\" }\n                }\n              ]\n            }\n\n        4. all_time:  \"dict, 检测频率，数据聚合的周期\"\n            {\n                \"every_time\": \"str, 每隔多久检测一次\",\n                \"group_time\": \"str, 时间聚合周期\",\n            }\n\n        5. extra_alert_tags: {dict} 额外添加的告警数据标签\n            {\n                \"extra_alert_tags\": \"str, 异常\"\n            }\n    \"\"\"\n    _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags)\n\n    db_res = _get_data(targets)\n\n    trigger_condition = condition_check_setting[\'rules\'][0][\'triggerCondition\']\n\n    for data in db_res:\n        print(data[\'tags\'])\n        df = _preprocessing(data)\n        anomalies = _range_check(df, trigger_condition)\n        if anomalies is not None:\n            print(\'异常时刻: \', anomalies[\'timestamp\'])\n            print(\'异常时间: \', datetime.datetime.fromtimestamp(anomalies[\'timestamp\'] + 28800))\n\n            actions = condition_check_setting[\'actions\']\n\n            level = condition_check_setting[\'rules\'][0][\'level\']\n            prev_level = None\n\n            event_item_tags = data[\'tags\']\n\n            # 得到event_id，异常检测器ID + 这条时序数据的tags 得到唯一的event_id\n            # 字典对象序列化，变成字符串\n            event_item_key = json.dumps(event_item_tags, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n            event_id = _get_event_id(trigger_info[\'uuid\'], event_item_key)\n\n            duration = None\n\n            field_alias = data[\'columns\'][1]\n            check_value_map = {field_alias: anomalies[\'anomaly\']}\n\n            keyevents_to_write, actions_to_perform = \\\n                _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level,\n                                              event_item_tags, event_id, duration, check_value_map,\n                                              extra_alert_tags=extra_alert_tags)\n            print(keyevents_to_write)\n            print(\'-------------------------------------------------\')\n            print(actions_to_perform)\n            _write_keyevents(keyevents_to_write, trigger_info)\n            _perform_actions(actions_to_perform)\n        else:\n            print(\'没有异常\')\n\n    return 1\n\n\ndef test():\n\n    # 模拟一个完整参数\n    kwargs = {\n        \"targets\": [\n          {\n            \"alias\": \"M1\",\n            \"qtype\": \"http\",\n            \"query\": {\n              \"fields\": [\n                {\n                  \"args\": [\n                    {\n                      \"name\": \"感染人数\"\n                    }\n                  ],\n                  \"name\": \"感染人数\",\n                  \"alias\": \"M1\",\n                  \"content\": \"\",\n                  \"funcName\": \"max\",\n                  \"fieldType\": \"integer\"\n                }\n              ],\n              \"filter\": {\n                \"tags\": [\n                  {\n                    \"name\": \"症状\",\n                    \"value\": [\n                      \"发烧\"\n                    ],\n                    \"condition\": \"\",\n                    \"operation\": \"=\"\n                  }\n                ]\n              },\n              \"period\": \"1m\",\n              \"groupBy\": [\n                \"症状\",\n                \"time(1m)\"\n              ],\n              \"measurements\": [\n                \"区间检测\"\n              ]\n            },\n            \"influxQL\": \"select (max(\\\"感染人数\\\")) as \\\"M1\\\" from \\\"biz_7ee6760f78ae4f2db672dfcd72be1aa8\\\".\\\"rp5\\\".\\\"区间检测\\\" where (\\\"time\\\" >=  $now - 110m and \\\"time\\\" <  $now ) and (\\\"症状\\\" = \'发烧\') group by \\\"症状\\\", time(1m)\"\n          }\n        ],\n        \"all_time\": {\n          \"every_time\": \"1m\",\n          \"group_time\": \"1m\"\n        },\n        \"trigger_info\": {\n          \"name\": \"区间检测\",\n          \"uuid\": \"rul_aa7c40aeb52a11eabdf68a480da894f2\",\n          \"dimensions\": [\n            \"症状\"\n          ],\n          \"workspaceToken\": \"tkn_69c090f66e14462787327653ba251e60\"\n        },\n        \"extra_alert_tags\": {\n          \"name\": \"区间检测\"\n        },\n        \"condition_check_setting\": {\n          \"rules\": [\n            {\n              \"level\": \"warning\",\n              \"triggerCondition\": {\n                \"direction\": \"both\",\n                \"periodNum\": 10,\n                \"checkPercent\": 10\n              }\n            }\n          ],\n          \"actions\": [\n            {\n              \"to\": [\n                \"chenyiyun@jiagouyun.com\"\n              ],\n              \"type\": \"mail\",\n              \"title\": \"区间检测 suanfa test\",\n              \"content\": \"您的区间检测存在异常，请及时处理！suanfa test\"\n            }\n          ]\n        }\n      }\n\n    range_check(**kwargs)\n', 'cc94715daac40a653613e8faf3f447d1', '\"\"\"\n区间检测\n\n注：目前调度是整分钟的01秒调用，根据InfluxQL里的聚合，如果按分钟聚合，最后一个数据点是该分钟聚合这一秒的数据（往往是缺失）\n目前的处理：最后一个数据点为空，则剔除，其他数据点的空值填补成0\n\n2020-06-18  解析参数，验证参数，获取数据，处理数据&异常检测，如果触发事件则利用dataway行协议写入__keyevent\n\n2020-06-28 新增支持 告警功能 & 模板变量 功能\n\"\"\"\n\n\nimport hashlib\nimport re\nimport time\nimport datetime\nimport arrow\nimport simplejson as json\nimport six\nimport pandas as pd\nimport numpy as np\nfrom adtk.data import validate_series\n\n_DFF_is_debug = True\nLEVEL_NAME_MAP = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nOK_LEVEL = \'ok\'\nCONDITION_LOGICS = (\'and\', \'or\')\nCONDITION_OPERATORS = \\\n    (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nALL_LEVELS = (\'critical\', \'error\', \'warning\', \'info\')\nACTION_TYPES = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS = (\'GET\', \'POST\')\nHTTP_BODY_TYPES = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\nMOCK_DATA = None\nRAISE_DATAWAY_EXCEPTION = True\n\n# 基础函数\n\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n    return six.ensure_str(s)\n\n\n# 以下函数为 模板变量功能 & 进阶模板功能 所需要的功能函数\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from = from_unit.lower()\n    _search_to = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n                                  carry=1000)\n\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n                                  carry=1024)\n\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n                                  carry=1024)\n\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n                                  carry=1024)\n\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\': __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\': __filter_volumn,\n    \'volumni\': __filter_volumn_i,\n    \'bitrate\': __filter_bit_rate,\n    \'byterate\': __filter_byte_rate,\n    \'upper\': __filter_upper,\n    \'lower\': __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\': __filter_if_true,\n    \'iffalse\': __filter_if_false,\n}\n\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, six.string_types):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result = DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\n\n# 步骤1：参数校验\ndef __param_targets_check(targets):\n    if not isinstance(targets, list):\n        raise Exception(\'`targets` should be a list\')\n\n    for i, t in enumerate(targets):\n        if not isinstance(t, dict):\n            raise Exception(\'`targets[{}]` should be a dict\'.format(i))\n\n        if not isinstance(t.get(\'alias\'), str):\n            raise Exception(\'`targets[{}][\"alias\"]` should be a str\'.format(i))\n\n        if \'influxQL\' in t and not isinstance(t.get(\'influxQL\'), str):\n            raise Exception(\'`targets[{}][\"influxQL\"]` should be a str\'.format(i))\n\n        if \'expression\' in t and not isinstance(t.get(\'expression\'), str):\n            raise Exception(\'`targets[{}][\"expression\"]` should be a str\'.format(i))\n\n\ndef __param_rules_check(rules):\n    if not isinstance(rules, list):\n        raise Exception(\'`rules` should be a list\')\n\n    for i, r in enumerate(rules):\n        if not isinstance(r, dict):\n            raise Exception(\'`rules[{}]` should be a dict\'.format(i))\n\n        if r.get(\'level\') not in ALL_LEVELS:\n            raise Exception(\'`rules[{}][\"level\"]` should be one of {}\'.format(i, \',\'.join(ALL_LEVELS)))\n\n        if not isinstance(r.get(\'triggerCondition\'), dict):\n            raise Exception(\'`rules[{}][\"triggerCondition\"]` should be a dict\'.format(i))\n\n        c = r[\'triggerCondition\']\n        if not isinstance(c.get(\'periodNum\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"periodNum\"]` should be an int\'.format(i))\n\n        # if not isinstance(c.get(\'strength\'), int):\n        #     raise Exception(\'`rules[{}][\"triggerCondition\"][\"strength\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'direction\'), str):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be a str\'.format(i))\n\n        d = c.get(\'direction\')\n        if d not in [\'up\', \'down\', \'both\']:\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be `up` or `down` or `both`\'.format(i))\n\n        if not isinstance(c.get(\'checkPercent\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"checkPercent\"]` should be an int\'.format(i))\n\n\ndef __param_action_check(actions, parent_path):\n    if not isinstance(actions, list):\n        raise Exception(\'`{}actions` should be a list\'.format(parent_path))\n\n    for i, a in enumerate(actions):\n        if not isinstance(a, dict):\n            raise Exception(\'`{}actions[{}]` should be a dict\'.format(parent_path, i))\n\n        if a.get(\'type\') not in ACTION_TYPES:\n            raise Exception(\n                \'`{}actions[{}][\"type\"]` should be one of {}\'.format(parent_path, i, \',\'.join(ACTION_TYPES)))\n\n        if a[\'type\'] == \'mail\':\n            if not isinstance(a.get(\'to\'), list):\n                raise Exception(\'`{}actions[{}][\"to\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'to\'])):\n                raise Exception(\'Element of `{}actions[{}][\"to\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'dingTalkRobot\':\n            if not isinstance(a.get(\'webhook\'), list):\n                raise Exception(\'`{}actions[{}][\"webhook\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'webhook\'])):\n                raise Exception(\'Element of `{}actions[{}][\"webhook\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'markdown\'), str):\n                raise Exception(\'`{}actions[{}][\"markdown\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'ProfWangAlertHub\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'suggestion\') is not None:\n                if not isinstance(a[\'suggestion\'], str):\n                    raise Exception(\'`{}actions[{}][\"suggestion\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'HTTPRequest\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'method\') not in HTTP_METHODS:\n                raise Exception(\n                    \'`{}actions[{}][\"method\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_METHODS)))\n\n            if a.get(\'headers\') is not None:\n                if not isinstance(a[\'headers\'], dict):\n                    raise Exception(\'`{}actions[{}][\"headers\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].values())):\n                    raise Exception(\'Value of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n            if a[\'method\'] == \'POST\':\n                if a.get(\'bodyType\') not in HTTP_BODY_TYPES:\n                    raise Exception(\'`{}actions[{}][\"bodyType\"]` should be one of {}\'.format(parent_path, i,\n                                                                                             \',\'.join(HTTP_BODY_TYPES)))\n\n                if not isinstance(a.get(\'body\'), str):\n                    raise Exception(\'`{}actions[{}][\"body\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'DataFluxFunc\':\n            if not isinstance(a.get(\'scriptSetRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptSetRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'scriptRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'funcRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"funcRefName\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'kwargs\') is not None:\n                if not isinstance(a[\'kwargs\'], dict):\n                    raise Exception(\'`{}actions[{}][\"kwargs\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'kwargs\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"kwargs\"]` should be a str\'.format(parent_path, i))\n\n\ndef _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    # 1. 检查`trigger_info`\n    if not isinstance(trigger_info, dict):\n        raise Exception(\'`trigger_info` should be a dict\')\n\n    if not isinstance(trigger_info.get(\'uuid\'), str):\n        raise Exception(\'`trigger_info[\"uuid\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'name\'), str):\n        raise Exception(\'`trigger_info[\"name\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'workspaceToken\'), str):\n        raise Exception(\'`trigger_info[\"workspaceToken\"]` should be a str\')\n\n    dimensions = trigger_info.get(\'dimensions\')\n    if dimensions is not None:\n        if not isinstance(dimensions, list):\n            raise Exception(\'`trigger_info[\"dimensions\"]` should be a list\')\n\n        for index, dimension in enumerate(dimensions):\n            if not isinstance(dimension, str):\n                raise Exception(\'`trigger_info[\"dimensions\"][{}]` should be a str\'.format(index))\n\n    # 2. 检查`targets`\n    __param_targets_check(targets)\n\n    # 3. 检查`condition_check_setting`\n    if not isinstance(condition_check_setting, dict):\n        raise Exception(\'`condition_check_setting` should be a dict\')\n\n    __param_rules_check(condition_check_setting.get(\'rules\'))\n    __param_action_check(condition_check_setting.get(\'actions\'), parent_path=\'condition_check_setting.\')\n\n    # 4. 检查`all_time`\n    if not isinstance(all_time, dict):\n        raise Exception(\'`all_time` should be a dict\')\n\n    if not isinstance(all_time.get(\'every_time\'), str):\n        raise Exception(\'`all_time[\"every_time\"]` should be a str\')\n\n    if not isinstance(all_time.get(\'group_time\'), str):\n        raise Exception(\'`all_time[\"group_time\"]` should be a str\')\n\n    # 正则表达式 匹配 *s, *m, *h, *d, *w, *M (秒，分，时，天，周，月)\n    match_every_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'every_time\'))\n    if not (match_every_time is not None and match_every_time.span(0)[1] == len(all_time.get(\'every_time\'))):\n        raise Exception(\'`all_time[\"every_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    match_group_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'group_time\'))\n    if not (match_group_time is not None and match_group_time.span(0)[1] == len(all_time.get(\'group_time\'))):\n        raise Exception(\'`all_time[\"group_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    # 5. 检查 `extra_alert_tags`\n    if extra_alert_tags is not None:\n        if not isinstance(extra_alert_tags, dict):\n            raise Exception(\'`extra_alert_tags` should be an dict\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.keys())):\n            raise Exception(\'Key of `extra_alert_tags` should be a str\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.values())):\n            raise Exception(\'Value of `extra_alert_tags` should be a str\')\n\n\n# 步骤2：获取数据\ndef _to_iso_time(d=None):\n    # 将unix时间戳转化成iso8601时间格式\n    return arrow.get(d).isoformat()\n\n\ndef _get_data(targets):\n\n    # 根据targets里的InfluxQL语句，获取数据，targets是个列表，目前仅支持单个target，所以取第一个\n    target = targets[0]\n    db_res = None\n    if MOCK_DATA:\n        db_res = MOCK_DATA[target[\'alias\']]\n    else:\n        bind_params = {}\n        # 转成真实触发时间\n        if \'$now\' in target[\'influxQL\']:\n            bind_params[\'now\'] = _to_iso_time(int(time.time()))\n\n        db_res = DFF.SRC(\'df_influxdb\').query(target[\'influxQL\'], bind_params=bind_params)\n\n    # 最终返回的valid_series_data是 由多个dict组成的list\n    valid_series_data = []\n    if \'series\' in db_res:\n        for s in db_res[\'series\']:\n            if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                continue\n\n            valid_series_data.append(s)\n\n    return valid_series_data\n\n\n# 步骤3：异常检测\n# 第二步返回的数据结果是 list of dict，第3步将遍历每一个dict（每个dict就是一条时间序列），进行单时间序列的异常检测\n\n\ndef _preprocessing(data):\n    \"\"\"\n    data为dict类型数据，处理成pd.Series\n    如果最后一个数据点为空，则剔除，其余为空则补0\n    \"\"\"\n    print(data)\n    df = pd.DataFrame(data[\'values\'])\n    df.index = df[0].apply(lambda x: x.replace(\'T\', \' \').replace(\'Z\', \'\'))\n    df = df[1]\n    df.index = pd.to_datetime(df.index)\n    df = df.astype(float)\n    if df.isnull().iloc[-1]:\n        df = df.iloc[:-1]\n    df.fillna(0, inplace=True)\n    print(df)\n    return df\n\n\ndef _range_check(df, trigger_condition, window=30, c=3.0):\n    \"\"\"\n    区间检测，对于要检测的每个点，根据这个点之前的历史n个点形成上区间和下区间，判定其是否异常点\n    注：目前是 使用历史 30 个点，计算四分位距，中位数median， 上区间：median + 3*四分位距 下区间：median -3*四分位距 .\n\n    :param df:  pd.Series. 要检测的时间序列数据 index: datetime  value: float\n    :param trigger_condition: dict. 异常检测器的参数设置\n    :param window: int. 移动时间窗口点数\n    :param c: float. Factor used to determine the bound of normal range (betweeen median-c*IQR and median+c*IQR).\n\n    :return: None or dict.\n    \"\"\"\n\n    # validate_series 是 adtk包的函数，可以对时序pandas数据进行预处理\n    df = validate_series(df)\n\n    # 滚动判断每个点是否异常\n    new_df = pd.DataFrame(df.values, index=df.index)\n    new_df[\'rolling_median\'] = new_df[0].rolling(window).median()\n    new_df[\'rolling_iqr\'] = new_df[0].rolling(window).quantile(0.75) - new_df[0].rolling(window).quantile(0.25)\n    new_df[\'up\'] = new_df[\'rolling_median\'] + c * new_df[\'rolling_iqr\']\n    new_df[\'down\'] = new_df[\'rolling_median\'] - c * new_df[\'rolling_iqr\']\n    range_df = new_df[[\'up\', \'down\']].copy()\n\n    if trigger_condition[\'direction\'] == \'up\':\n        range_df[\'down\'] = -float(\'inf\')\n    elif trigger_condition[\'direction\'] == \'down\':\n        range_df[\'up\'] = float(\'inf\')\n\n    anomalies = (new_df[0] > range_df[\'up\']) | (new_df[0] < range_df[\'down\'])\n    anomalies = anomalies.astype(int)\n\n    # 只检测最近的 point_num 个点\n    point_num = int(trigger_condition[\'periodNum\'])\n\n    anomalies = anomalies.iloc[-point_num:]\n    anomalies = anomalies[anomalies == 1]\n    anomalies = df[anomalies.index]\n\n    # 只有anomalies比例 超过 checkPercent时，才当做关键事件\n    check_count = int(np.ceil(trigger_condition[\'checkPercent\'] / 100 * point_num))\n    # 刚好触达 checkCount的那个异常点所在的时刻 定义为 关键事件的时刻\n    if anomalies.shape[0] < check_count:\n        res = None\n    else:\n        dps = [[int(k.timestamp()), v] for k, v in anomalies.items()]\n        new_df = new_df[-point_num:]\n        up_dps = [[int(k.timestamp()), v] for k, v in new_df[\'up\'].items()]\n        down_dps = [[int(k.timestamp()), v] for k, v in new_df[\'down\'].items()]\n        res = {\n            \'timestamp\': int(anomalies.index[check_count - 1].timestamp()),\n            \'anomalies_dps\': dps,\n            \'up_dps\': up_dps,\n            \'down_dps\': down_dps,\n            \'anomaly\': anomalies.iloc[check_count - 1]\n        }\n        print(res)\n    return res\n\n\n# 步骤4：生成关键事件，和告警内容；存入关键事件，实施告警\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\n\ndef _get_event_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    return _get_md5(str_to_md5)\n\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, event_item_tags, event_id, duration, check_value_map=None,\n                     prev_level=None):\n    \"\"\"\n    创建变量字典\n    相关变量定义参考 模板变量文档\n    https://help.dataflux.cn/doc/1ee7e06f0f06162dd3ee3bd834611b6f0c3100d8\n\n    注：目前prev_level默认缺失，即默认为 OK\n    \"\"\"\n\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\': level,\n        \'status\': level,\n        \'prevLevel\': prev_level,\n        \'prevStatus\': prev_level,\n        \'levelName\': LEVEL_NAME_MAP[level],\n        \'statusName\': LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'prevStatusName\': prev_level_name,\n        \'tags\': _json_dumps(event_item_tags),\n        \'tagsEscaped\': None,\n        \'fields\': None,\n        \'fieldsEscaped\': None,\n        \'duration\': duration * 1000 if duration is not None else None,\n        \'durationHuman\': None,\n        \'ruleId\': trigger_uuid,\n        \'ruleName\': trigger_name,\n        \'alertId\': event_id,\n        \'eventId\': event_id\n    }\n\n    # 补全tags.KEY\n    for k, v in event_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全 fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全 fieldsEscaped、tagsEscaped\n    # 在json结构的string类型值里面再放tags, fields 的 json, 需要转义\n    # 如果人家填写的整个body是 {\"tags\": \"#{tags}\"} ，最终结果会变成\n    # {\"tags\": \"{\"a\": 1}\"}，这个是错误的json结构\n    # 因此提供了转义后的json字符串，保证上述例子输出为 {\"tags\": \"{\\\"a\\\": 1}\"}\n    # 如果人家body填的是 {\"tags\":  # {tags} } 就没事，最终结果为 {\"tags\": {\"a\": 1}}\n\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    if duration is not None:\n        _m, _s = divmod(duration, 60)\n        _h, _m = divmod(_m, 60)\n        _d, _h = divmod(_h, 24)\n        _human = \'{}秒\'.format(_s)\n        if _m > 0:\n            _human = \'{}分钟\'.format(_m) + _human\n        if _h > 0:\n            _human = \'{}小时\'.format(_h) + _human\n        if _d > 0:\n            _human = \'{}天\'.format(_d) + _human\n\n        var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to = action[\'to\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\': to,\n            \'title\': title,\n            \'content\': {\'text\': content},\n            \'sender\': \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook = action[\'webhook\']\n    title = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\': title,\n                \'text\': markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url = action[\'url\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\': var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\': profwang_level,\n            \'type\': \'alarm\',\n            \'title\': title,\n            \'content\': content,\n            \'suggestion\': suggestion,\n            \'origin\': \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': \'POST\',\n                \'url\': _url,\n                \'body\': _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    url = action[\'url\']\n    method = action[\'method\'].upper()\n    headers = action.get(\'headers\')\n    body = action.get(\'body\') or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': method,\n                \'url\': _url,\n                \'headers\': headers,\n                \'query\': None,\n                \'body\': body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'url\': url,\n            \'method\': method,\n            \'headers\': headers,\n            \'body\': body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name = action[\'scriptRefName\']\n    func_ref_name = action[\'funcRefName\']\n    kwargs = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\': script_ref_name,\n            \'funcRefName\': func_ref_name,\n            \'kwargs\': kwargs,\n        }),\n    }\n    return action_content, None\n\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\': _create_action_debug,\n    \'mail\': _create_action_mail,\n    \'dingTalkRobot\': _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\': _create_action_http_request,\n    \'DataFluxFunc\': _create_action_dataflux_func,\n}\n\n\ndef _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level, event_item_tags,\n                                  event_id, duration, check_value_map, extra_alert_tags=None, is_no_data=False):\n    trigger_uuid = trigger_info[\'uuid\']\n    trigger_name = trigger_info[\'name\']\n    # dimensions = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    var_dict = _create_var_dict(\n        trigger_uuid=trigger_uuid,\n        trigger_name=trigger_name,\n        level=level,\n        prev_level=prev_level,\n        event_item_tags=event_item_tags,\n        event_id=event_id,\n        duration=duration,\n        check_value_map=check_value_map)\n\n    for action in actions:\n        action_type = action[\'type\']\n\n        # 记录执行动作\n        _action = _json_copy(action)\n        action_content = None\n        action_task = None\n        try:\n            action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n        except Exception as e:\n            print(e)\n        else:\n            if action_task:\n                if isinstance(action_task, list):\n                    actions_to_perform.extend(action_task)\n                else:\n                    actions_to_perform.append(action_task)\n\n        # 记录写入__keyevent\n        print(action_content)\n        _title = action_content[\'title\']\n        _content = action_content.get(\'content\') or None\n        # _suggestion = action_content.get(\'suggestion\') or None\n        # _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n        # _keyevent_type = None\n        # if is_no_data:\n        #     _keyevent_type = \'noData\'\n\n        tags = {\n            \'__status\': level,\n            \'__eventId\': event_id,\n            \'__source\': \'datafluxTrigger\',\n            \'__ruleId\': trigger_uuid,\n            \'__ruleName\': trigger_name,\n            \'__actionType\': action_type\n        }\n        if extra_alert_tags:\n            tags.update(extra_alert_tags)\n        tags.update(event_item_tags)\n\n        __targets = [{\n            \'qtype\': targets[0][\'qtype\'],\n            \'query\': targets[0][\'query\']\n        }]\n\n        __outlierDps = [\n            {\'dps\': anomalies[\'anomalies_dps\']\n             }\n        ]\n\n        __extendDps = [\n            {\n                \'dps\': anomalies[\'up_dps\']\n            },\n            {\n                \'dps\': anomalies[\'down_dps\']\n            }\n        ]\n\n        fields = {\n            \'__title\': _title,\n            \'__content\': _content,\n            \'__dimensions\': json.dumps(trigger_info[\'dimensions\'], ensure_ascii=False, separators=(\',\', \':\')),\n            \'__targets\': json.dumps(__targets, ensure_ascii=False, separators=(\',\', \':\')),\n            \'__outlierDps\': json.dumps(__outlierDps, ensure_ascii=False, separators=(\',\', \':\')),\n            \'__extendDps\': json.dumps(__extendDps, ensure_ascii=False, separators=(\',\', \':\'))\n        }\n\n        key_event = {\n            \'measurement\': \'__keyevent\',\n            \'tags\': tags,\n            \'fields\': fields,\n            \'timestamp\': anomalies[\'timestamp\']\n        }\n\n        keyevents_to_write.append(key_event)\n\n    return keyevents_to_write, actions_to_perform\n\n\ndef _perform_actions(actions_to_perform):\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n\n\ndef _write_keyevents(keyevents_to_write, trigger_info):\n    # 批量写入关键事件\n    # 实例化dataway对象，用于写入数据（行协议）\n    dataway = DFF.SRC(\'df_dataway\', token=trigger_info[\'workspaceToken\'])\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    # 利用行协议方法写入dataway，接口返回200表示写入成功\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.post_line_protocol(points=keyevents_to_write, path=\'/v1/write/keyevent\')\n        print(\'DataWay返回: {}\'.format(_resp))\n    else:\n        print(\'没有key event需要写入\')\n\n\n@DFF.API(\'区间检测\', category=\'builtinCheck\', is_hidden=True)\ndef range_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    \"\"\"\n    区间检测，利用历史数据预测出数据点的上下区间，超出区间外的为异常点\n    参数如下：\n        1. trigger_info {dict} 异常检测规则（以前叫触发器）的基本信息，结构如下：\n            {\n              \"uuid\"          : \"str, <UUID> 即规则ID\",\n              \"name\"          : \"str, <名称> 即规则名称\",\n              \"workspaceToken\": \"str, <工作空间令牌> 用于判断出事件之后写入__keyevent\",\n              \"dimensions\"    : \"<触发维度> list类型，元素为str\"\n            }\n\n        2. targets {list} 检测的目标，结构如下：\n            [\n              {\n                \"alias\"     : \"str, <别名> 默认即M1, M2 ...，用户可以改为其他内容，用于各类告警文案、数据填充\",\n                \"influxQL\"  : \"str, <InfluxDB 查询语句>\", 注：实际查询时需 替换占位符（$now) 为当前的时刻;\n                                注：influxQL里，where的时间窗里面，取 $now - （聚合周期个数+100）* 聚合周期 到 $now 的数据\",\n                \"qtype\"     : \"str, 如http，注：该字段内容算法用不上，只是当写入keyevennt时原封不动一并传入\",\n                \"query\"     : {\"注：该字段内容算法用不上，只是当写入keyevennt时一并传入，目的是使前端可以拉取keyevent同时也能根                                        据query获取原始的时序数据。该字段结构由后端和前端一起定义\"}\n              }\n            ]\n\n        3. condition_check_setting {dict} 异常检测的触发条件配置，结构如下，分为检测规则和告警动作：\n            {\n              \"rules\": [\n              {\n                \"triggerCondition\": {\n                    \"periodNum\": \"int, 检测几个聚合周期\",\n                    \"strength\": \"int, 检测的强度，目前页面上可选强，中，弱，传过来是数字3，2，1，强度越高代表异常的判定越苛刻\",\n                    \"direction\": \"str, 检测的方向, 页面上三个单选项对应的是 up, down, both\",\n                    \"checkPercent\": \"int，1-100，当异常点数百分比 >= 该值时，触发事件\"\n                },\n                \"level\": \"str, 异常级别，目前默认 warning，写死\"\n            }],\n              \"actions\": [\n                {\n                  \"type\"   : \"mail\",\n                  \"to\"     : [\"邮件地址1\", \"邮件地址2\"],\n                  \"title\"  : \"<邮件标题>\",\n                  \"content\": \"<邮件内容>\"\n                },\n                {\n                  \"type\"    : \"dingTalkRobot\",\n                  \"webhook\" : [\"钉钉机器人调用地址1\", \"钉钉机器人调用地址2\"],\n                  \"title\"   : \"<通知标题>\",\n                  \"markdown\": \"<MarkDown格式通知内容>\"\n                },\n                {\n                  \"type\"   : \"ProfWangAlertHub\",\n                  \"url\"    : [\"王教授AlertHub调用地址1\", \"王教授AlertHub调用地址2\"],\n                  \"title\"  : \"<情报标题>\",\n                  \"content\": \"<情报内容>\"\n                },\n                {\n                  \"type\"    : \"HTTPRequest\",\n                  \"url\"     : [\"请求地址1\", \"请求地址2\"],\n                  \"method\"  : \"<请求方式> GET|POST\",\n                  \"headers\" : {\"Header1\": \"HeaderValue1\", \"Header2\": \"HeaderValue2\"},\n                  \"body\"    : \"<method=POST 请求体>\",\n                  \"bodyType\": \"<method=POST 请求体类型> text|json|form\"\n                },\n                {\n                  \"type\"            : \"DataFluxFunc\",\n                  \"scriptSetRefName\": \"<脚本集引用名>\",\n                  \"scriptRefName\"   : \"<脚本引用名>\",\n                  \"funcRefName\"     : \"<函数名>\",\n                  \"kwargs\"          : { \"<调用参数名>\": \"<调用参数值>\" }\n                }\n              ]\n            }\n\n        4. all_time:  \"dict, 检测频率，数据聚合的周期\"\n            {\n                \"every_time\": \"str, 每隔多久检测一次\",\n                \"group_time\": \"str, 时间聚合周期\",\n            }\n\n        5. extra_alert_tags: {dict} 额外添加的告警数据标签\n            {\n                \"extra_alert_tags\": \"str, 异常\"\n            }\n    \"\"\"\n    _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags)\n\n    db_res = _get_data(targets)\n\n    trigger_condition = condition_check_setting[\'rules\'][0][\'triggerCondition\']\n\n    for data in db_res:\n        print(data[\'tags\'])\n        df = _preprocessing(data)\n        anomalies = _range_check(df, trigger_condition)\n        if anomalies is not None:\n            print(\'异常时刻: \', anomalies[\'timestamp\'])\n            print(\'异常时间: \', datetime.datetime.fromtimestamp(anomalies[\'timestamp\'] + 28800))\n\n            actions = condition_check_setting[\'actions\']\n\n            level = condition_check_setting[\'rules\'][0][\'level\']\n            prev_level = None\n\n            event_item_tags = data[\'tags\']\n\n            # 得到event_id，异常检测器ID + 这条时序数据的tags 得到唯一的event_id\n            # 字典对象序列化，变成字符串\n            event_item_key = json.dumps(event_item_tags, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n            event_id = _get_event_id(trigger_info[\'uuid\'], event_item_key)\n\n            duration = None\n\n            field_alias = data[\'columns\'][1]\n            check_value_map = {field_alias: anomalies[\'anomaly\']}\n\n            keyevents_to_write, actions_to_perform = \\\n                _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level,\n                                              event_item_tags, event_id, duration, check_value_map,\n                                              extra_alert_tags=extra_alert_tags)\n            print(keyevents_to_write)\n            print(\'-------------------------------------------------\')\n            print(actions_to_perform)\n            _write_keyevents(keyevents_to_write, trigger_info)\n            _perform_actions(actions_to_perform)\n        else:\n            print(\'没有异常\')\n\n    return 1\n\n\ndef test():\n\n    # 模拟一个完整参数\n    kwargs = {\n        \"targets\": [\n          {\n            \"alias\": \"M1\",\n            \"qtype\": \"http\",\n            \"query\": {\n              \"fields\": [\n                {\n                  \"args\": [\n                    {\n                      \"name\": \"感染人数\"\n                    }\n                  ],\n                  \"name\": \"感染人数\",\n                  \"alias\": \"M1\",\n                  \"content\": \"\",\n                  \"funcName\": \"max\",\n                  \"fieldType\": \"integer\"\n                }\n              ],\n              \"filter\": {\n                \"tags\": [\n                  {\n                    \"name\": \"症状\",\n                    \"value\": [\n                      \"发烧\"\n                    ],\n                    \"condition\": \"\",\n                    \"operation\": \"=\"\n                  }\n                ]\n              },\n              \"period\": \"1m\",\n              \"groupBy\": [\n                \"症状\",\n                \"time(1m)\"\n              ],\n              \"measurements\": [\n                \"区间检测\"\n              ]\n            },\n            \"influxQL\": \"select (max(\\\"感染人数\\\")) as \\\"M1\\\" from \\\"biz_7ee6760f78ae4f2db672dfcd72be1aa8\\\".\\\"rp5\\\".\\\"区间检测\\\" where (\\\"time\\\" >=  $now - 110m and \\\"time\\\" <  $now ) and (\\\"症状\\\" = \'发烧\') group by \\\"症状\\\", time(1m)\"\n          }\n        ],\n        \"all_time\": {\n          \"every_time\": \"1m\",\n          \"group_time\": \"1m\"\n        },\n        \"trigger_info\": {\n          \"name\": \"区间检测\",\n          \"uuid\": \"rul_aa7c40aeb52a11eabdf68a480da894f2\",\n          \"dimensions\": [\n            \"症状\"\n          ],\n          \"workspaceToken\": \"tkn_69c090f66e14462787327653ba251e60\"\n        },\n        \"extra_alert_tags\": {\n          \"name\": \"区间检测\"\n        },\n        \"condition_check_setting\": {\n          \"rules\": [\n            {\n              \"level\": \"warning\",\n              \"triggerCondition\": {\n                \"direction\": \"both\",\n                \"periodNum\": 10,\n                \"checkPercent\": 10\n              }\n            }\n          ],\n          \"actions\": [\n            {\n              \"to\": [\n                \"chenyiyun@jiagouyun.com\"\n              ],\n              \"type\": \"mail\",\n              \"title\": \"区间检测 suanfa test\",\n              \"content\": \"您的区间检测存在异常，请及时处理！suanfa test\"\n            }\n          ]\n        }\n      }\n\n    range_check(**kwargs)\n', 'cc94715daac40a653613e8faf3f447d1', '2020-06-29 09:37:40', '2020-06-29 09:41:07');
INSERT INTO `biz_main_script` (`seq`, `id`, `title`, `description`, `scriptSetId`, `refName`, `publishVersion`, `type`, `code`, `codeMD5`, `codeDraft`, `codeDraftMD5`, `createTime`, `updateTime`) VALUES
('9', 'scpt-spike_chk', 'FT突变检测', NULL, 'sset-ft_lib', 'spike_chk', '1', 'python', '\"\"\"\n突变检测\n\n注：目前调度是整分钟的01秒调用，根据InfluxQL里的聚合，如果按分钟聚合，最后一个数据点是该分钟聚合这一秒的数据（往往是缺失）\n目前的处理：最后一个数据点为空，则剔除，其他数据点的空值填补成0\n\n2020-06-18  解析参数，验证参数，获取数据，处理数据&异常检测，如果触发事件则利用dataway行协议写入__keyevent\n\n2020-06-28 新增支持 告警功能 & 模板变量 功能\n\"\"\"\n\n\nimport hashlib\nimport re\nimport time\nimport datetime\nimport arrow\nimport simplejson as json\nimport six\nimport pandas as pd\nfrom adtk.data import validate_series\nfrom adtk.detector import PersistAD\n\n_DFF_is_debug = True\nLEVEL_NAME_MAP = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nOK_LEVEL = \'ok\'\nCONDITION_LOGICS = (\'and\', \'or\')\nCONDITION_OPERATORS = \\\n    (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nALL_LEVELS = (\'critical\', \'error\', \'warning\', \'info\')\nACTION_TYPES = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS = (\'GET\', \'POST\')\nHTTP_BODY_TYPES = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\nMOCK_DATA = None\nRAISE_DATAWAY_EXCEPTION = True\n\n# 基础函数\n\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n    return six.ensure_str(s)\n\n\n# 以下函数为 模板变量功能 & 进阶模板功能 所需要的功能函数\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from = from_unit.lower()\n    _search_to = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n                                  carry=1000)\n\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n                                  carry=1024)\n\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n                                  carry=1024)\n\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n                                  carry=1024)\n\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\': __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\': __filter_volumn,\n    \'volumni\': __filter_volumn_i,\n    \'bitrate\': __filter_bit_rate,\n    \'byterate\': __filter_byte_rate,\n    \'upper\': __filter_upper,\n    \'lower\': __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\': __filter_if_true,\n    \'iffalse\': __filter_if_false,\n}\n\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, six.string_types):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result = DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\n\n# 步骤1：参数校验\ndef __param_targets_check(targets):\n    if not isinstance(targets, list):\n        raise Exception(\'`targets` should be a list\')\n\n    for i, t in enumerate(targets):\n        if not isinstance(t, dict):\n            raise Exception(\'`targets[{}]` should be a dict\'.format(i))\n\n        if not isinstance(t.get(\'alias\'), str):\n            raise Exception(\'`targets[{}][\"alias\"]` should be a str\'.format(i))\n\n        if \'influxQL\' in t and not isinstance(t.get(\'influxQL\'), str):\n            raise Exception(\'`targets[{}][\"influxQL\"]` should be a str\'.format(i))\n\n        if \'expression\' in t and not isinstance(t.get(\'expression\'), str):\n            raise Exception(\'`targets[{}][\"expression\"]` should be a str\'.format(i))\n\n\ndef __param_rules_check(rules):\n    if not isinstance(rules, list):\n        raise Exception(\'`rules` should be a list\')\n\n    for i, r in enumerate(rules):\n        if not isinstance(r, dict):\n            raise Exception(\'`rules[{}]` should be a dict\'.format(i))\n\n        if r.get(\'level\') not in ALL_LEVELS:\n            raise Exception(\'`rules[{}][\"level\"]` should be one of {}\'.format(i, \',\'.join(ALL_LEVELS)))\n\n        if not isinstance(r.get(\'triggerCondition\'), dict):\n            raise Exception(\'`rules[{}][\"triggerCondition\"]` should be a dict\'.format(i))\n\n        c = r[\'triggerCondition\']\n        if not isinstance(c.get(\'periodNum\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"periodNum\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'strength\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"strength\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'direction\'), str):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be a str\'.format(i))\n\n        d = c.get(\'direction\')\n        if d not in [\'up\', \'down\', \'both\']:\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be `up` or `down` or `both`\'.format(i))\n\n        if not isinstance(c.get(\'checkCount\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"checkCount\"]` should be an int\'.format(i))\n\n\ndef __param_action_check(actions, parent_path):\n    if not isinstance(actions, list):\n        raise Exception(\'`{}actions` should be a list\'.format(parent_path))\n\n    for i, a in enumerate(actions):\n        if not isinstance(a, dict):\n            raise Exception(\'`{}actions[{}]` should be a dict\'.format(parent_path, i))\n\n        if a.get(\'type\') not in ACTION_TYPES:\n            raise Exception(\n                \'`{}actions[{}][\"type\"]` should be one of {}\'.format(parent_path, i, \',\'.join(ACTION_TYPES)))\n\n        if a[\'type\'] == \'mail\':\n            if not isinstance(a.get(\'to\'), list):\n                raise Exception(\'`{}actions[{}][\"to\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'to\'])):\n                raise Exception(\'Element of `{}actions[{}][\"to\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'dingTalkRobot\':\n            if not isinstance(a.get(\'webhook\'), list):\n                raise Exception(\'`{}actions[{}][\"webhook\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'webhook\'])):\n                raise Exception(\'Element of `{}actions[{}][\"webhook\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'markdown\'), str):\n                raise Exception(\'`{}actions[{}][\"markdown\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'ProfWangAlertHub\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'suggestion\') is not None:\n                if not isinstance(a[\'suggestion\'], str):\n                    raise Exception(\'`{}actions[{}][\"suggestion\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'HTTPRequest\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'method\') not in HTTP_METHODS:\n                raise Exception(\n                    \'`{}actions[{}][\"method\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_METHODS)))\n\n            if a.get(\'headers\') is not None:\n                if not isinstance(a[\'headers\'], dict):\n                    raise Exception(\'`{}actions[{}][\"headers\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].values())):\n                    raise Exception(\'Value of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n            if a[\'method\'] == \'POST\':\n                if a.get(\'bodyType\') not in HTTP_BODY_TYPES:\n                    raise Exception(\'`{}actions[{}][\"bodyType\"]` should be one of {}\'.format(parent_path, i,\n                                                                                             \',\'.join(HTTP_BODY_TYPES)))\n\n                if not isinstance(a.get(\'body\'), str):\n                    raise Exception(\'`{}actions[{}][\"body\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'DataFluxFunc\':\n            if not isinstance(a.get(\'scriptSetRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptSetRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'scriptRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'funcRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"funcRefName\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'kwargs\') is not None:\n                if not isinstance(a[\'kwargs\'], dict):\n                    raise Exception(\'`{}actions[{}][\"kwargs\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'kwargs\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"kwargs\"]` should be a str\'.format(parent_path, i))\n\n\ndef _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    # 1. 检查`trigger_info`\n    if not isinstance(trigger_info, dict):\n        raise Exception(\'`trigger_info` should be a dict\')\n\n    if not isinstance(trigger_info.get(\'uuid\'), str):\n        raise Exception(\'`trigger_info[\"uuid\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'name\'), str):\n        raise Exception(\'`trigger_info[\"name\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'workspaceToken\'), str):\n        raise Exception(\'`trigger_info[\"workspaceToken\"]` should be a str\')\n\n    dimensions = trigger_info.get(\'dimensions\')\n    if dimensions is not None:\n        if not isinstance(dimensions, list):\n            raise Exception(\'`trigger_info[\"dimensions\"]` should be a list\')\n\n        for index, dimension in enumerate(dimensions):\n            if not isinstance(dimension, str):\n                raise Exception(\'`trigger_info[\"dimensions\"][{}]` should be a str\'.format(index))\n\n    # 2. 检查`targets`\n    __param_targets_check(targets)\n\n    # 3. 检查`condition_check_setting`\n    if not isinstance(condition_check_setting, dict):\n        raise Exception(\'`condition_check_setting` should be a dict\')\n\n    __param_rules_check(condition_check_setting.get(\'rules\'))\n    __param_action_check(condition_check_setting.get(\'actions\'), parent_path=\'condition_check_setting.\')\n\n    # 4. 检查`all_time`\n    if not isinstance(all_time, dict):\n        raise Exception(\'`all_time` should be a dict\')\n\n    if not isinstance(all_time.get(\'every_time\'), str):\n        raise Exception(\'`all_time[\"every_time\"]` should be a str\')\n\n    if not isinstance(all_time.get(\'group_time\'), str):\n        raise Exception(\'`all_time[\"group_time\"]` should be a str\')\n\n    # 正则表达式 匹配 *s, *m, *h, *d, *w, *M (秒，分，时，天，周，月)\n    match_every_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'every_time\'))\n    if not (match_every_time is not None and match_every_time.span(0)[1] == len(all_time.get(\'every_time\'))):\n        raise Exception(\'`all_time[\"every_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    match_group_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'group_time\'))\n    if not (match_group_time is not None and match_group_time.span(0)[1] == len(all_time.get(\'group_time\'))):\n        raise Exception(\'`all_time[\"group_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    # 5. 检查 `extra_alert_tags`\n    if extra_alert_tags is not None:\n        if not isinstance(extra_alert_tags, dict):\n            raise Exception(\'`extra_alert_tags` should be an dict\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.keys())):\n            raise Exception(\'Key of `extra_alert_tags` should be a str\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.values())):\n            raise Exception(\'Value of `extra_alert_tags` should be a str\')\n\n\n# 步骤2：获取数据\ndef _to_iso_time(d=None):\n    # 将unix时间戳转化成iso8601时间格式\n    return arrow.get(d).isoformat()\n\n\ndef _get_data(targets):\n\n    # 根据targets里的InfluxQL语句，获取数据，targets是个列表，目前仅支持单个target，所以取第一个\n    target = targets[0]\n    db_res = None\n    if MOCK_DATA:\n        db_res = MOCK_DATA[target[\'alias\']]\n    else:\n        bind_params = {}\n        # 转成真实触发时间\n        if \'$now\' in target[\'influxQL\']:\n            bind_params[\'now\'] = _to_iso_time(int(time.time()))\n\n        db_res = DFF.SRC(\'df_influxdb\').query(target[\'influxQL\'], bind_params=bind_params)\n\n    # 最终返回的valid_series_data是 由多个dict组成的list\n    valid_series_data = []\n    if \'series\' in db_res:\n        for s in db_res[\'series\']:\n            if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                continue\n\n            valid_series_data.append(s)\n\n    return valid_series_data\n\n\n# 步骤3：异常检测\n# 第二步返回的数据结果是 list of dict，第3步将遍历每一个dict（每个dict就是一条时间序列），进行单时间序列的异常检测\n\n\ndef _preprocessing(data):\n    \'\'\'\n    data为dict类型数据，处理成pd.Series\n    如果最后一个数据点为空，则剔除，其余为空则补0\n    \'\'\'\n    print(data)\n    df = pd.DataFrame(data[\'values\'])\n    df.index = df[0].apply(lambda x: x.replace(\'T\', \' \').replace(\'Z\', \'\'))\n    df = df[1]\n    df.index = pd.to_datetime(df.index)\n    df = df.astype(float)\n    if df.isnull().iloc[-1]:\n        df = df.iloc[:-1]\n    df.fillna(0, inplace=True)\n    print(df)\n    return df\n\n\ndef _spike_check(df, trigger_condition):\n    \"\"\"\n    突变检测，用于检测出突然上升或者下降的突变点（尖刺）\n\n    :param df:  pd.Series. 要检测的时间序列数据 index: datetime  value: float\n    :param trigger_condition: dict. 异常检测器的参数设置\n\n    :return: None or dict\n    \"\"\"\n\n    # validate_series 是 adtk包的函数，可以对时序pandas数据进行预处理\n    df = validate_series(df)\n    # 检测强度，强度越高，对于异常值的检测越苛刻\n    c_dict = {1: 1, 2: 2, 3: 3}\n    c = c_dict[trigger_condition[\'strength\']]\n    if trigger_condition[\'direction\'] == \'up\':\n        side = \'positive\'\n    elif trigger_condition[\'direction\'] == \'down\':\n        side = \'negative\'\n    else:\n        side = \'both\'\n    check_count = trigger_condition[\'checkCount\']\n    window = min(100, max(10, check_count * 5))\n    persist_ad = PersistAD(window=10, c=c, side=side)\n    anomalies = persist_ad.fit_detect(df)\n\n    # 只检测最近的 point_num 个点\n    point_num = int(trigger_condition[\'periodNum\'])\n    anomalies = anomalies.iloc[-point_num:]\n    anomalies = anomalies[anomalies == 1]\n    anomalies = df[anomalies.index]\n    # 只有anomalies个数 超过 checkCount时，才当做关键事件\n    # 刚好触达 checkCount的那个异常点所在的时刻 定义为 关键事件的时刻\n    if anomalies.shape[0] < check_count:\n        res = None\n    else:\n        dps = [[int(k.timestamp()), v] for k, v in anomalies.items()]\n        res = {\n            \'timestamp\': int(anomalies.index[trigger_condition[\'checkCount\'] - 1].timestamp()),\n            \'anomalies_dps\': dps,\n            \'anomaly\': anomalies.iloc[trigger_condition[\'checkCount\'] - 1]\n        }\n    return res\n\n\n# 步骤4：生成关键事件，和告警内容；存入关键事件，实施告警\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\n\ndef _get_event_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    return _get_md5(str_to_md5)\n\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, event_item_tags, event_id, duration, check_value_map=None,\n                     prev_level=None):\n    \"\"\"\n    创建变量字典\n    相关变量定义参考 模板变量文档\n    https://help.dataflux.cn/doc/1ee7e06f0f06162dd3ee3bd834611b6f0c3100d8\n\n    注：目前prev_level默认缺失，即默认为 OK\n    \"\"\"\n\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\': level,\n        \'status\': level,\n        \'prevLevel\': prev_level,\n        \'prevStatus\': prev_level,\n        \'levelName\': LEVEL_NAME_MAP[level],\n        \'statusName\': LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'prevStatusName\': prev_level_name,\n        \'tags\': _json_dumps(event_item_tags),\n        \'tagsEscaped\': None,\n        \'fields\': None,\n        \'fieldsEscaped\': None,\n        \'duration\': duration * 1000 if duration is not None else None,\n        \'durationHuman\': None,\n        \'ruleId\': trigger_uuid,\n        \'ruleName\': trigger_name,\n        \'alertId\': event_id,\n        \'eventId\': event_id\n    }\n\n    # 补全tags.KEY\n    for k, v in event_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全 fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全 fieldsEscaped、tagsEscaped\n    # 在json结构的string类型值里面再放tags, fields 的 json, 需要转义\n    # 如果人家填写的整个body是 {\"tags\": \"#{tags}\"} ，最终结果会变成\n    # {\"tags\": \"{\"a\": 1}\"}，这个是错误的json结构\n    # 因此提供了转义后的json字符串，保证上述例子输出为 {\"tags\": \"{\\\"a\\\": 1}\"}\n    # 如果人家body填的是 {\"tags\":  # {tags} } 就没事，最终结果为 {\"tags\": {\"a\": 1}}\n\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    if duration is not None:\n        _m, _s = divmod(duration, 60)\n        _h, _m = divmod(_m, 60)\n        _d, _h = divmod(_h, 24)\n        _human = \'{}秒\'.format(_s)\n        if _m > 0:\n            _human = \'{}分钟\'.format(_m) + _human\n        if _h > 0:\n            _human = \'{}小时\'.format(_h) + _human\n        if _d > 0:\n            _human = \'{}天\'.format(_d) + _human\n\n        var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to = action[\'to\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\': to,\n            \'title\': title,\n            \'content\': {\'text\': content},\n            \'sender\': \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook = action[\'webhook\']\n    title = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\': title,\n                \'text\': markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url = action[\'url\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\': var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\': profwang_level,\n            \'type\': \'alarm\',\n            \'title\': title,\n            \'content\': content,\n            \'suggestion\': suggestion,\n            \'origin\': \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': \'POST\',\n                \'url\': _url,\n                \'body\': _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    url = action[\'url\']\n    method = action[\'method\'].upper()\n    headers = action.get(\'headers\')\n    body = action.get(\'body\') or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': method,\n                \'url\': _url,\n                \'headers\': headers,\n                \'query\': None,\n                \'body\': body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'url\': url,\n            \'method\': method,\n            \'headers\': headers,\n            \'body\': body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name = action[\'scriptRefName\']\n    func_ref_name = action[\'funcRefName\']\n    kwargs = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\': script_ref_name,\n            \'funcRefName\': func_ref_name,\n            \'kwargs\': kwargs,\n        }),\n    }\n    return action_content, None\n\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\': _create_action_debug,\n    \'mail\': _create_action_mail,\n    \'dingTalkRobot\': _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\': _create_action_http_request,\n    \'DataFluxFunc\': _create_action_dataflux_func,\n}\n\n\ndef _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level, event_item_tags,\n                                  event_id, duration, check_value_map, extra_alert_tags=None, is_no_data=False):\n    trigger_uuid = trigger_info[\'uuid\']\n    trigger_name = trigger_info[\'name\']\n    # dimensions = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    var_dict = _create_var_dict(\n        trigger_uuid=trigger_uuid,\n        trigger_name=trigger_name,\n        level=level,\n        prev_level=prev_level,\n        event_item_tags=event_item_tags,\n        event_id=event_id,\n        duration=duration,\n        check_value_map=check_value_map)\n\n    for action in actions:\n        action_type = action[\'type\']\n\n        # 记录执行动作\n        _action = _json_copy(action)\n        action_content = None\n        action_task = None\n        try:\n            action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n        except Exception as e:\n            print(e)\n        else:\n            if action_task:\n                if isinstance(action_task, list):\n                    actions_to_perform.extend(action_task)\n                else:\n                    actions_to_perform.append(action_task)\n\n        # 记录写入__keyevent\n        print(action_content)\n        _title = action_content[\'title\']\n        _content = action_content.get(\'content\') or None\n        # _suggestion = action_content.get(\'suggestion\') or None\n        # _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n        # _keyevent_type = None\n        # if is_no_data:\n        #     _keyevent_type = \'noData\'\n\n        tags = {\n            \'__status\': level,\n            \'__eventId\': event_id,\n            \'__source\': \'datafluxTrigger\',\n            \'__ruleId\': trigger_uuid,\n            \'__ruleName\': trigger_name,\n            \'__actionType\': action_type\n        }\n        if extra_alert_tags:\n            tags.update(extra_alert_tags)\n        tags.update(event_item_tags)\n\n        __targets = [{\n            \'qtype\': targets[0][\'qtype\'],\n            \'query\': targets[0][\'query\']\n        }]\n\n        __outlierDps = [\n            {\'dps\': anomalies[\'anomalies_dps\']\n             }\n        ]\n\n        fields = {\n            \'__title\': _title,\n            \'__content\': _content,\n            \'__dimensions\': json.dumps(trigger_info[\'dimensions\'], ensure_ascii=False, separators=(\',\', \':\')),\n            \'__targets\': json.dumps(__targets, ensure_ascii=False, separators=(\',\', \':\')),\n            \'__outlierDps\': json.dumps(__outlierDps, ensure_ascii=False, separators=(\',\', \':\'))\n        }\n\n        key_event = {\n            \'measurement\': \'__keyevent\',\n            \'tags\': tags,\n            \'fields\': fields,\n            \'timestamp\': anomalies[\'timestamp\']\n        }\n\n        keyevents_to_write.append(key_event)\n\n    return keyevents_to_write, actions_to_perform\n\n\ndef _perform_actions(actions_to_perform):\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n\n\ndef _write_keyevents(keyevents_to_write, trigger_info):\n    # 批量写入关键事件\n    # 实例化dataway对象，用于写入数据（行协议）\n    dataway = DFF.SRC(\'df_dataway\', token=trigger_info[\'workspaceToken\'])\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    # 利用行协议方法写入dataway，接口返回200表示写入成功\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.post_line_protocol(points=keyevents_to_write, path=\'/v1/write/keyevent\')\n        print(\'DataWay返回: {}\'.format(_resp))\n    else:\n        print(\'没有key event需要写入\')\n\n\n@DFF.API(\'突变检测\', category=\'builtinCheck\', is_hidden=True)\ndef spike_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    \"\"\"\n    突变检测，检测突然上升或者下降的点\n    参数如下：\n        1. trigger_info {dict} 异常检测规则（以前叫触发器）的基本信息，结构如下：\n            {\n              \"uuid\"          : \"str, <UUID> 即规则ID\",\n              \"name\"          : \"str, <名称> 即规则名称\",\n              \"workspaceToken\": \"str, <工作空间令牌> 用于判断出事件之后写入__keyevent\",\n              \"dimensions\"    : \"<触发维度> list类型，元素为str\"\n            }\n\n        2. targets {list} 检测的目标，结构如下：\n            [\n              {\n                \"alias\"     : \"str, <别名> 默认即M1, M2 ...，用户可以改为其他内容，用于各类告警文案、数据填充\",\n                \"influxQL\"  : \"str, <InfluxDB 查询语句>\", 注：实际查询时需 替换占位符（$now) 为当前的时刻;\n                                注：influxQL里，where的时间窗里面，取 $now - （聚合周期个数+100）* 聚合周期 到 $now 的数据\",\n                \"qtype\"     : \"str, 如http，注：该字段内容算法用不上，只是当写入keyevennt时原封不动一并传入\",\n                \"query\"     : {\"注：该字段内容算法用不上，只是当写入keyevennt时一并传入，目的是使前端可以拉取keyevent同时也能根                                        据query获取原始的时序数据。该字段结构由后端和前端一起定义\"}\n              }\n            ]\n\n        3. condition_check_setting {dict} 异常检测的触发条件配置，结构如下，分为检测规则和告警动作：\n            {\n              \"rules\": [\n              {\n                \"triggerCondition\": {\n                    \"periodNum\": \"int, 检测几个聚合周期\",\n                    \"strength\": \"int, 检测的强度，目前页面上可选强，中，弱，传过来是数字3，2，1，强度越高代表异常的判定越苛刻\",\n                    \"direction\": \"str, 检测的方向, 页面上三个单选项对应的是 up, down, both\",\n                    \"checkCount\": \"int, 当异常点数>=该值时，触发事件\"\n                },\n                \"level\": \"str, 异常级别，目前默认 warning，写死\"\n            }],\n              \"actions\": [\n                {\n                  \"type\"   : \"mail\",\n                  \"to\"     : [\"邮件地址1\", \"邮件地址2\"],\n                  \"title\"  : \"<邮件标题>\",\n                  \"content\": \"<邮件内容>\"\n                },\n                {\n                  \"type\"    : \"dingTalkRobot\",\n                  \"webhook\" : [\"钉钉机器人调用地址1\", \"钉钉机器人调用地址2\"],\n                  \"title\"   : \"<通知标题>\",\n                  \"markdown\": \"<MarkDown格式通知内容>\"\n                },\n                {\n                  \"type\"   : \"ProfWangAlertHub\",\n                  \"url\"    : [\"王教授AlertHub调用地址1\", \"王教授AlertHub调用地址2\"],\n                  \"title\"  : \"<情报标题>\",\n                  \"content\": \"<情报内容>\"\n                },\n                {\n                  \"type\"    : \"HTTPRequest\",\n                  \"url\"     : [\"请求地址1\", \"请求地址2\"],\n                  \"method\"  : \"<请求方式> GET|POST\",\n                  \"headers\" : {\"Header1\": \"HeaderValue1\", \"Header2\": \"HeaderValue2\"},\n                  \"body\"    : \"<method=POST 请求体>\",\n                  \"bodyType\": \"<method=POST 请求体类型> text|json|form\"\n                },\n                {\n                  \"type\"            : \"DataFluxFunc\",\n                  \"scriptSetRefName\": \"<脚本集引用名>\",\n                  \"scriptRefName\"   : \"<脚本引用名>\",\n                  \"funcRefName\"     : \"<函数名>\",\n                  \"kwargs\"          : { \"<调用参数名>\": \"<调用参数值>\" }\n                }\n              ]\n            }\n\n        4. all_time:  \"dict, 检测频率，数据聚合的周期\"\n            {\n                \"every_time\": \"str, 每隔多久检测一次\",\n                \"group_time\": \"str, 时间聚合周期\",\n            }\n\n        5. extra_alert_tags: {dict} 额外添加的告警数据标签\n            {\n                \"extra_alert_tags\": \"str, 异常\"\n            }\n    \"\"\"\n    _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags)\n\n    db_res = _get_data(targets)\n\n    trigger_condition = condition_check_setting[\'rules\'][0][\'triggerCondition\']\n\n    for data in db_res:\n        print(data[\'tags\'])\n        df = _preprocessing(data)\n        anomalies = _spike_check(df, trigger_condition)\n        if anomalies is not None:\n            print(\'异常时刻: \', anomalies[\'timestamp\'])\n            print(\'异常时间: \', datetime.datetime.fromtimestamp(anomalies[\'timestamp\'] + 28800))\n\n            actions = condition_check_setting[\'actions\']\n\n            level = condition_check_setting[\'rules\'][0][\'level\']\n            prev_level = None\n\n            event_item_tags = data[\'tags\']\n\n            # 得到event_id，异常检测器ID + 这条时序数据的tags 得到唯一的event_id\n            # 字典对象序列化，变成字符串\n            event_item_key = json.dumps(event_item_tags, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n            event_id = _get_event_id(trigger_info[\'uuid\'], event_item_key)\n\n            duration = None\n\n            field_alias = data[\'columns\'][1]\n            check_value_map = {field_alias: anomalies[\'anomaly\']}\n\n            keyevents_to_write, actions_to_perform = \\\n                _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level,\n                                              event_item_tags, event_id, duration, check_value_map,\n                                              extra_alert_tags=extra_alert_tags)\n            print(keyevents_to_write)\n            print(\'-------------------------------------------------\')\n            print(actions_to_perform)\n            _write_keyevents(keyevents_to_write, trigger_info)\n            _perform_actions(actions_to_perform)\n        else:\n            print(\'没有异常\')\n\n    return 1\n\n\ndef test():\n\n    # 模拟一个完整参数\n    kwargs = {\n        \"targets\": [\n          {\n            \"alias\": \"M1\",\n            \"qtype\": \"http\",\n            \"query\": {\n              \"fields\": [\n                {\n                  \"fx\": \"$$value+10\",\n                  \"args\": [\n                    {\n                      \"name\": \"衣服类\"\n                    }\n                  ],\n                  \"name\": \"衣服类\",\n                  \"alias\": \"M1\",\n                  \"content\": \"\",\n                  \"funcName\": \"max\",\n                  \"fieldType\": \"float\"\n                }\n              ],\n              \"filter\": {\n                \"tags\": [\n                  {\n                    \"name\": \"衣服\",\n                    \"value\": [\n                      \"T桖\"\n                    ],\n                    \"condition\": \"\",\n                    \"operation\": \"=\"\n                  }\n                ]\n              },\n              \"period\": \"1m\",\n              \"groupBy\": [\n                \"衣服\",\n                \"time(1m)\"\n              ],\n              \"measurements\": [\n                \"突变检测\"\n              ]\n            },\n            \"influxQL\": \"select (max(\\\"衣服类\\\")+10) as \\\"M1\\\" from \\\"biz_7ee6760f78ae4f2db672dfcd72be1aa8\\\".\\\"rp5\\\".\\\"突变检测\\\" where (\\\"time\\\" >=  $now - 103m and \\\"time\\\" <  $now ) and (\\\"衣服\\\" = \'T桖\') group by \\\"衣服\\\", time(1m)\"\n          }\n        ],\n        \"all_time\": {\n          \"every_time\": \"1m\",\n          \"group_time\": \"1m\"\n        },\n        \"trigger_info\": {\n          \"name\": \"突变检测\",\n          \"uuid\": \"rul_d10e297ab60711eaa7c27617d43a21ba\",\n          \"dimensions\": [\n            \"衣服\"\n          ],\n          \"workspaceToken\": \"tkn_69c090f66e14462787327653ba251e60\"\n        },\n        \"extra_alert_tags\": {\n          \"name\": \"突变检测\"\n        },\n        \"condition_check_setting\": {\n          \"rules\": [\n            {\n              \"level\": \"warning\",\n              \"triggerCondition\": {\n                \"strength\": 3,\n                \"direction\": \"down\",\n                \"periodNum\": 60,\n                \"checkCount\": 2\n              }\n            }\n          ],\n          \"actions\": [\n            {\n              \"to\": [\n                \"chenyiyun@jiagouyun.com\"\n              ],\n              \"type\": \"mail\",\n              \"title\": \"突变检测\",\n              \"content\": \"您的突变检测存在异常，请及时查看！\"\n            }\n          ]\n        }\n      }\n\n    spike_check(**kwargs)\n', '4481c55f6101c1f314815c497f6bb2ee', '\"\"\"\n突变检测\n\n注：目前调度是整分钟的01秒调用，根据InfluxQL里的聚合，如果按分钟聚合，最后一个数据点是该分钟聚合这一秒的数据（往往是缺失）\n目前的处理：最后一个数据点为空，则剔除，其他数据点的空值填补成0\n\n2020-06-18  解析参数，验证参数，获取数据，处理数据&异常检测，如果触发事件则利用dataway行协议写入__keyevent\n\n2020-06-28 新增支持 告警功能 & 模板变量 功能\n\"\"\"\n\n\nimport hashlib\nimport re\nimport time\nimport datetime\nimport arrow\nimport simplejson as json\nimport six\nimport pandas as pd\nfrom adtk.data import validate_series\nfrom adtk.detector import PersistAD\n\n_DFF_is_debug = True\nLEVEL_NAME_MAP = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nOK_LEVEL = \'ok\'\nCONDITION_LOGICS = (\'and\', \'or\')\nCONDITION_OPERATORS = \\\n    (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nALL_LEVELS = (\'critical\', \'error\', \'warning\', \'info\')\nACTION_TYPES = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS = (\'GET\', \'POST\')\nHTTP_BODY_TYPES = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\nMOCK_DATA = None\nRAISE_DATAWAY_EXCEPTION = True\n\n# 基础函数\n\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n    return six.ensure_str(s)\n\n\n# 以下函数为 模板变量功能 & 进阶模板功能 所需要的功能函数\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from = from_unit.lower()\n    _search_to = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n                                  carry=1000)\n\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n                                  carry=1024)\n\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n                                  carry=1024)\n\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n                                  all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n                                  carry=1024)\n\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\': __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\': __filter_volumn,\n    \'volumni\': __filter_volumn_i,\n    \'bitrate\': __filter_bit_rate,\n    \'byterate\': __filter_byte_rate,\n    \'upper\': __filter_upper,\n    \'lower\': __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\': __filter_if_true,\n    \'iffalse\': __filter_if_false,\n}\n\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, six.string_types):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result = DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\n\n# 步骤1：参数校验\ndef __param_targets_check(targets):\n    if not isinstance(targets, list):\n        raise Exception(\'`targets` should be a list\')\n\n    for i, t in enumerate(targets):\n        if not isinstance(t, dict):\n            raise Exception(\'`targets[{}]` should be a dict\'.format(i))\n\n        if not isinstance(t.get(\'alias\'), str):\n            raise Exception(\'`targets[{}][\"alias\"]` should be a str\'.format(i))\n\n        if \'influxQL\' in t and not isinstance(t.get(\'influxQL\'), str):\n            raise Exception(\'`targets[{}][\"influxQL\"]` should be a str\'.format(i))\n\n        if \'expression\' in t and not isinstance(t.get(\'expression\'), str):\n            raise Exception(\'`targets[{}][\"expression\"]` should be a str\'.format(i))\n\n\ndef __param_rules_check(rules):\n    if not isinstance(rules, list):\n        raise Exception(\'`rules` should be a list\')\n\n    for i, r in enumerate(rules):\n        if not isinstance(r, dict):\n            raise Exception(\'`rules[{}]` should be a dict\'.format(i))\n\n        if r.get(\'level\') not in ALL_LEVELS:\n            raise Exception(\'`rules[{}][\"level\"]` should be one of {}\'.format(i, \',\'.join(ALL_LEVELS)))\n\n        if not isinstance(r.get(\'triggerCondition\'), dict):\n            raise Exception(\'`rules[{}][\"triggerCondition\"]` should be a dict\'.format(i))\n\n        c = r[\'triggerCondition\']\n        if not isinstance(c.get(\'periodNum\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"periodNum\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'strength\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"strength\"]` should be an int\'.format(i))\n\n        if not isinstance(c.get(\'direction\'), str):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be a str\'.format(i))\n\n        d = c.get(\'direction\')\n        if d not in [\'up\', \'down\', \'both\']:\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"direction\"]` should be `up` or `down` or `both`\'.format(i))\n\n        if not isinstance(c.get(\'checkCount\'), int):\n            raise Exception(\'`rules[{}][\"triggerCondition\"][\"checkCount\"]` should be an int\'.format(i))\n\n\ndef __param_action_check(actions, parent_path):\n    if not isinstance(actions, list):\n        raise Exception(\'`{}actions` should be a list\'.format(parent_path))\n\n    for i, a in enumerate(actions):\n        if not isinstance(a, dict):\n            raise Exception(\'`{}actions[{}]` should be a dict\'.format(parent_path, i))\n\n        if a.get(\'type\') not in ACTION_TYPES:\n            raise Exception(\n                \'`{}actions[{}][\"type\"]` should be one of {}\'.format(parent_path, i, \',\'.join(ACTION_TYPES)))\n\n        if a[\'type\'] == \'mail\':\n            if not isinstance(a.get(\'to\'), list):\n                raise Exception(\'`{}actions[{}][\"to\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'to\'])):\n                raise Exception(\'Element of `{}actions[{}][\"to\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'dingTalkRobot\':\n            if not isinstance(a.get(\'webhook\'), list):\n                raise Exception(\'`{}actions[{}][\"webhook\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'webhook\'])):\n                raise Exception(\'Element of `{}actions[{}][\"webhook\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'markdown\'), str):\n                raise Exception(\'`{}actions[{}][\"markdown\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'ProfWangAlertHub\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'suggestion\') is not None:\n                if not isinstance(a[\'suggestion\'], str):\n                    raise Exception(\'`{}actions[{}][\"suggestion\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'HTTPRequest\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'method\') not in HTTP_METHODS:\n                raise Exception(\n                    \'`{}actions[{}][\"method\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_METHODS)))\n\n            if a.get(\'headers\') is not None:\n                if not isinstance(a[\'headers\'], dict):\n                    raise Exception(\'`{}actions[{}][\"headers\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].values())):\n                    raise Exception(\'Value of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n            if a[\'method\'] == \'POST\':\n                if a.get(\'bodyType\') not in HTTP_BODY_TYPES:\n                    raise Exception(\'`{}actions[{}][\"bodyType\"]` should be one of {}\'.format(parent_path, i,\n                                                                                             \',\'.join(HTTP_BODY_TYPES)))\n\n                if not isinstance(a.get(\'body\'), str):\n                    raise Exception(\'`{}actions[{}][\"body\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'DataFluxFunc\':\n            if not isinstance(a.get(\'scriptSetRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptSetRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'scriptRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'funcRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"funcRefName\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'kwargs\') is not None:\n                if not isinstance(a[\'kwargs\'], dict):\n                    raise Exception(\'`{}actions[{}][\"kwargs\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'kwargs\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"kwargs\"]` should be a str\'.format(parent_path, i))\n\n\ndef _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    # 1. 检查`trigger_info`\n    if not isinstance(trigger_info, dict):\n        raise Exception(\'`trigger_info` should be a dict\')\n\n    if not isinstance(trigger_info.get(\'uuid\'), str):\n        raise Exception(\'`trigger_info[\"uuid\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'name\'), str):\n        raise Exception(\'`trigger_info[\"name\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'workspaceToken\'), str):\n        raise Exception(\'`trigger_info[\"workspaceToken\"]` should be a str\')\n\n    dimensions = trigger_info.get(\'dimensions\')\n    if dimensions is not None:\n        if not isinstance(dimensions, list):\n            raise Exception(\'`trigger_info[\"dimensions\"]` should be a list\')\n\n        for index, dimension in enumerate(dimensions):\n            if not isinstance(dimension, str):\n                raise Exception(\'`trigger_info[\"dimensions\"][{}]` should be a str\'.format(index))\n\n    # 2. 检查`targets`\n    __param_targets_check(targets)\n\n    # 3. 检查`condition_check_setting`\n    if not isinstance(condition_check_setting, dict):\n        raise Exception(\'`condition_check_setting` should be a dict\')\n\n    __param_rules_check(condition_check_setting.get(\'rules\'))\n    __param_action_check(condition_check_setting.get(\'actions\'), parent_path=\'condition_check_setting.\')\n\n    # 4. 检查`all_time`\n    if not isinstance(all_time, dict):\n        raise Exception(\'`all_time` should be a dict\')\n\n    if not isinstance(all_time.get(\'every_time\'), str):\n        raise Exception(\'`all_time[\"every_time\"]` should be a str\')\n\n    if not isinstance(all_time.get(\'group_time\'), str):\n        raise Exception(\'`all_time[\"group_time\"]` should be a str\')\n\n    # 正则表达式 匹配 *s, *m, *h, *d, *w, *M (秒，分，时，天，周，月)\n    match_every_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'every_time\'))\n    if not (match_every_time is not None and match_every_time.span(0)[1] == len(all_time.get(\'every_time\'))):\n        raise Exception(\'`all_time[\"every_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    match_group_time = re.match(\'\\d+[smhdwM]\', all_time.get(\'group_time\'))\n    if not (match_group_time is not None and match_group_time.span(0)[1] == len(all_time.get(\'group_time\'))):\n        raise Exception(\'`all_time[\"group_time\"]` should be `*s` or `*m` or `*h` or `*d` or `*w` or `*M`\')\n\n    # 5. 检查 `extra_alert_tags`\n    if extra_alert_tags is not None:\n        if not isinstance(extra_alert_tags, dict):\n            raise Exception(\'`extra_alert_tags` should be an dict\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.keys())):\n            raise Exception(\'Key of `extra_alert_tags` should be a str\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.values())):\n            raise Exception(\'Value of `extra_alert_tags` should be a str\')\n\n\n# 步骤2：获取数据\ndef _to_iso_time(d=None):\n    # 将unix时间戳转化成iso8601时间格式\n    return arrow.get(d).isoformat()\n\n\ndef _get_data(targets):\n\n    # 根据targets里的InfluxQL语句，获取数据，targets是个列表，目前仅支持单个target，所以取第一个\n    target = targets[0]\n    db_res = None\n    if MOCK_DATA:\n        db_res = MOCK_DATA[target[\'alias\']]\n    else:\n        bind_params = {}\n        # 转成真实触发时间\n        if \'$now\' in target[\'influxQL\']:\n            bind_params[\'now\'] = _to_iso_time(int(time.time()))\n\n        db_res = DFF.SRC(\'df_influxdb\').query(target[\'influxQL\'], bind_params=bind_params)\n\n    # 最终返回的valid_series_data是 由多个dict组成的list\n    valid_series_data = []\n    if \'series\' in db_res:\n        for s in db_res[\'series\']:\n            if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                continue\n\n            valid_series_data.append(s)\n\n    return valid_series_data\n\n\n# 步骤3：异常检测\n# 第二步返回的数据结果是 list of dict，第3步将遍历每一个dict（每个dict就是一条时间序列），进行单时间序列的异常检测\n\n\ndef _preprocessing(data):\n    \'\'\'\n    data为dict类型数据，处理成pd.Series\n    如果最后一个数据点为空，则剔除，其余为空则补0\n    \'\'\'\n    print(data)\n    df = pd.DataFrame(data[\'values\'])\n    df.index = df[0].apply(lambda x: x.replace(\'T\', \' \').replace(\'Z\', \'\'))\n    df = df[1]\n    df.index = pd.to_datetime(df.index)\n    df = df.astype(float)\n    if df.isnull().iloc[-1]:\n        df = df.iloc[:-1]\n    df.fillna(0, inplace=True)\n    print(df)\n    return df\n\n\ndef _spike_check(df, trigger_condition):\n    \"\"\"\n    突变检测，用于检测出突然上升或者下降的突变点（尖刺）\n\n    :param df:  pd.Series. 要检测的时间序列数据 index: datetime  value: float\n    :param trigger_condition: dict. 异常检测器的参数设置\n\n    :return: None or dict\n    \"\"\"\n\n    # validate_series 是 adtk包的函数，可以对时序pandas数据进行预处理\n    df = validate_series(df)\n    # 检测强度，强度越高，对于异常值的检测越苛刻\n    c_dict = {1: 1, 2: 2, 3: 3}\n    c = c_dict[trigger_condition[\'strength\']]\n    if trigger_condition[\'direction\'] == \'up\':\n        side = \'positive\'\n    elif trigger_condition[\'direction\'] == \'down\':\n        side = \'negative\'\n    else:\n        side = \'both\'\n    check_count = trigger_condition[\'checkCount\']\n    window = min(100, max(10, check_count * 5))\n    persist_ad = PersistAD(window=10, c=c, side=side)\n    anomalies = persist_ad.fit_detect(df)\n\n    # 只检测最近的 point_num 个点\n    point_num = int(trigger_condition[\'periodNum\'])\n    anomalies = anomalies.iloc[-point_num:]\n    anomalies = anomalies[anomalies == 1]\n    anomalies = df[anomalies.index]\n    # 只有anomalies个数 超过 checkCount时，才当做关键事件\n    # 刚好触达 checkCount的那个异常点所在的时刻 定义为 关键事件的时刻\n    if anomalies.shape[0] < check_count:\n        res = None\n    else:\n        dps = [[int(k.timestamp()), v] for k, v in anomalies.items()]\n        res = {\n            \'timestamp\': int(anomalies.index[trigger_condition[\'checkCount\'] - 1].timestamp()),\n            \'anomalies_dps\': dps,\n            \'anomaly\': anomalies.iloc[trigger_condition[\'checkCount\'] - 1]\n        }\n    return res\n\n\n# 步骤4：生成关键事件，和告警内容；存入关键事件，实施告警\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\n\ndef _get_event_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    return _get_md5(str_to_md5)\n\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, event_item_tags, event_id, duration, check_value_map=None,\n                     prev_level=None):\n    \"\"\"\n    创建变量字典\n    相关变量定义参考 模板变量文档\n    https://help.dataflux.cn/doc/1ee7e06f0f06162dd3ee3bd834611b6f0c3100d8\n\n    注：目前prev_level默认缺失，即默认为 OK\n    \"\"\"\n\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\': level,\n        \'status\': level,\n        \'prevLevel\': prev_level,\n        \'prevStatus\': prev_level,\n        \'levelName\': LEVEL_NAME_MAP[level],\n        \'statusName\': LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'prevStatusName\': prev_level_name,\n        \'tags\': _json_dumps(event_item_tags),\n        \'tagsEscaped\': None,\n        \'fields\': None,\n        \'fieldsEscaped\': None,\n        \'duration\': duration * 1000 if duration is not None else None,\n        \'durationHuman\': None,\n        \'ruleId\': trigger_uuid,\n        \'ruleName\': trigger_name,\n        \'alertId\': event_id,\n        \'eventId\': event_id\n    }\n\n    # 补全tags.KEY\n    for k, v in event_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全 fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全 fieldsEscaped、tagsEscaped\n    # 在json结构的string类型值里面再放tags, fields 的 json, 需要转义\n    # 如果人家填写的整个body是 {\"tags\": \"#{tags}\"} ，最终结果会变成\n    # {\"tags\": \"{\"a\": 1}\"}，这个是错误的json结构\n    # 因此提供了转义后的json字符串，保证上述例子输出为 {\"tags\": \"{\\\"a\\\": 1}\"}\n    # 如果人家body填的是 {\"tags\":  # {tags} } 就没事，最终结果为 {\"tags\": {\"a\": 1}}\n\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    if duration is not None:\n        _m, _s = divmod(duration, 60)\n        _h, _m = divmod(_m, 60)\n        _d, _h = divmod(_h, 24)\n        _human = \'{}秒\'.format(_s)\n        if _m > 0:\n            _human = \'{}分钟\'.format(_m) + _human\n        if _h > 0:\n            _human = \'{}小时\'.format(_h) + _human\n        if _d > 0:\n            _human = \'{}天\'.format(_d) + _human\n\n        var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to = action[\'to\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\': to,\n            \'title\': title,\n            \'content\': {\'text\': content},\n            \'sender\': \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook = action[\'webhook\']\n    title = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\': title,\n                \'text\': markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url = action[\'url\']\n    title = action[\'title\']\n    content = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\': var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\': profwang_level,\n            \'type\': \'alarm\',\n            \'title\': title,\n            \'content\': content,\n            \'suggestion\': suggestion,\n            \'origin\': \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': \'POST\',\n                \'url\': _url,\n                \'body\': _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    url = action[\'url\']\n    method = action[\'method\'].upper()\n    headers = action.get(\'headers\')\n    body = action.get(\'body\') or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\': method,\n                \'url\': _url,\n                \'headers\': headers,\n                \'query\': None,\n                \'body\': body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'url\': url,\n            \'method\': method,\n            \'headers\': headers,\n            \'body\': body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name = action[\'scriptRefName\']\n    func_ref_name = action[\'funcRefName\']\n    kwargs = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\': title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\': script_ref_name,\n            \'funcRefName\': func_ref_name,\n            \'kwargs\': kwargs,\n        }),\n    }\n    return action_content, None\n\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\': _create_action_debug,\n    \'mail\': _create_action_mail,\n    \'dingTalkRobot\': _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\': _create_action_http_request,\n    \'DataFluxFunc\': _create_action_dataflux_func,\n}\n\n\ndef _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level, event_item_tags,\n                                  event_id, duration, check_value_map, extra_alert_tags=None, is_no_data=False):\n    trigger_uuid = trigger_info[\'uuid\']\n    trigger_name = trigger_info[\'name\']\n    # dimensions = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    var_dict = _create_var_dict(\n        trigger_uuid=trigger_uuid,\n        trigger_name=trigger_name,\n        level=level,\n        prev_level=prev_level,\n        event_item_tags=event_item_tags,\n        event_id=event_id,\n        duration=duration,\n        check_value_map=check_value_map)\n\n    for action in actions:\n        action_type = action[\'type\']\n\n        # 记录执行动作\n        _action = _json_copy(action)\n        action_content = None\n        action_task = None\n        try:\n            action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n        except Exception as e:\n            print(e)\n        else:\n            if action_task:\n                if isinstance(action_task, list):\n                    actions_to_perform.extend(action_task)\n                else:\n                    actions_to_perform.append(action_task)\n\n        # 记录写入__keyevent\n        print(action_content)\n        _title = action_content[\'title\']\n        _content = action_content.get(\'content\') or None\n        # _suggestion = action_content.get(\'suggestion\') or None\n        # _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n        # _keyevent_type = None\n        # if is_no_data:\n        #     _keyevent_type = \'noData\'\n\n        tags = {\n            \'__status\': level,\n            \'__eventId\': event_id,\n            \'__source\': \'datafluxTrigger\',\n            \'__ruleId\': trigger_uuid,\n            \'__ruleName\': trigger_name,\n            \'__actionType\': action_type\n        }\n        if extra_alert_tags:\n            tags.update(extra_alert_tags)\n        tags.update(event_item_tags)\n\n        __targets = [{\n            \'qtype\': targets[0][\'qtype\'],\n            \'query\': targets[0][\'query\']\n        }]\n\n        __outlierDps = [\n            {\'dps\': anomalies[\'anomalies_dps\']\n             }\n        ]\n\n        fields = {\n            \'__title\': _title,\n            \'__content\': _content,\n            \'__dimensions\': json.dumps(trigger_info[\'dimensions\'], ensure_ascii=False, separators=(\',\', \':\')),\n            \'__targets\': json.dumps(__targets, ensure_ascii=False, separators=(\',\', \':\')),\n            \'__outlierDps\': json.dumps(__outlierDps, ensure_ascii=False, separators=(\',\', \':\'))\n        }\n\n        key_event = {\n            \'measurement\': \'__keyevent\',\n            \'tags\': tags,\n            \'fields\': fields,\n            \'timestamp\': anomalies[\'timestamp\']\n        }\n\n        keyevents_to_write.append(key_event)\n\n    return keyevents_to_write, actions_to_perform\n\n\ndef _perform_actions(actions_to_perform):\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n\n\ndef _write_keyevents(keyevents_to_write, trigger_info):\n    # 批量写入关键事件\n    # 实例化dataway对象，用于写入数据（行协议）\n    dataway = DFF.SRC(\'df_dataway\', token=trigger_info[\'workspaceToken\'])\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    # 利用行协议方法写入dataway，接口返回200表示写入成功\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.post_line_protocol(points=keyevents_to_write, path=\'/v1/write/keyevent\')\n        print(\'DataWay返回: {}\'.format(_resp))\n    else:\n        print(\'没有key event需要写入\')\n\n\n@DFF.API(\'突变检测\', category=\'builtinCheck\', is_hidden=True)\ndef spike_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags=None):\n    \"\"\"\n    突变检测，检测突然上升或者下降的点\n    参数如下：\n        1. trigger_info {dict} 异常检测规则（以前叫触发器）的基本信息，结构如下：\n            {\n              \"uuid\"          : \"str, <UUID> 即规则ID\",\n              \"name\"          : \"str, <名称> 即规则名称\",\n              \"workspaceToken\": \"str, <工作空间令牌> 用于判断出事件之后写入__keyevent\",\n              \"dimensions\"    : \"<触发维度> list类型，元素为str\"\n            }\n\n        2. targets {list} 检测的目标，结构如下：\n            [\n              {\n                \"alias\"     : \"str, <别名> 默认即M1, M2 ...，用户可以改为其他内容，用于各类告警文案、数据填充\",\n                \"influxQL\"  : \"str, <InfluxDB 查询语句>\", 注：实际查询时需 替换占位符（$now) 为当前的时刻;\n                                注：influxQL里，where的时间窗里面，取 $now - （聚合周期个数+100）* 聚合周期 到 $now 的数据\",\n                \"qtype\"     : \"str, 如http，注：该字段内容算法用不上，只是当写入keyevennt时原封不动一并传入\",\n                \"query\"     : {\"注：该字段内容算法用不上，只是当写入keyevennt时一并传入，目的是使前端可以拉取keyevent同时也能根                                        据query获取原始的时序数据。该字段结构由后端和前端一起定义\"}\n              }\n            ]\n\n        3. condition_check_setting {dict} 异常检测的触发条件配置，结构如下，分为检测规则和告警动作：\n            {\n              \"rules\": [\n              {\n                \"triggerCondition\": {\n                    \"periodNum\": \"int, 检测几个聚合周期\",\n                    \"strength\": \"int, 检测的强度，目前页面上可选强，中，弱，传过来是数字3，2，1，强度越高代表异常的判定越苛刻\",\n                    \"direction\": \"str, 检测的方向, 页面上三个单选项对应的是 up, down, both\",\n                    \"checkCount\": \"int, 当异常点数>=该值时，触发事件\"\n                },\n                \"level\": \"str, 异常级别，目前默认 warning，写死\"\n            }],\n              \"actions\": [\n                {\n                  \"type\"   : \"mail\",\n                  \"to\"     : [\"邮件地址1\", \"邮件地址2\"],\n                  \"title\"  : \"<邮件标题>\",\n                  \"content\": \"<邮件内容>\"\n                },\n                {\n                  \"type\"    : \"dingTalkRobot\",\n                  \"webhook\" : [\"钉钉机器人调用地址1\", \"钉钉机器人调用地址2\"],\n                  \"title\"   : \"<通知标题>\",\n                  \"markdown\": \"<MarkDown格式通知内容>\"\n                },\n                {\n                  \"type\"   : \"ProfWangAlertHub\",\n                  \"url\"    : [\"王教授AlertHub调用地址1\", \"王教授AlertHub调用地址2\"],\n                  \"title\"  : \"<情报标题>\",\n                  \"content\": \"<情报内容>\"\n                },\n                {\n                  \"type\"    : \"HTTPRequest\",\n                  \"url\"     : [\"请求地址1\", \"请求地址2\"],\n                  \"method\"  : \"<请求方式> GET|POST\",\n                  \"headers\" : {\"Header1\": \"HeaderValue1\", \"Header2\": \"HeaderValue2\"},\n                  \"body\"    : \"<method=POST 请求体>\",\n                  \"bodyType\": \"<method=POST 请求体类型> text|json|form\"\n                },\n                {\n                  \"type\"            : \"DataFluxFunc\",\n                  \"scriptSetRefName\": \"<脚本集引用名>\",\n                  \"scriptRefName\"   : \"<脚本引用名>\",\n                  \"funcRefName\"     : \"<函数名>\",\n                  \"kwargs\"          : { \"<调用参数名>\": \"<调用参数值>\" }\n                }\n              ]\n            }\n\n        4. all_time:  \"dict, 检测频率，数据聚合的周期\"\n            {\n                \"every_time\": \"str, 每隔多久检测一次\",\n                \"group_time\": \"str, 时间聚合周期\",\n            }\n\n        5. extra_alert_tags: {dict} 额外添加的告警数据标签\n            {\n                \"extra_alert_tags\": \"str, 异常\"\n            }\n    \"\"\"\n    _param_check(trigger_info, targets, condition_check_setting, all_time, extra_alert_tags)\n\n    db_res = _get_data(targets)\n\n    trigger_condition = condition_check_setting[\'rules\'][0][\'triggerCondition\']\n\n    for data in db_res:\n        print(data[\'tags\'])\n        df = _preprocessing(data)\n        anomalies = _spike_check(df, trigger_condition)\n        if anomalies is not None:\n            print(\'异常时刻: \', anomalies[\'timestamp\'])\n            print(\'异常时间: \', datetime.datetime.fromtimestamp(anomalies[\'timestamp\'] + 28800))\n\n            actions = condition_check_setting[\'actions\']\n\n            level = condition_check_setting[\'rules\'][0][\'level\']\n            prev_level = None\n\n            event_item_tags = data[\'tags\']\n\n            # 得到event_id，异常检测器ID + 这条时序数据的tags 得到唯一的event_id\n            # 字典对象序列化，变成字符串\n            event_item_key = json.dumps(event_item_tags, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n            event_id = _get_event_id(trigger_info[\'uuid\'], event_item_key)\n\n            duration = None\n\n            field_alias = data[\'columns\'][1]\n            check_value_map = {field_alias: anomalies[\'anomaly\']}\n\n            keyevents_to_write, actions_to_perform = \\\n                _create_keyevents_and_actions(trigger_info, targets, actions, anomalies, level, prev_level,\n                                              event_item_tags, event_id, duration, check_value_map,\n                                              extra_alert_tags=extra_alert_tags)\n            print(keyevents_to_write)\n            print(\'-------------------------------------------------\')\n            print(actions_to_perform)\n            _write_keyevents(keyevents_to_write, trigger_info)\n            _perform_actions(actions_to_perform)\n        else:\n            print(\'没有异常\')\n\n    return 1\n\n\ndef test():\n\n    # 模拟一个完整参数\n    kwargs = {\n        \"targets\": [\n          {\n            \"alias\": \"M1\",\n            \"qtype\": \"http\",\n            \"query\": {\n              \"fields\": [\n                {\n                  \"fx\": \"$$value+10\",\n                  \"args\": [\n                    {\n                      \"name\": \"衣服类\"\n                    }\n                  ],\n                  \"name\": \"衣服类\",\n                  \"alias\": \"M1\",\n                  \"content\": \"\",\n                  \"funcName\": \"max\",\n                  \"fieldType\": \"float\"\n                }\n              ],\n              \"filter\": {\n                \"tags\": [\n                  {\n                    \"name\": \"衣服\",\n                    \"value\": [\n                      \"T桖\"\n                    ],\n                    \"condition\": \"\",\n                    \"operation\": \"=\"\n                  }\n                ]\n              },\n              \"period\": \"1m\",\n              \"groupBy\": [\n                \"衣服\",\n                \"time(1m)\"\n              ],\n              \"measurements\": [\n                \"突变检测\"\n              ]\n            },\n            \"influxQL\": \"select (max(\\\"衣服类\\\")+10) as \\\"M1\\\" from \\\"biz_7ee6760f78ae4f2db672dfcd72be1aa8\\\".\\\"rp5\\\".\\\"突变检测\\\" where (\\\"time\\\" >=  $now - 103m and \\\"time\\\" <  $now ) and (\\\"衣服\\\" = \'T桖\') group by \\\"衣服\\\", time(1m)\"\n          }\n        ],\n        \"all_time\": {\n          \"every_time\": \"1m\",\n          \"group_time\": \"1m\"\n        },\n        \"trigger_info\": {\n          \"name\": \"突变检测\",\n          \"uuid\": \"rul_d10e297ab60711eaa7c27617d43a21ba\",\n          \"dimensions\": [\n            \"衣服\"\n          ],\n          \"workspaceToken\": \"tkn_69c090f66e14462787327653ba251e60\"\n        },\n        \"extra_alert_tags\": {\n          \"name\": \"突变检测\"\n        },\n        \"condition_check_setting\": {\n          \"rules\": [\n            {\n              \"level\": \"warning\",\n              \"triggerCondition\": {\n                \"strength\": 3,\n                \"direction\": \"down\",\n                \"periodNum\": 60,\n                \"checkCount\": 2\n              }\n            }\n          ],\n          \"actions\": [\n            {\n              \"to\": [\n                \"chenyiyun@jiagouyun.com\"\n              ],\n              \"type\": \"mail\",\n              \"title\": \"突变检测\",\n              \"content\": \"您的突变检测存在异常，请及时查看！\"\n            }\n          ]\n        }\n      }\n\n    spike_check(**kwargs)\n', '4481c55f6101c1f314815c497f6bb2ee', '2020-06-29 09:37:40', '2020-06-29 09:41:07');

INSERT INTO `biz_main_script_set` (`seq`, `id`, `title`, `description`, `refName`, `type`, `createTime`, `updateTime`) VALUES
('1', 'sset-demo', '示例脚本集', '主要包含了一些用于展示DataFlux.f(x) 功能的脚本，可以删除', 'demo', 'user', '2019-12-09 15:15:05', '2020-02-26 00:59:55'),
('2', 'sset-ft_lib', '场景支持', '场景支持脚本集', 'ft_lib', 'official', '2020-06-29 09:37:40', '2020-06-29 09:41:07');

INSERT INTO `wat_main_organization` (`seq`, `id`, `uniqueId`, `name`, `markers`, `isDisabled`, `createTime`, `updateTime`) VALUES
('1', 'o-sys', 'system', 'System Organization', NULL, '0', '2017-07-28 18:08:03', '2018-05-24 00:47:06');

INSERT INTO `wat_main_user` (`seq`, `id`, `organizationId`, `username`, `passwordHash`, `name`, `mobile`, `markers`, `roles`, `customPrivileges`, `isDisabled`, `createTime`, `updateTime`) VALUES
('1', 'u-admin', 'o-sys', 'admin', '03449cf93ebd8f67f652f9a82b2148380b2597eedd777963245472be3311e75f3ae516244b6d7648b9b044e2523c2840bdf86a852037db7e58e9b216539b2d21', '系统管理员', NULL, NULL, 'sa', '*', '0', '2017-07-28 18:08:03', '2019-02-25 03:19:24');



/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;