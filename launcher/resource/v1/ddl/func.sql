-- -------------------------------------------------------------
-- TablePlus 3.5.4(317)
--
-- https://tableplus.com/
--
-- Database: ft_data_processor
-- Generation Time: 2020-06-08 21:03:01.4790
-- -------------------------------------------------------------


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;


DROP TABLE IF EXISTS `biz_cache_df_workspace`;
CREATE TABLE `biz_cache_df_workspace` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `uuid` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT 'UUID',
  `wsName` varchar(256) DEFAULT NULL COMMENT '名称',
  `dataJSON` json NOT NULL COMMENT '数据JSON',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `BIZ` (`id`),
  UNIQUE KEY `UUID` (`uuid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='DataFlux工作空间（缓存）';

DROP TABLE IF EXISTS `biz_main_auth_link`;
CREATE TABLE `biz_main_auth_link` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '兼任Token',
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '函数ID',
  `funcKwargsJSON` json NOT NULL COMMENT '函数参数JSON (kwargs)',
  `expireTime` datetime DEFAULT NULL COMMENT '过期时间（NULL表示永不过期）',
  `throttlingJSON` json DEFAULT NULL COMMENT '限流JSON（value="<From Parameter>"表示从参数获取）',
  `origin` varchar(64) NOT NULL DEFAULT 'API' COMMENT '来源 API|UI',
  `showInDoc` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否在文档中显示',
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否禁用',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='授权链接';

DROP TABLE IF EXISTS `biz_main_batch`;
CREATE TABLE `biz_main_batch` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '兼任Token',
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '函数ID',
  `funcKwargsJSON` json NOT NULL COMMENT '函数参数JSON (kwargs)',
  `tagsJSON` json DEFAULT NULL COMMENT '批处理标签JSON',
  `origin` varchar(64) NOT NULL DEFAULT 'API' COMMENT '来源 API|UI',
  `showInDoc` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否在文档中显示',
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否禁用',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='批处理';

DROP TABLE IF EXISTS `biz_main_batch_task_info`;
CREATE TABLE `biz_main_batch_task_info` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `batchId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '批处理ID',
  `queueTime` timestamp NULL DEFAULT NULL COMMENT '入队时间',
  `startTime` timestamp NULL DEFAULT NULL COMMENT '启动时间',
  `endTime` timestamp NULL DEFAULT NULL COMMENT '结束时间',
  `status` varchar(64) NOT NULL DEFAULT 'queued' COMMENT '状态 queued|pending|success|failure',
  `logMessageTEXT` text COMMENT '日志信息TEXT',
  `einfoTEXT` text COMMENT '错误信息TEXT',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `BATCH_ID` (`batchId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='批处理任务信息';

DROP TABLE IF EXISTS `biz_main_crontab_config`;
CREATE TABLE `biz_main_crontab_config` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '函数ID',
  `funcKwargsJSON` json NOT NULL COMMENT '函数参数JSON (kwargs)',
  `crontab` varchar(64) DEFAULT NULL COMMENT '执行频率（Crontab语法）',
  `tagsJSON` json DEFAULT NULL COMMENT '自动触发配置标签JSON',
  `saveResult` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否需要保存结果',
  `scope` varchar(256) NOT NULL DEFAULT 'GLOBAL' COMMENT '范围',
  `configHash` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL COMMENT '配置哈希',
  `expireTime` timestamp NULL DEFAULT NULL COMMENT '过期时间',
  `origin` varchar(64) NOT NULL DEFAULT 'API' COMMENT '来源 API|UI',
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否已禁用',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `SCOPE_CONFIG_HASH` (`scope`,`configHash`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='自动触发配置';

DROP TABLE IF EXISTS `biz_main_crontab_task_info`;
CREATE TABLE `biz_main_crontab_task_info` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `crontabConfigId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '自动触发配置ID',
  `queueTime` timestamp NULL DEFAULT NULL COMMENT '入队时间',
  `startTime` timestamp NULL DEFAULT NULL COMMENT '启动时间',
  `endTime` timestamp NULL DEFAULT NULL COMMENT '结束时间',
  `status` varchar(64) NOT NULL DEFAULT 'queued' COMMENT '状态 queued|pending|success|failure',
  `logMessageTEXT` text COMMENT '日志信息TEXT',
  `einfoTEXT` text COMMENT '错误信息TEXT',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `CRONTAB_CONFIG_ID` (`crontabConfigId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='自动触发任务信息';

DROP TABLE IF EXISTS `biz_main_data_source`;
CREATE TABLE `biz_main_data_source` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述',
  `refName` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '引用名',
  `type` varchar(64) NOT NULL COMMENT '类型 influxdb|mysql|redis|..',
  `configJSON` json NOT NULL COMMENT '配置JSON',
  `isBuiltin` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否为内建数据源',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`refName`)
) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8mb4 COMMENT='数据源';

DROP TABLE IF EXISTS `biz_main_env_variable`;
CREATE TABLE `biz_main_env_variable` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述',
  `refName` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '引用名',
  `valueTEXT` text NOT NULL COMMENT '值',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`refName`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='环境变量';

DROP TABLE IF EXISTS `biz_main_func`;
CREATE TABLE `biz_main_func` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述（函数文档）',
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '所属脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '所属脚本ID',
  `refName` varchar(256) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '引用名（函数名）',
  `definition` varchar(256) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '定义（函数签名）',
  `argsJSON` json DEFAULT NULL COMMENT '位置参数JSON',
  `kwargsJSON` json DEFAULT NULL COMMENT '命名参数JSON',
  `extraConfigJSON` json DEFAULT NULL COMMENT '函数额外配置JSON',
  `category` varchar(64) NOT NULL DEFAULT 'general' COMMENT '类别 general|prediction|transformation|action|command|query|check',
  `tagsJSON` json DEFAULT NULL COMMENT '函数标签JSON',
  `defOrder` int(11) NOT NULL DEFAULT '0' COMMENT '定义顺序',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6),
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`scriptId`,`refName`),
  KEY `REF_NAME` (`refName`),
  KEY `CATEGORY` (`category`)
) ENGINE=InnoDB AUTO_INCREMENT=46 DEFAULT CHARSET=utf8mb4 COMMENT='函数';

DROP TABLE IF EXISTS `biz_main_func_store`;
CREATE TABLE `biz_main_func_store` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `key` varchar(256) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '键名',
  `valueJSON` json NOT NULL COMMENT '值JSON',
  `scope` varchar(256) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT 'GLOBAL' COMMENT '范围',
  `expireAt` int(11) DEFAULT NULL COMMENT '过期时间（秒级UNIX时间戳）',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`scope`,`key`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='函数存储';

DROP TABLE IF EXISTS `biz_main_script`;
CREATE TABLE `biz_main_script` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述',
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '所属脚本集ID',
  `refName` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '引用名',
  `publishVersion` bigint(20) NOT NULL DEFAULT '0' COMMENT '发布版本',
  `type` varchar(64) NOT NULL DEFAULT 'python' COMMENT '类型 python|javascript',
  `code` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_bin COMMENT '代码',
  `codeMD5` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL COMMENT '代码MD5',
  `codeDraft` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_bin COMMENT '代码（编辑中草稿）',
  `codeDraftMD5` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL COMMENT '代码（编辑中草稿）MD5',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`scriptSetId`,`refName`),
  FULLTEXT KEY `FT` (`refName`,`code`,`codeDraft`)
) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8mb4 COMMENT='脚本';

DROP TABLE IF EXISTS `biz_main_script_failure`;
CREATE TABLE `biz_main_script_failure` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `scriptPublishVersion` bigint(20) NOT NULL COMMENT '脚本发布版本',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '函数ID',
  `execMode` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '执行模式 sync|async|crontab',
  `einfoTEXT` text COMMENT '错误信息',
  `exception` varchar(64) DEFAULT NULL COMMENT '异常',
  `traceInfoJSON` json DEFAULT NULL COMMENT '跟踪信息JSON',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ENTRY` (`scriptId`,`scriptPublishVersion`,`funcId`),
  KEY `CREATE_TIME` (`createTime`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本故障信息';

DROP TABLE IF EXISTS `biz_main_script_log`;
CREATE TABLE `biz_main_script_log` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `scriptSetId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `scriptPublishVersion` bigint(20) NOT NULL COMMENT '脚本发布版本',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '函数ID',
  `execMode` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '执行模式 sync|async|crontab',
  `messageTEXT` text COMMENT '日志信息TEXT',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ENTRY` (`scriptId`,`scriptPublishVersion`,`funcId`),
  KEY `CREATE_TIME` (`createTime`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本日志信息';

DROP TABLE IF EXISTS `biz_main_script_publish_history`;
CREATE TABLE `biz_main_script_publish_history` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `scriptSetId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `scriptPublishVersion` bigint(20) NOT NULL COMMENT '脚本发布版本',
  `scriptCode_cache` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本代码（缓存字段）',
  `note` text COMMENT '发布备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`scriptId`,`scriptPublishVersion`),
  FULLTEXT KEY `FT` (`scriptCode_cache`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本发布历史';

DROP TABLE IF EXISTS `biz_main_script_recover_point`;
CREATE TABLE `biz_main_script_recover_point` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `type` varchar(64) NOT NULL COMMENT '类型 publish|import|manual',
  `tableDumpJSON` json NOT NULL COMMENT '表备份数据JSON',
  `note` text COMMENT '备注',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本还原点';

DROP TABLE IF EXISTS `biz_main_script_set`;
CREATE TABLE `biz_main_script_set` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) DEFAULT NULL COMMENT '标题',
  `description` text COMMENT '描述',
  `refName` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '引用名',
  `type` varchar(64) NOT NULL DEFAULT 'user' COMMENT '类型 user|official',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `BIZ` (`refName`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 COMMENT='脚本集';

DROP TABLE IF EXISTS `biz_main_script_set_export_history`;
CREATE TABLE `biz_main_script_set_export_history` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `importToken` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '导入令牌',
  `exportType` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '导出类型 user|official',
  `operatorName` text COMMENT '操作人',
  `operationNote` text COMMENT '操作备注',
  `summaryJSON` json NOT NULL COMMENT '导出摘要JSON',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='导出历史';

DROP TABLE IF EXISTS `biz_main_script_set_import_history`;
CREATE TABLE `biz_main_script_set_import_history` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `importType` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '导入类型 user|official',
  `operationNote` text COMMENT '操作备注',
  `summaryJSON` json NOT NULL COMMENT '导出摘要JSON',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='导出历史';

DROP TABLE IF EXISTS `biz_main_task_result_data_processor`;
CREATE TABLE `biz_main_task_result_data_processor` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `task` varchar(128) NOT NULL,
  `origin` varchar(128) DEFAULT NULL,
  `startTime` int(11) DEFAULT NULL COMMENT '任务开始时间(秒级UNIX时间戳)',
  `endTime` int(11) DEFAULT NULL COMMENT '任务结束时间(秒级UNIX时间戳)',
  `argsJSON` json DEFAULT NULL COMMENT '列表参数JSON',
  `kwargsJSON` json DEFAULT NULL COMMENT '字典参数JSON',
  `retvalJSON` json DEFAULT NULL COMMENT '执行结果JSON',
  `status` varchar(64) NOT NULL COMMENT '任务状态: SUCCESS|FAILURE',
  `einfoTEXT` text COMMENT '错误信息TEXT',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `TASK` (`task`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='FTDataProcessor 任务结果';

DROP TABLE IF EXISTS `biz_rel_func_df_workspace`;
CREATE TABLE `biz_rel_func_df_workspace` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `funcId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '函数ID',
  `dfWorkspaceUUID` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT 'DataFlux 工作空间 UUID',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `BIZ` (`funcId`,`dfWorkspaceUUID`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='函数-DataFlux工作空间关联';

DROP TABLE IF EXISTS `biz_rel_script_running_info`;
CREATE TABLE `biz_rel_script_running_info` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `scriptSetId` char(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本集ID',
  `scriptId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '脚本ID',
  `funcId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT '' COMMENT '函数ID',
  `execMode` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '执行模式 sync|async|crontab',
  `succeedCount` int(11) NOT NULL DEFAULT '0' COMMENT '成功次数',
  `failCount` int(11) NOT NULL DEFAULT '0' COMMENT '失败次数',
  `minCost` int(11) DEFAULT NULL COMMENT '最小耗时（毫秒）',
  `maxCost` int(11) DEFAULT NULL COMMENT '最大耗时（毫秒）',
  `totalCost` bigint(11) DEFAULT NULL COMMENT '累积耗时（毫秒）',
  `latestCost` int(11) DEFAULT NULL COMMENT '最近消耗（毫秒）',
  `latestSucceedTime` timestamp NULL DEFAULT NULL COMMENT '最近成功时间',
  `latestFailTime` timestamp NULL DEFAULT NULL COMMENT '最近失败时间',
  `status` varchar(64) NOT NULL DEFAULT 'neverStarted' COMMENT '状态 neverStarted|succeed|fail|timeout',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `BIZ` (`scriptSetId`,`scriptId`,`funcId`,`execMode`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='脚本执行信息';

DROP TABLE IF EXISTS `wat_main_access_key`;
CREATE TABLE `wat_main_access_key` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `organizationId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `userId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `name` varchar(256) NOT NULL,
  `secret` varchar(64) NOT NULL,
  `webhookURL` text,
  `webhookEvents` text,
  `allowWebhookEcho` tinyint(1) NOT NULL DEFAULT '0',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORGANIZATION_ID` (`organizationId`),
  KEY `USER_ID` (`userId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_file`;
CREATE TABLE `wat_main_file` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `organizationId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `userId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `originalFileName` text,
  `md5Sum` char(32) DEFAULT NULL,
  `byteSize` int(11) DEFAULT NULL,
  `note` text,
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORGANIZATION_ID` (`organizationId`),
  KEY `USER_ID` (`userId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_organization`;
CREATE TABLE `wat_main_organization` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `uniqueId` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `name` varchar(256) NOT NULL,
  `markers` text,
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `UNIQUE_ID` (`uniqueId`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_post`;
CREATE TABLE `wat_main_post` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `organizationId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `userId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `title` varchar(256) NOT NULL,
  `content` text,
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `ORGANIZATION_ID` (`organizationId`),
  KEY `USER_ID` (`userId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_system_config`;
CREATE TABLE `wat_main_system_config` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `value` text CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL COMMENT '值',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_task_result_example`;
CREATE TABLE `wat_main_task_result_example` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `task` varchar(128) NOT NULL,
  `origin` varchar(128) DEFAULT NULL,
  `startTime` int(11) DEFAULT NULL,
  `endTime` int(11) DEFAULT NULL,
  `argsJSON` text,
  `kwargsJSON` text,
  `retvalJSON` text,
  `status` varchar(64) NOT NULL,
  `einfoTEXT` text,
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  KEY `TASK` (`task`),
  KEY `ORIGIN` (`origin`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_main_user`;
CREATE TABLE `wat_main_user` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `id` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `organizationId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `username` varchar(64) NOT NULL,
  `passwordHash` text,
  `name` varchar(256) DEFAULT NULL,
  `mobile` varchar(32) DEFAULT NULL,
  `markers` text,
  `roles` text,
  `customPrivileges` text,
  `isDisabled` tinyint(1) NOT NULL DEFAULT '0',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `ID` (`id`),
  UNIQUE KEY `USERNAME` (`username`),
  KEY `ORGANIZATION_ID` (`organizationId`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4;

DROP TABLE IF EXISTS `wat_ref_tag`;
CREATE TABLE `wat_ref_tag` (
  `seq` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `entityId` char(65) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL,
  `entityName` varchar(64) NOT NULL,
  `tagK` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  `tagV` text,
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`seq`),
  UNIQUE KEY `TAG` (`entityId`,`tagK`),
  KEY `ENTITY_ID` (`entityId`),
  KEY `TAG_K` (`tagK`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

INSERT INTO `biz_main_data_source` (`seq`, `id`, `title`, `description`, `refName`, `type`, `configJSON`, `isBuiltin`, `createTime`, `updateTime`) VALUES
('1', 'dsrc-demo_influxdb', '示例InfluxDB', '一个用于演示的InfluxDB', 'demo_influxdb', 'influxdb', '{\"host\": \"127.0.0.1\", \"port\": 8086, \"user\": null, \"charset\": null, \"database\": \"demo\", \"password\": null}', '0', '2019-12-09 15:30:11', '2020-02-26 00:57:15'),
('2', 'dsrc-demo_mysql', '示例MySQL', '一个用于演示的MySQL', 'demo_mysql', 'mysql', '{\"host\": \"127.0.0.1\", \"port\": 3306, \"user\": \"dev\", \"charset\": \"utf8mb4\", \"database\": \"demo\", \"password\": \"dev\"}', '0', '2019-12-09 15:33:02', '2020-02-26 00:57:18'),
('3', 'dsrc-demo_redis', '示例Redis', '一个用于演示的Redis', 'demo_redis', 'redis', '{\"host\": \"127.0.0.1\", \"port\": 6379, \"database\": \"15\", \"password\": null}', '0', '2019-12-09 15:33:57', '2020-02-26 00:57:22'),
('4', 'dsrc-demo_clickhouse', '示例ClickHouse', '一个用于演示的ClickHouse', 'demo_clickhouse', 'clickhouse', '{\"host\": \"127.0.0.1\", \"port\": 9000, \"user\": \"default\", \"database\": \"demo\", \"passwordCipher\": \"sfbR0odZIw5oYEudbhc6lw==\"}', '0', '2020-02-25 15:58:55', '2020-02-26 00:57:27');

INSERT INTO `biz_main_func` (`seq`, `id`, `title`, `description`, `scriptSetId`, `scriptId`, `refName`, `definition`, `argsJSON`, `kwargsJSON`, `extraConfigJSON`, `category`, `tagsJSON`, `defOrder`, `createTime`, `updateTime`) VALUES
('1', 'func-WmsMjxrBesLPNTFpMoLWUe', '平均值预测', '一个对时序数据进行平均值预测\n参数：\n    dps【必须】: [ , ... ]\n返回：\n    [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_pred', 'avg_prediction', 'avg_prediction(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'prediction', NULL, '0', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('2', 'func-kNQM6frnawiKyvRxxTwLLc', '移动均值预测', '一个对时序数据进行平均值预测\n参数：\n    dps【必须】: [ , ... ]\n返回：\n    [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_pred', 'moving_prediction', 'moving_prediction(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'prediction', NULL, '1', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('3', 'func-tdsfTorGHBHGqX2DypkbJn', 'Holt函数预测', '一个对时序数据进行平均值预测\n参数：\n    dps【必须】: [ , ... ]\n返回：\n    [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_pred', 'holt_prediction', 'holt_prediction(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'prediction', NULL, '2', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('4', 'func-gggLctbayAhMhB72XJEXG6', 'MA函数预测', NULL, 'sset-ft_lib', 'scpt-ft_pred', 'ma_prediction', 'ma_prediction(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'prediction', NULL, '3', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('5', 'func-W4avvcyk5PzKQoyPn2dJAR', 'AR函数预测', NULL, 'sset-ft_lib', 'scpt-ft_pred', 'ar_prediction', 'ar_prediction(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'prediction', NULL, '4', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('6', 'func-SJ4T2jPvowQGDdayDNbMyP', 'ARIMA函数预测', NULL, 'sset-ft_lib', 'scpt-ft_pred', 'arima_prediction', 'arima_prediction(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'prediction', NULL, '5', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('7', 'func-yeJoDBaWFqUN4cXRiRJHhL', 'ABS绝对值转换', '返回数字的绝对值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'abs_transformation', 'abs_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '0', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('8', 'func-PnKuMRJCezk2wPn9a39Xt9', 'AVG平均值转换', '返回数字的平均值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'avg_transformation', 'avg_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '1', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('9', 'func-SqugQfs7eAZrb3Y7HFr7A8', 'MAX最大值转换', '返回数字的最大值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'max_transformation', 'max_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '2', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('10', 'func-BQbZ5hymZyngZQz2bVrgJP', 'MIN最小值转换', '返回数字的最小值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'min_transformation', 'min_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '3', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('11', 'func-mF3Tpm5k2dpSJMJMPH4kQn', 'Even向上取偶数转换', '将数字向上舍入为最接近的偶数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'even_transformation', 'even_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '4', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('12', 'func-xFhi3RpAVn8ADmRigdC3b4', 'Odd向上取奇数转换', '将数字向上舍入为最接近的奇数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'odd_transformation', 'odd_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '5', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('13', 'func-guZeCRgvaGrKCMe3bAavBT', 'Fact取整阶乘转换', '返回数字的阶乘。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'fact_transformation', 'fact_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '6', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('14', 'func-6RcmqQ6LWBzJALR5mN43JX', 'SumSQ平方和转换', '返回数字的平方和。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'sumsq_transformation', 'sumsq_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '7', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('15', 'func-3KfGMDXCiLqtYikoYMqtQT', 'RoundUp绝对值增大取整转换', '向绝对值增大的方向舍入数字。\n参数：\n   dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'roundup_transformation', 'roundup_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '8', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('16', 'func-DJ4Z77G3z6Ghjz6tCRMk4d', 'RoundDown绝对值减小取整转换', '向绝对值减小的方向舍入数字。\n参数：\n  dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'rounddown_transformation', 'rounddown_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '9', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('17', 'func-3W2y4ADKnQMAxyhUmjxcpn', 'Power指数幂转换', '返回给定次幂的结果。\n参数：\n dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n significance:指定指数', 'sset-ft_lib', 'scpt-ft_tran', 'power_transformation', 'power_transformation(dps, significance)', '[\"dps\", \"significance\"]', '{\"dps\": null, \"significance\": null}', NULL, 'transformation', NULL, '10', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('18', 'func-8UcWPvd4AC35bybJKCdTAH', 'Ceiling取整转换', '将数字四舍五入为最接近的整数或最接近的指定基数的倍数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    significance:指定基数的倍数', 'sset-ft_lib', 'scpt-ft_tran', 'ceiling_transformation', 'ceiling_transformation(dps, significance)', '[\"dps\", \"significance\"]', '{\"dps\": null, \"significance\": null}', NULL, 'transformation', NULL, '11', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('19', 'func-mSakGXLnC58wm82y7mDeBB', 'AccumuAll累加转换', '将前n个元素累加求和。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'accumuall_transformation', 'accumuall_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '12', '2020-04-02 17:47:45', '2020-06-06 07:07:04.914246'),
('20', 'func-ppeJAW8CAQFQBJtvRuxTJG', 'Accumu2求和转换', '将每相邻2个元素累加求和。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'accumu2_transformation', 'accumu2_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '13', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('21', 'func-o7YnpcRJCcTN2tEiBkMZ8F', 'ACOSH反双曲余弦转换', '返回数字的反双曲余弦值。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran', 'acosh_transformation', 'acosh_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '14', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('22', 'func-SWvP6bzYLCvu2YoCsbvA5U', 'Clean删除不可打印字符', '从文本中删除不可打印的字符，去掉了前31个特殊的ASCII字符。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'clean_transformation', 'clean_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '0', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('23', 'func-CARHseV33FfsjSY8oXZkP4', 'Fixed数字格式化', '将数字格式设置为具有固定小数位数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    decimals:进行四舍五入后小数点后面需要保留的位数,如果是负数，则在小数点左侧进行四舍五入。若此参数                         decimals省略，则默认此参数为2。\n    no_commas:如果是 true，返回的结果不包含逗号千分位。如果是false或省略不写，返回的结果包含逗号千分位。', 'sset-ft_lib', 'scpt-ft_tran_str', 'fixed_transformation', 'fixed_transformation(dps, no_commas=\'True\', decimals=2)', '[\"dps\", \"no_commas\", \"decimals\"]', '{\"dps\": null, \"decimals\": null, \"no_commas\": null}', NULL, 'transformation', NULL, '1', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('24', 'func-V7UtvTq2hMpqD4q5m8iCjH', 'Left截取左边字符', '返回文本值中最左边的字符。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    num_chars:需要截取字符的个数。默认截取1个', 'sset-ft_lib', 'scpt-ft_tran_str', 'left_transformation', 'left_transformation(dps, num_chars=1)', '[\"dps\", \"num_chars\"]', '{\"dps\": null, \"num_chars\": null}', NULL, 'transformation', NULL, '2', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('25', 'func-eaYrA9wLeJLZPuvjaB5Tjd', 'Right截取右边字符', '返回文本值中最右边的字符。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    num_chars:需要截取字符的个数。默认截取1个', 'sset-ft_lib', 'scpt-ft_tran_str', 'right_transformation', 'right_transformation(dps, num_chars=None)', '[\"dps\", \"num_chars\"]', '{\"dps\": null, \"num_chars\": null}', NULL, 'transformation', NULL, '3', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('26', 'func-svkUj2CTi7mPMQogfGvTBB', 'LEN字符串长度', '返回文本字符串中的字符数。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'len_transformation', 'len_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '4', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('27', 'func-XtQ7cZsvRqEtKj34JHHDTc', 'Lower字符小写转换', '将文本转换为小写。\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'lower_transformation', 'lower_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '5', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('28', 'func-PSq9c97zudhPzkHhkXdpRi', 'MID截取字符', '在文本字符串中,从您所指定的位置开始返回特定数量的字符。\n参数：\n   dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n   start_num:从左侧第几位开始截取。\n   num_chars:需要截取字符的个数。默认截取1个。', 'sset-ft_lib', 'scpt-ft_tran_str', 'mid_transformation', 'mid_transformation(dps, start_num=1, num_chars=1)', '[\"dps\", \"start_num\", \"num_chars\"]', '{\"dps\": null, \"num_chars\": null, \"start_num\": null}', NULL, 'transformation', NULL, '6', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('29', 'func-G6x4PKF2HUKpBHXR2iCUN6', 'PROPER首字母转大写', '将字符串的首字母转为大写，其余变为小写规范化输出\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    sep【默认是空格】：要转换的文本的切分符', 'sset-ft_lib', 'scpt-ft_tran_str', 'proper_transformation', 'proper_transformation(dps, sep=None)', '[\"dps\", \"sep\"]', '{\"dps\": null, \"sep\": null}', NULL, 'transformation', NULL, '7', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('30', 'func-tuyhhfeRrvjPPdtWyeVxjg', 'REPLACE替换目标字符', '替换字符串中的目标字符\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    target：被替换的字符\n    replace:要替换的字符', 'sset-ft_lib', 'scpt-ft_tran_str', 'replace_transformation', 'replace_transformation(dps, target, replace)', '[\"dps\", \"target\", \"replace\"]', '{\"dps\": null, \"target\": null, \"replace\": null}', NULL, 'transformation', NULL, '8', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('31', 'func-NQD39Hb9HNgTojP6orwwg5', 'REPT复制目标文本', '复制目标文本追加到文本末尾\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    target：要复制的字符\n    num:复制次数', 'sset-ft_lib', 'scpt-ft_tran_str', 'rept_transformation', 'rept_transformation(dps, target, num)', '[\"dps\", \"target\", \"num\"]', '{\"dps\": null, \"num\": null, \"target\": null}', NULL, 'transformation', NULL, '9', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('32', 'func-qytKQtGsTHT5TYqQKcv5YS', 'SUBSTITUTE替换文本', '在文本字符串中用新文本替换旧文本\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    replace：进行替换的字符', 'sset-ft_lib', 'scpt-ft_tran_str', 'substitute_transformation', 'substitute_transformation(dps, replace)', '[\"dps\", \"replace\"]', '{\"dps\": null, \"replace\": null}', NULL, 'transformation', NULL, '10', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('33', 'func-wG5mztbsDTgn5eTSke5cGa', 'TRIM去除文本空格', '去除文本中空格\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'trim_transformation', 'trim_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '11', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('34', 'func-zcoiFwWcUHsYvA4WYK4BM9', 'UPPER将文本转为大写', '将文本转为大写\n参数：\n    dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]', 'sset-ft_lib', 'scpt-ft_tran_str', 'upper_transformation', 'upper_transformation(dps)', '[\"dps\"]', '{\"dps\": null}', NULL, 'transformation', NULL, '12', '2020-04-02 17:47:46', '2020-06-06 07:07:04.914246'),
('35', 'func-CdQEmEhgJjVZ24iAgcT3ZK', '日志处理（按JSON字段提取）', '按照JSON方式处理日志', 'sset-ft_lib', 'scpt-ft_bat_log', 'log_process_json_abstract', 'log_process_json_abstract(log_data, rules)', '[\"log_data\", \"rules\"]', '{\"rules\": {}, \"log_data\": {}}', NULL, 'builtinBatchLog', NULL, '0', '2020-06-05 07:38:47', '2020-06-08 13:01:12.895229'),
('36', 'func-x8Qpr3MHYGVEZucwgmo9s7', '日志处理（按正则分组提取）', '按照正则分组方式处理日志', 'sset-ft_lib', 'scpt-ft_bat_log', 'log_process_regexp_abstract', 'log_process_regexp_abstract(log_data, regexp, rules)', '[\"log_data\", \"regexp\", \"rules\"]', '{\"rules\": {}, \"regexp\": {}, \"log_data\": {}}', '{\"isHidden\": true}', 'builtinBatchLog', NULL, '1', '2020-06-05 07:38:47', '2020-06-08 13:01:12.895229'),
('37', 'func-BEELfaMsyqwAWCmarsGmwj', '日志处理（grok方式提取）', '按照grok方式处理日志', 'sset-ft_lib', 'scpt-ft_bat_log', 'log_process_grok_abstract', 'log_process_grok_abstract(log_data, pattern)', '[\"log_data\", \"pattern\"]', '{\"pattern\": {}, \"log_data\": {}}', NULL, 'builtinBatchLog', NULL, '2', '2020-06-05 07:38:47', '2020-06-08 13:01:12.895229'),
('38', 'func-nWTzXJzPtpVJEY9QiPL4y3', 'Hello, world!', '一个最简单的示例，直接返回\"Hello, world!\"字符串', 'sset-demo', 'scpt-demo', 'hello_world', 'hello_world()', '[]', '{}', '{\"cacheResult\": 300}', 'general', NULL, '0', '2020-06-06 07:06:14', '2020-06-08 13:01:12.895229'),
('39', 'func-kBQ2ANTg79pehd6qCjHdEE', '问候', '一个简单的带参数示例，返回JSON', 'sset-demo', 'scpt-demo', 'greeting', 'greeting(your_name)', '[\"your_name\"]', '{\"your_name\": {}}', '{\"cacheResult\": 300}', 'general', NULL, '1', '2020-06-06 07:06:14', '2020-06-08 13:01:12.895229'),
('40', 'func-res7t9X5WR3nLiMhk35RWC', 'InfluxDB操作演示', '本函数从数据源`demo_influxdb`中查询3条`demo`指标并返回', 'sset-demo', 'scpt-demo', 'influxdb_demo', 'influxdb_demo()', '[]', '{}', NULL, 'general', NULL, '2', '2020-06-06 07:06:14', '2020-06-08 13:01:12.895229'),
('41', 'func-kiFSXSPnqdbTjkNMABzKe', 'MySQL操作演示', '本函数从数据源`demo_mysql`中查询`demo`表数据并返回', 'sset-demo', 'scpt-demo', 'mysql_demo', 'mysql_demo()', '[]', '{}', NULL, 'general', NULL, '3', '2020-06-06 07:06:14', '2020-06-08 13:01:12.895229'),
('42', 'func-VFhYxCbCLo4xEBLrUGGV8g', 'Redis操作演示', '本函数对数据源`demo_redis`中的`demo`键进行加一计数\n并返回当前`demo`键的值', 'sset-demo', 'scpt-demo', 'redis_demo', 'redis_demo()', '[]', '{}', NULL, 'general', NULL, '4', '2020-06-06 07:06:14', '2020-06-08 13:01:12.895229'),
('43', 'func-BtEZPB5xQGFpEvZAnpqeZY', 'ClickHouse操作演示', '本函数从数据源`demo_clickhouse`中查询`demo_table`表数据并返回', 'sset-demo', 'scpt-demo', 'clickhouse_demo', 'clickhouse_demo()', '[]', '{}', NULL, 'general', NULL, '5', '2020-06-06 07:06:14', '2020-06-08 13:01:12.895229'),
('44', 'func-Q4gKsywduZBWgrkV7eQiJL', '内置DataFlux DataWay操作演示', '本函数对数据源`df_dataway`添加一个数据点，并返回是否成功', 'sset-demo', 'scpt-demo', 'df_dataway_demo', 'df_dataway_demo()', '[]', '{}', NULL, 'general', NULL, '6', '2020-06-06 07:06:14', '2020-06-08 13:01:12.895229'),
('45', 'func-UYqwk9p3XdNNJJw8fJyTAC', '简单检测', '简单检测函数', 'sset-ft_lib', 'scpt-ft_chk', 'simple_check', 'simple_check(trigger_info, targets, condition_check_setting, no_data_check_setting=None, silent_timeout=None, extra_alert_tags=None)', '[\"trigger_info\", \"targets\", \"condition_check_setting\", \"no_data_check_setting\", \"silent_timeout\", \"extra_alert_tags\"]', '{\"targets\": {}, \"trigger_info\": {}, \"silent_timeout\": {\"default\": null}, \"extra_alert_tags\": {\"default\": null}, \"no_data_check_setting\": {\"default\": null}, \"condition_check_setting\": {}}', '{\"isHidden\": true}', 'builtinCheck', NULL, '0', '2020-06-08 13:00:56', '2020-06-08 13:01:12.895229');

INSERT INTO `biz_main_script` (`seq`, `id`, `title`, `description`, `scriptSetId`, `refName`, `publishVersion`, `type`, `code`, `codeMD5`, `codeDraft`, `codeDraftMD5`, `createTime`, `updateTime`) VALUES
('1', 'scpt-demo', '示例代码', '主要包含了一些用于展示DataFlux.f(x) 功能的代码，可以删除', 'sset-demo', 'demo', '1', 'python', '@DFF.API(\'Hello, world!\', cache_result=300)\ndef hello_world():\n    \'\'\'\n    一个最简单的示例，直接返回\"Hello, world!\"字符串\n    \'\'\'\n    return \'Hello, world!\'\n\n@DFF.API(\'问候\', cache_result=300)\ndef greeting(your_name):\n    \'\'\'\n    一个简单的带参数示例，返回JSON\n    \'\'\'\n    ret = {\n        \'message\': \'Hello! {}\'.format(your_name)\n    }\n    return ret\n\n@DFF.API(\'InfluxDB操作演示\')\ndef influxdb_demo():\n    \'\'\'\n    本函数从数据源`demo_influxdb`中查询3条`demo`指标并返回\n    \'\'\'\n    db = DFF.SRC(\'demo_influxdb\')\n    return db.query(\'SELECT * FROM demo LIMIT 3\')\n\n@DFF.API(\'MySQL操作演示\')\ndef mysql_demo():\n    \'\'\'\n    本函数从数据源`demo_mysql`中查询`demo`表数据并返回\n    \'\'\'\n    helper = DFF.SRC(\'demo_mysql\')\n    return helper.query(\'SELECT * FROM demo\')\n\n@DFF.API(\'Redis操作演示\')\ndef redis_demo():\n    \'\'\'\n    本函数对数据源`demo_redis`中的`demo`键进行加一计数\n    并返回当前`demo`键的值\n    \'\'\'\n    helper = DFF.SRC(\'demo_redis\')\n    helper.run(\'incr\', \'demo\')\n    return helper.run(\'get\', \'demo\')\n\n@DFF.API(\'ClickHouse操作演示\')\ndef clickhouse_demo():\n    \'\'\'\n    本函数从数据源`demo_clickhouse`中查询`demo_table`表数据并返回\n    \'\'\'\n    helper = DFF.SRC(\'demo_clickhouse\')\n    return helper.query(\'SELECT * FROM demo_table\')\n\n@DFF.API(\'内置DataFlux DataWay操作演示\')\ndef df_dataway_demo():\n    \'\'\'\n    本函数对数据源`df_dataway`添加一个数据点，并返回是否成功\n    \'\'\'\n    helper = DFF.SRC(\'df_dataway\')\n    return helper.write_point(\n        measurement=\'some_measurement\',\n        tags={\'name\': \'Tom\'},\n        fields={\'value\': 10},\n        timestamp=None)', '5cd6e2f732e45afbc8fc92659735111b', '@DFF.API(\'Hello, world!\', cache_result=300)\ndef hello_world():\n    \'\'\'\n    一个最简单的示例，直接返回\"Hello, world!\"字符串\n    \'\'\'\n    return \'Hello, world!\'\n\n@DFF.API(\'问候\', cache_result=300)\ndef greeting(your_name):\n    \'\'\'\n    一个简单的带参数示例，返回JSON\n    \'\'\'\n    ret = {\n        \'message\': \'Hello! {}\'.format(your_name)\n    }\n    return ret\n\n@DFF.API(\'InfluxDB操作演示\')\ndef influxdb_demo():\n    \'\'\'\n    本函数从数据源`demo_influxdb`中查询3条`demo`指标并返回\n    \'\'\'\n    db = DFF.SRC(\'demo_influxdb\')\n    return db.query(\'SELECT * FROM demo LIMIT 3\')\n\n@DFF.API(\'MySQL操作演示\')\ndef mysql_demo():\n    \'\'\'\n    本函数从数据源`demo_mysql`中查询`demo`表数据并返回\n    \'\'\'\n    helper = DFF.SRC(\'demo_mysql\')\n    return helper.query(\'SELECT * FROM demo\')\n\n@DFF.API(\'Redis操作演示\')\ndef redis_demo():\n    \'\'\'\n    本函数对数据源`demo_redis`中的`demo`键进行加一计数\n    并返回当前`demo`键的值\n    \'\'\'\n    helper = DFF.SRC(\'demo_redis\')\n    helper.run(\'incr\', \'demo\')\n    return helper.run(\'get\', \'demo\')\n\n@DFF.API(\'ClickHouse操作演示\')\ndef clickhouse_demo():\n    \'\'\'\n    本函数从数据源`demo_clickhouse`中查询`demo_table`表数据并返回\n    \'\'\'\n    helper = DFF.SRC(\'demo_clickhouse\')\n    return helper.query(\'SELECT * FROM demo_table\')\n\n@DFF.API(\'内置DataFlux DataWay操作演示\')\ndef df_dataway_demo():\n    \'\'\'\n    本函数对数据源`df_dataway`添加一个数据点，并返回是否成功\n    \'\'\'\n    helper = DFF.SRC(\'df_dataway\')\n    return helper.write_point(\n        measurement=\'some_measurement\',\n        tags={\'name\': \'Tom\'},\n        fields={\'value\': 10},\n        timestamp=None)', '5cd6e2f732e45afbc8fc92659735111b', '2019-12-09 15:25:22', '2020-06-06 07:07:04'),
('2', 'scpt-ft_chk', 'FT检测函数', '检测函数脚本', 'sset-ft_lib', 'ft_chk', '1', 'python', '\'\'\'\n注意！\n本脚本由驻云官方提供，且在脚本中大量使用了DataFlux.f(x)的内部接口。\n这些内部接口可能会跟随系统更新随时改变，请勿在您的用户脚本中使用！\n\n2020-04-18 优化了动作执行速度\n2020-05-21 新增进阶模板语法，支持指标间运算\n2020-05-25 根据最新数据结构，$alert合并进$keyevent\n2020-06-09 $keyevent改为__keyevent（即相关字段同步修改）\n\'\'\'\n\nimport hashlib\nimport re\nimport time\n\nimport arrow\nimport simplejson as json\nimport six\n\nMOCK_DATA = None\nRAISE_DATAWAY_EXCEPTION = True\n\nALL_LEVELS          = (\'critical\', \'error\', \'warning\', \'info\')\nOK_LEVEL            = \'ok\'\nLEVEL_NAME_MAP      = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nCONDITION_LOGICS    = (\'and\', \'or\')\nCONDITION_OPERATORS = (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nACTION_TYPES        = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS        = (\'GET\', \'POST\')\nHTTP_BODY_TYPES     = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\n_DFF_is_debug = True\n\ndef _set_mock_data(mock_data):\n    global MOCK_DATA\n    MOCK_DATA = mock_data\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n\n    return six.ensure_str(s)\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\ndef _to_cn_time(d=None):\n    return arrow.get(d).to(\'Asia/Shanghai\').format(\'YYYY-MM-DD HH:mm:ss\')\n\ndef _to_iso_time(d=None):\n    return arrow.get(d).isoformat()\n\ndef __param_targets_check(targets):\n    if not isinstance(targets, list):\n        raise Exception(\'`targets` should be a list\')\n\n    for i, t in enumerate(targets):\n        if not isinstance(t, dict):\n            raise Exception(\'`targets[{}]` should be a dict\'.format(i))\n\n        if not isinstance(t.get(\'alias\'), str):\n            raise Exception(\'`targets[{}][\"alias\"]` should be a str\'.format(i))\n\n        if \'influxQL\' in t and not isinstance(t.get(\'influxQL\'), str):\n            raise Exception(\'`targets[{}][\"influxQL\"]` should be a str\'.format(i))\n\n        if \'expression\' in t and not isinstance(t.get(\'expression\'), str):\n            raise Exception(\'`targets[{}][\"expression\"]` should be a str\'.format(i))\n\ndef __param_rules_check(rules, target_alias):\n    if not isinstance(rules, list):\n        raise Exception(\'`rules` should be a list\')\n\n    for i, r in enumerate(rules):\n        if not isinstance(r, dict):\n            raise Exception(\'`rules[{}]` should be a dict\'.format(i))\n\n        if r.get(\'level\') not in ALL_LEVELS:\n            raise Exception(\'`rules[{}][\"level\"]` should be one of {}\'.format(i, \',\'.join(ALL_LEVELS)))\n\n        if r.get(\'conditionLogic\') not in CONDITION_LOGICS:\n            raise Exception(\'`rules[{}][\"conditionLogic\"]` should be one of {}\'.format(i, \',\'.join(CONDITION_LOGICS)))\n\n        if not isinstance(r.get(\'conditions\'), list):\n            raise Exception(\'`rules[{}][\"conditions\"]` should be a list\'.format(i))\n\n        if list(filter(lambda x: not isinstance(x, dict), r[\'conditions\'])):\n            raise Exception(\'Element of `rule[{}][\"conditions\"]` should be a dict\'.format(i))\n\n        for j, c in enumerate(r[\'conditions\']):\n            if c.get(\'targetAlias\') not in target_alias:\n                raise Exception(\'`rules[{}][\"conditions\"][{}][\"targetAlias\"]` should be one of {}\'.format(i, j, \',\'.join(target_alias)))\n\n            if c.get(\'operator\') not in CONDITION_OPERATORS:\n                raise Exception(\'`rules[{}][\"conditions\"][{}][\"operator\"]` should be one of {}\'.format(i, j, \',\'.join(CONDITION_OPERATORS)))\n\n            if not isinstance(c.get(\'operands\'), list):\n                raise Exception(\'`rules[{}][\"conditions\"][{}][\"operands\"]` should be a list\'.format(i, j))\n\n            if list(filter(lambda x: not isinstance(x, (str, int, float)), c[\'operands\'])):\n                raise Exception(\'Element of `rules[{}][\"conditions\"][{}][\"operands\"]` should be a str, int or float\'.format(i))\n\ndef __param_action_check(actions, parent_path):\n    if not isinstance(actions, list):\n        raise Exception(\'`{}actions` should be a list\'.format(parent_path))\n\n    for i, a in enumerate(actions):\n        if not isinstance(a, dict):\n            raise Exception(\'`{}actions[{}]` should be a dict\'.format(parent_path, i))\n\n        if a.get(\'type\') not in ACTION_TYPES:\n            raise Exception(\'`{}actions[{}][\"type\"]` should be one of {}\'.format(parent_path, i, \',\'.join(ACTION_TYPES)))\n\n        if a[\'type\'] == \'mail\':\n            if not isinstance(a.get(\'to\'), list):\n                raise Exception(\'`{}actions[{}][\"to\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'to\'])):\n                raise Exception(\'Element of `{}actions[{}][\"to\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'dingTalkRobot\':\n            if not isinstance(a.get(\'webhook\'), list):\n                raise Exception(\'`{}actions[{}][\"webhook\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'webhook\'])):\n                raise Exception(\'Element of `{}actions[{}][\"webhook\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'markdown\'), str):\n                raise Exception(\'`{}actions[{}][\"markdown\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'ProfWangAlertHub\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'suggestion\') is not None:\n                if not isinstance(a[\'suggestion\'], str):\n                    raise Exception(\'`{}actions[{}][\"suggestion\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'HTTPRequest\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'method\') not in HTTP_METHODS:\n                raise Exception(\'`{}actions[{}][\"method\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_METHODS)))\n\n            if a.get(\'headers\') is not None:\n                if not isinstance(a[\'headers\'], dict):\n                    raise Exception(\'`{}actions[{}][\"headers\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].values())):\n                    raise Exception(\'Value of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n            if a[\'method\'] == \'POST\':\n                if a.get(\'bodyType\') not in HTTP_BODY_TYPES:\n                    raise Exception(\'`{}actions[{}][\"bodyType\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_BODY_TYPES)))\n\n                if not isinstance(a.get(\'body\'), str):\n                    raise Exception(\'`{}actions[{}][\"body\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'DataFluxFunc\':\n            if not isinstance(a.get(\'scriptSetRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptSetRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'scriptRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'funcRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"funcRefName\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'kwargs\') is not None:\n                if not isinstance(a[\'kwargs\'], dict):\n                    raise Exception(\'`{}actions[{}][\"kwargs\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'kwargs\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"kwargs\"]` should be a str\'.format(parent_path, i))\n\ndef _param_check(trigger_info, targets, condition_check_setting, no_data_check_setting=None, silent_timeout=None, extra_alert_tags=None):\n    # 检查`trigger_info`\n    if not isinstance(trigger_info, dict):\n        raise Exception(\'`trigger_info` should be a dict\')\n\n    if not isinstance(trigger_info.get(\'uuid\'), str):\n        raise Exception(\'`trigger_info[\"uuid\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'name\'), str):\n        raise Exception(\'`trigger_info[\"name\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'workspaceToken\'), str):\n        raise Exception(\'`trigger_info[\"workspaceToken\"]` should be a str\')\n\n    dimensions = trigger_info.get(\'dimensions\')\n    if dimensions is not None:\n        if not isinstance(dimensions, list):\n            raise Exception(\'`trigger_info[\"dimensions\"]` should be a list\')\n\n        for index, dimension in enumerate(dimensions):\n            if not isinstance(dimension, str):\n                raise Exception(\'`trigger_info[\"dimensions\"][{}]` should be a str\'.format(index))\n\n    # 检查`targets`\n    __param_targets_check(targets)\n\n    # 检查`condition_check_setting`\n    if not isinstance(condition_check_setting, dict):\n        raise Exception(\'`condition_check_setting` should be a dict\')\n\n    target_alias = [x[\'alias\'] for x in targets]\n    __param_rules_check(condition_check_setting.get(\'rules\'), target_alias=target_alias)\n    __param_action_check(condition_check_setting.get(\'actions\'), parent_path=\'condition_check_setting.\')\n\n    # 检查`no_data_check_setting`\n    if no_data_check_setting is not None:\n        if not isinstance(no_data_check_setting, dict):\n            raise Exception(\'`no_data_check_setting` should be a dict\')\n\n        if no_data_check_setting.get(\'level\') is not None:\n            if no_data_check_setting[\'level\'] not in ALL_LEVELS:\n                raise Exception(\'`no_data_check_setting[\"level\"]` should be one of {}\'.format(\',\'.join(ALL_LEVELS)))\n\n        if no_data_check_setting.get(\'timeout\') is not None:\n            if not isinstance(no_data_check_setting[\'timeout\'], int):\n                raise Exception(\'`no_data_check_setting[\"timeout\"]` should be a int\')\n\n        if no_data_check_setting.get(\'actions\') is not None:\n            __param_action_check(no_data_check_setting[\'actions\'], parent_path=\'no_data_check_setting.\')\n\n    # 检查`silent_timeout`\n    if silent_timeout is not None:\n        if not isinstance(silent_timeout, int):\n            raise Exception(\'`silent_timeout` should be an int\')\n\n    # 检查`extra_alert_tags`\n    if extra_alert_tags is not None:\n        if not isinstance(extra_alert_tags, dict):\n            raise Exception(\'`extra_alert_tags` should be an dict\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.keys())):\n            raise Exception(\'Key of `extra_alert_tags` should be a str\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.values())):\n            raise Exception(\'Value of `extra_alert_tags` should be a str\')\n\ndef _get_store_scope(trigger_uuid):\n    return \'builtinCheck.simpleCheck@{}\'.format(_get_md5(trigger_uuid))\n\ndef _get_alert_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    if _DFF_is_debug:\n        print(\'【告警对象】{}\'.format(str_to_md5))\n    return _get_md5(str_to_md5)\n\ndef _get_alert_store(trigger_uuid, key, alert_id):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.GET(store_key, scope=store_scope)\n\ndef _set_alert_store(trigger_uuid, key, alert_id, value, only_not_exists=False):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.SET(store_key, value=value, scope=store_scope, only_not_exists=only_not_exists)\n\ndef _delete_alert_store(trigger_uuid, key, alert_id):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.DEL(store_key, scope=store_scope)\n\ndef _check_if_alert_level_changed(trigger_uuid, alert_id, level):\n    prev_level = _get_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id) or OK_LEVEL\n    is_level_changed = prev_level != level\n\n    if _DFF_is_debug:\n        print(\'上次检测结果：{}，本次检测结果：{}\'.format(prev_level, level))\n\n    return is_level_changed, prev_level\n\ndef _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed=None, silent_timeout=None):\n    if is_level_changed:\n        # 级别发生改变一定触发动作\n        if _DFF_is_debug:\n            print(\'检测结果改变，一定触发动作。当前检测结果：{}\'.format(level))\n\n        return True\n\n    else:\n        if _DFF_is_debug:\n            print(\'检测结果不变，当前检测结果：{}\'.format(level))\n\n        if level != OK_LEVEL and silent_timeout is not None:\n            # 级别未发生改变时，且保持非`ok`级别，则到达超时时间才触发动作\n            alert_report_time = _get_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id) or 0\n            past_time = _DFF_start_time - alert_report_time\n            should_do_action = past_time > silent_timeout\n\n            if _DFF_is_debug:\n                print(\'距离上次告警经过：{}s\'.format(past_time))\n                if should_do_action:\n                    print(\'由于距离上次告警时间（{}s）已超过沉默时长（{}s），因此需要触发动作\'.format(past_time, silent_timeout))\n                else:\n                    print(\'由于距离上次告警时间（{}s）尚未超过沉默时长（{}s），因此不需要触发动作\'.format(past_time, silent_timeout))\n\n            return should_do_action\n\n    if _DFF_is_debug:\n        print(\'当前检测结果：{}，不需要触发动作\'.format(level))\n\n    return False\n\ndef _update_no_data_duration(trigger_uuid, alert_id, is_no_data):\n    prev_no_data_time = _get_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id)\n    if _DFF_is_debug:\n        if prev_no_data_time:\n            print(\'无数据开始时间：{}\'.format(_to_cn_time(prev_no_data_time)))\n        else:\n            print(\'首次处理无数据告警\')\n\n    if is_no_data:\n        if prev_no_data_time:\n            if _DFF_is_debug:\n                print(\'已经记录过无数据时间，继续...\')\n\n            return _DFF_start_time - prev_no_data_time\n\n        else:\n            _set_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id, value=_DFF_start_time)\n\n            if _DFF_is_debug:\n                print(\'记录无数据时间\')\n\n            return 0\n\n    else:\n        if prev_no_data_time:\n            _delete_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id)\n\n            if _DFF_is_debug:\n                print(\'清除无数据时间\')\n\n            return _DFF_start_time - prev_no_data_time\n\n        else:\n            if _DFF_is_debug:\n                print(\'数据正常，无需处理\')\n\n            return 0\n\ndef _get_alert_duration(trigger_uuid, alert_id):\n    alert_level_change_time = _get_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id) or _DFF_start_time\n\n    if _DFF_is_debug:\n        print(\'告警级别变化时间：{}\'.format(_to_cn_time(alert_level_change_time)))\n\n    return _DFF_start_time - alert_level_change_time\n\ndef _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action):\n    if level == OK_LEVEL:\n        _delete_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id)\n        _delete_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id)\n        _delete_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id)\n\n    else:\n        if is_level_changed:\n            _set_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id, value=_DFF_start_time)\n            _set_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id, value=level)\n\n        if should_do_action:\n            _set_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id, value=_DFF_start_time)\n\ndef _get_check_value(series_data):\n    # 获取非`time`列的索引号\n    for index, name in enumerate(series_data[\'columns\']):\n        if name == \'time\':\n            continue\n\n        return series_data[\'values\'][0][index]\n\n    return None\n\ndef _get_alert_item_tags_no_data(target_alias):\n    alert_item_tags = {\n        \'noData\': target_alias\n    }\n    return alert_item_tags\n\ndef _get_alert_item_tags_json(alert_item_tags):\n    return _json_dumps(alert_item_tags)\n\ndef _from_alert_item_tags_json(alert_item_tags_json):\n    return json.loads(alert_item_tags_json)\n\ndef __do_condition_math(check_value, operator, operand):\n    if isinstance(operand, list):\n        operand = operand[0]\n\n    try:\n        T = type(check_value)\n        operand = T(operand)\n\n        if operator == \'==\':\n            return check_value == operand\n        elif operator == \'!=\':\n            return check_value != operand\n        elif operator == \'>\':\n            return check_value > operand\n        elif operator == \'>=\':\n            return check_value >= operand\n        elif operator == \'<\':\n            return check_value < operand\n        elif operator == \'<=\':\n            return check_value <= operand\n\n    except Exception as e:\n        print(e)\n        return False\n\ndef _do_condition_eq(check_value, operand):\n    return __do_condition_math(check_value, \'==\', operand[0])\n\ndef _do_condition_ne(check_value, operand):\n    return __do_condition_math(check_value, \'!=\', operand[0])\n\ndef _do_condition_gt(check_value, operand):\n    return __do_condition_math(check_value, \'>\', operand[0])\n\ndef _do_condition_ge(check_value, operand):\n    return __do_condition_math(check_value, \'>=\', operand[0])\n\ndef _do_condition_lt(check_value, operand):\n    return __do_condition_math(check_value, \'<\', operand[0])\n\ndef _do_condition_le(check_value, operand):\n    return __do_condition_math(check_value, \'<=\', operand[0])\n\ndef _do_condition_between(check_value, operand):\n    return __do_condition_math(check_value, \'>=\', operand[0]) and __do_condition_math(check_value, \'<=\', operand[1])\n\ndef _do_condition_like(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return operand[0] in check_value\n\ndef _do_condition_not_like(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return not _do_condition_like(check_value, operand)\n\ndef _do_condition_in(check_value, operand):\n    check_value = _as_str(check_value)\n    return check_value in operand\n\ndef _do_condition_not_in(check_value, operand):\n    return not _do_condition_in(check_value, operand)\n\ndef _do_condition_regexp(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return re.search(operand[0], check_value) is not None\n\ndef _do_condition_not_regexp(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return not _do_condition_regexp(check_value, operand)\n\nDO_CONDITION_FUNC_MAP = {\n    \'=\'        : _do_condition_eq,\n    \'==\'       : _do_condition_eq,\n    \'!=\'       : _do_condition_ne,\n    \'>\'        : _do_condition_gt,\n    \'>=\'       : _do_condition_ge,\n    \'<\'        : _do_condition_lt,\n    \'<=\'       : _do_condition_le,\n    \'between\'  : _do_condition_between,\n    \'like\'     : _do_condition_like,\n    \'notlike\'  : _do_condition_not_like,\n    \'in\'       : _do_condition_in,\n    \'notin\'    : _do_condition_not_in,\n    \'regexp\'   : _do_condition_regexp,\n    \'notregexp\': _do_condition_not_regexp,\n}\n\ndef _merge_condition_result(a, b, logic):\n    if a is None:\n        return b\n\n    if b is None:\n        return a\n\n    if logic == \'and\':\n        return a and b\n    elif logic == \'or\':\n        return a or b\n\n    return None\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, alert_item_tags, alert_id, duration, check_value_map=None, prev_level=None):\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\'        : level,\n        \'prevLevel\'    : prev_level,\n        \'levelName\'    : LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'tags\'         : _json_dumps(alert_item_tags),\n        \'tagsEscaped\'  : None,\n        \'fields\'       : None,\n        \'fieldsEscaped\': None,\n        \'duration\'     : duration * 1000,\n        \'durationHuman\': None,\n        \'ruleId\'       : trigger_uuid,\n        \'ruleName\'     : trigger_name,\n        \'alertId\'      : alert_id,\n    }\n\n    # $alert 改为 __keyevent 后需要兼容的字段\n    var_dict[\'status\']         = var_dict[\'level\']\n    var_dict[\'prevStatus\']     = var_dict[\'prevLevel\']\n    var_dict[\'statusName\']     = var_dict[\'levelName\']\n    var_dict[\'prevStatusName\'] = var_dict[\'prevLevelName\']\n    var_dict[\'eventId\']        = var_dict[\'alertId\']\n\n    # 补全tags.KEY\n    for k, v in alert_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全fieldsEscaped、tagsEscaped\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    _m, _s = divmod(duration, 60)\n    _h, _m = divmod(_m, 60)\n    _d, _h = divmod(_h, 24)\n    _human = \'{}秒\'.format(_s)\n    if _m > 0:\n        _human = \'{}分钟\'.format(_m) + _human\n    if _h > 0:\n        _human = \'{}小时\'.format(_h) + _human\n    if _d > 0:\n        _human = \'{}天\'.format(_d) + _human\n\n    var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        # print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep      = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from  = from_unit.lower()\n    _search_to    = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n        carry=1000)\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n        carry=1024)\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n        carry=1024)\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n        carry=1024)\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\'    : __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\'   : __filter_volumn,\n    \'volumni\'  : __filter_volumn_i,\n    \'bitrate\'  : __filter_bit_rate,\n    \'byterate\' : __filter_byte_rate,\n    \'upper\'    : __filter_upper,\n    \'lower\'    : __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\'   : __filter_if_true,\n    \'iffalse\'  : __filter_if_false,\n}\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break;\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break;\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, (six.string_types)):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result =  DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title   = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title   = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to      = action[\'to\']\n    title   = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title   = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\'     : to,\n            \'title\'  : title,\n            \'content\': { \'text\': content },\n            \'sender\' : \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook  = action[\'webhook\']\n    title    = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title    = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\'  : title,\n                \'text\'   : markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url        = action[\'url\']\n    title      = action[\'title\']\n    content    = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title      = __render_text(title, var_dict)\n    content    = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\'    : var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\'      : profwang_level,\n            \'type\'       : \'alarm\',\n            \'title\'      : title,\n            \'content\'    : content,\n            \'suggestion\' : suggestion,\n            \'origin\'     : \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\'  : \'POST\',\n                \'url\'     : _url,\n                \'body\'    : _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'     : title,\n        \'content\'   : content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title    = var_dict[\'ruleName\']\n    url      = action[\'url\']\n    method   = action[\'method\'].upper()\n    headers  = action.get(\'headers\')\n    body     = action.get(\'body\')     or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\'  : method,\n                \'url\'     : _url,\n                \'headers\' : headers,\n                \'query\'   : None,\n                \'body\'    : body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': _json_dumps({\n            \'url\'     : url,\n            \'method\'  : method,\n            \'headers\' : headers,\n            \'body\'    : body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title               = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name     = action[\'scriptRefName\']\n    func_ref_name       = action[\'funcRefName\']\n    kwargs              = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\'   : script_ref_name,\n            \'funcRefName\'     : func_ref_name,\n            \'kwargs\'          : kwargs,\n        }),\n    }\n    return action_content, None\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\'           : _create_action_debug,\n    \'mail\'            : _create_action_mail,\n    \'dingTalkRobot\'   : _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\'     : _create_action_http_request,\n    \'DataFluxFunc\'    : _create_action_dataflux_func,\n}\n\n@DFF.API(\'简单检测\', category=\'builtinCheck\', is_hidden=True)\ndef simple_check(trigger_info, targets, condition_check_setting, no_data_check_setting=None, silent_timeout=None, extra_alert_tags=None):\n    \'\'\'\n    简单检测函数\n    \'\'\'\n    # 参数检查\n    _param_check(trigger_info, targets, condition_check_setting, no_data_check_setting, silent_timeout, extra_alert_tags)\n\n    # 包装上下文函数\n    trigger_uuid    = trigger_info[\'uuid\']\n    trigger_name    = trigger_info[\'name\']\n    workspace_token = trigger_info[\'workspaceToken\']\n    dimensions      = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    dataway_ref_name = \'df_dataway\'\n\n    dataway = DFF.SRC(dataway_ref_name, token=workspace_token)\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    def get_alert_id(alert_item_tags):\n        return _get_alert_id(trigger_uuid, alert_item_tags)\n\n    def create_var_dict(level, alert_item_tags, alert_id, duration=None, check_value_map=None, prev_level=None):\n        return _create_var_dict(trigger_uuid, trigger_name, level, alert_item_tags, alert_id, duration, check_value_map, prev_level)\n\n    def check_if_alert_level_changed(alert_id, level):\n        return _check_if_alert_level_changed(trigger_uuid, alert_id, level)\n\n    def check_if_alert_should_do_action(alert_id, level, is_level_changed):\n        return _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed, silent_timeout)\n\n    def update_no_data_duration(alert_id, is_no_data):\n        return _update_no_data_duration(trigger_uuid, alert_id, is_no_data)\n\n    def get_alert_duration(alert_id):\n        return _get_alert_duration(trigger_uuid, alert_id)\n\n    def store_alert_info(alert_id, level, is_level_changed, should_do_action):\n        return _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action)\n\n    # 查询检测目标数据\n    # 目标数据，结构如下：\n    #     {\n    #         \"<Alert Item Tags JSON>\": {\n    #             \"<Target Alias>\": \"<Check Value>\"\n    #         }\n    #     }\n\n    target_data_map    = {}\n    no_data_alias_list = []\n\n    # 先搜集查询数据\n    for target in filter(lambda x: x.get(\'influxQL\'), targets):\n        db_res = None\n        if MOCK_DATA:\n            db_res = MOCK_DATA[target[\'alias\']]\n        else:\n            bind_params = {}\n            if \'$now\' in target[\'influxQL\']:\n                bind_params[\'now\'] = _to_iso_time(_DFF_trigger_time)\n\n            db_res = DFF.SRC(\'df_influxdb\').query(target[\'influxQL\'], bind_params=bind_params)\n\n        target_alias = target[\'alias\']\n\n        valid_series_data = []\n        if \'series\' in db_res:\n            for s in db_res[\'series\']:\n                if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                    continue\n\n                valid_series_data.append(s)\n\n        if valid_series_data:\n            for series_data in valid_series_data:\n                alert_item_tags_json = _get_alert_item_tags_json(series_data[\'tags\'])\n\n                if alert_item_tags_json not in target_data_map:\n                    target_data_map[alert_item_tags_json] = {}\n\n                target_data_map[alert_item_tags_json][target_alias] = _get_check_value(series_data)\n\n        else:\n            no_data_alias_list.append(target_alias)\n\n    # 再搜集expression数据\n    for alert_item_tags_json, alias_value_map in target_data_map.items():\n        for target in filter(lambda x: x.get(\'expression\'), targets):\n            target_alias      = target[\'alias\']\n            target_expression = target[\'expression\']\n\n            target_data_map[alert_item_tags_json][target_alias] = _compute_target_expression(alias_value_map, target_expression)\n\n    if _DFF_is_debug:\n        print(\'检测对象及数据：{}\'.format(target_data_map))\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    # 1. 执行无数据判断\n    no_data_check_level   = no_data_check_setting.get(\'level\') or \'critical\'\n    no_data_check_timeout = no_data_check_setting.get(\'timeout\') or 0\n    no_data_check_actions = no_data_check_setting.get(\'actions\')\n\n    # 配置了触发动作才实际运行\n    if _DFF_is_debug:\n        print(\'=== 开始执行无数据判断 ===\')\n\n    if no_data_check_actions:\n        for target in filter(lambda x: x.get(\'influxQL\'), targets):\n            # 无数据告警仅针对查询数据\n            target_alias = target[\'alias\']\n\n            alert_item_tags      = _get_alert_item_tags_no_data(target_alias)\n            alert_item_tags_json = _get_alert_item_tags_json(alert_item_tags)\n            alert_id             = get_alert_id(alert_item_tags_json)\n\n            is_no_data = target_alias in no_data_alias_list\n            no_data_duration = update_no_data_duration(alert_id, is_no_data)\n\n            # 计算告警级别\n            level = OK_LEVEL\n            if is_no_data:\n                if no_data_duration > no_data_check_timeout:\n                    level = no_data_check_level\n\n                    if _DFF_is_debug:\n                        print(\'`{}`没有数据持续（{}s），超过指定容许时长（{}s）！\'.format(target_alias, no_data_duration, no_data_check_timeout))\n\n                else:\n                    if _DFF_is_debug:\n                        print(\'`{}`没有数据持续（{}s），但尚未超过指定容许时长（{}s）！\'.format(target_alias, no_data_duration, no_data_check_timeout))\n\n            else:\n                if _DFF_is_debug:\n                    print(\'`{}`存在数据\'.format(target_alias))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, level)\n\n            # 处于沉默期间的，不执行动作\n            if not check_if_alert_should_do_action(alert_id, level, is_level_changed):\n                should_do_action = False\n\n            # 触发动作\n            if should_do_action:\n                check_value_map = {\n                    target_alias: None\n                }\n                var_dict = create_var_dict(\n                        level=level,\n                        prev_level=prev_level,\n                        alert_item_tags=alert_item_tags,\n                        alert_id=alert_id,\n                        duration=no_data_duration,\n                        check_value_map=check_value_map)\n\n                for action in no_data_check_actions:\n                    action_type = action[\'type\']\n\n                    # 记录执行动作\n                    _action = _json_copy(action)\n                    action_content = None\n                    action_task    = None\n                    try:\n                        action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n                    except Exception as e:\n                        print(e)\n                    else:\n                        if action_task:\n                            if isinstance(action_task, list):\n                                actions_to_perform.extend(action_task)\n                            else:\n                                actions_to_perform.append(action_task)\n\n                    # 记录写入__keyevent指标\n                    _title              = action_content[\'title\']\n                    _content            = action_content.get(\'content\')          or None\n                    _suggestion         = action_content.get(\'suggestion\')       or None\n                    _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n\n                    keyevent = {\n                        \'title\'          : _title,\n                        \'timestamp\'      : _DFF_start_time,\n                        \'event_id\'       : alert_id,\n                        \'source\'         : \'datafluxTrigger\',\n                        \'status\'         : level,\n                        \'rule_id\'        : trigger_uuid,\n                        \'rule_name\'      : trigger_name,\n                        \'type\'           : \'noData\',\n                        \'alert_item_tags\': alert_item_tags,\n                        \'action_type\'    : action_type,\n                        \'content\'        : _content,\n                        \'suggestion\'     : _suggestion,\n                        \'duration\'       : no_data_duration,\n                        \'dimensions\'     : dimensions,\n                        \'tags\'           : extra_alert_tags,\n                        \'fields\'         : _extra_alert_fields,\n                    }\n                    keyevents_to_write.append(keyevent)\n\n            # 记录告警信息\n            store_alert_info(alert_id, level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 2. 执行条件判断（级别从高到低）\n    if _DFF_is_debug:\n        print(\'=== 开始执行条件判断 ===\')\n\n    condition_rules = condition_check_setting[\'rules\']\n    condition_rules.sort(key=lambda x: ALL_LEVELS.index(x[\'level\']))\n    condition_actions = condition_check_setting[\'actions\']\n\n    # 配置了触发动作才实际运行\n    if condition_actions:\n        # 遍历检测对象\n        for alert_item_tags_json, check_value_map in target_data_map.items():\n            alert_id        = get_alert_id(alert_item_tags_json)\n            alert_item_tags = _from_alert_item_tags_json(alert_item_tags_json)\n            duration        = get_alert_duration(alert_id)\n\n            if _DFF_is_debug:\n                print(\'检测对象：{}\'.format(alert_item_tags_json))\n\n            # 遍历规则\n            matched_level = OK_LEVEL\n            for rule in condition_rules:\n                rule_level      = rule[\'level\']\n                condition_logic = rule[\'conditionLogic\']\n                conditions      = rule[\'conditions\']\n\n                # 遍历条件\n                is_rule_matched = None\n                for index, condition in enumerate(conditions):\n                    target_alias = condition[\'targetAlias\']\n                    check_value = check_value_map.get(target_alias)\n\n                    condition_result = False\n                    if check_value is not None:\n                        operator = condition[\'operator\']\n                        operands = condition[\'operands\']\n\n                        condition_func = DO_CONDITION_FUNC_MAP[operator.lower()]\n                        condition_result = condition_func(check_value, operands)\n\n                        if _DFF_is_debug:\n                            if index == 0:\n                                print(\'开始检查 {} 规则\'.format(rule_level))\n\n                            _logic_name = \'\'\n                            if index > 0:\n                                _logic_name = condition_logic.upper() + \' \'\n                            print(\'> {}条件{}：{}({}) {} {} 结果：{}\'.format(\n                                    _logic_name, index + 1, target_alias, check_value, operator, operands, condition_result))\n\n                    is_rule_matched = _merge_condition_result(a=is_rule_matched, b=condition_result, logic=condition_logic)\n\n                    # 短路判断\n                    if condition_logic == \'and\' and not is_rule_matched:\n                        # `and`逻辑遇到`false`\n                        break\n                    elif condition_logic == \'or\' and is_rule_matched:\n                        # `or`逻辑遇到`true`\n                        break\n\n                # 计算告警级别\n                if is_rule_matched:\n                    matched_level = rule_level\n                    break;\n\n            if _DFF_is_debug:\n                print(\'检测结果：{}\'.format(matched_level))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, matched_level)\n\n            # 处于沉默期间的，不做任何处理\n            if not check_if_alert_should_do_action(alert_id, matched_level, is_level_changed):\n                should_do_action = False\n\n            # 触发动作\n            if should_do_action:\n                var_dict = create_var_dict(\n                        level=matched_level,\n                        prev_level=prev_level,\n                        alert_item_tags=alert_item_tags,\n                        alert_id=alert_id,\n                        duration=duration,\n                        check_value_map=check_value_map)\n\n                for action in condition_actions:\n                    action_type = action[\'type\']\n\n                    # 记录执行动作\n                    _action = _json_copy(action)\n                    action_content = None\n                    action_task    = None\n                    try:\n                        action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n                    except Exception as e:\n                        print(e)\n                    else:\n                        if action_task:\n                            if isinstance(action_task, list):\n                                actions_to_perform.extend(action_task)\n                            else:\n                                actions_to_perform.append(action_task)\n\n                    # 记录写入__keyevent指标\n                    _title              = action_content[\'title\']\n                    _content            = action_content.get(\'content\')          or None\n                    _suggestion         = action_content.get(\'suggestion\')       or None\n                    _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n\n                    keyevent = {\n                        \'title\'          : _title,\n                        \'timestamp\'      : _DFF_start_time,\n                        \'event_id\'       : alert_id,\n                        \'source\'         : \'datafluxTrigger\',\n                        \'status\'         : matched_level,\n                        \'rule_id\'        : trigger_uuid,\n                        \'rule_name\'      : trigger_name,\n                        \'type\'           : None,\n                        \'alert_item_tags\': alert_item_tags,\n                        \'action_type\'    : action_type,\n                        \'content\'        : _content,\n                        \'suggestion\'     : _suggestion,\n                        \'duration\'       : duration,\n                        \'dimensions\'     : dimensions,\n                        \'tags\'           : extra_alert_tags,\n                        \'fields\'         : _extra_alert_fields,\n                    }\n                    keyevents_to_write.append(keyevent)\n\n            # 记录告警信息\n            store_alert_info(alert_id, matched_level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 批量写入告警数据\n    if _DFF_is_debug:\n        print(\'写入__keyevent数据：{}\'.format(_json_dumps(keyevents_to_write)))\n\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.write_keyevents(keyevents_to_write)\n        print(\'DataWay返回: {}\'.format(_resp))\n\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n', 'bdba41cb855ed418c9583223e18f9030', '\'\'\'\n注意！\n本脚本由驻云官方提供，且在脚本中大量使用了DataFlux.f(x)的内部接口。\n这些内部接口可能会跟随系统更新随时改变，请勿在您的用户脚本中使用！\n\n2020-04-18 优化了动作执行速度\n2020-05-21 新增进阶模板语法，支持指标间运算\n2020-05-25 根据最新数据结构，$alert合并进$keyevent\n2020-06-09 $keyevent改为__keyevent（即相关字段同步修改）\n\'\'\'\n\nimport hashlib\nimport re\nimport time\n\nimport arrow\nimport simplejson as json\nimport six\n\nMOCK_DATA = None\nRAISE_DATAWAY_EXCEPTION = True\n\nALL_LEVELS          = (\'critical\', \'error\', \'warning\', \'info\')\nOK_LEVEL            = \'ok\'\nLEVEL_NAME_MAP      = {\'critical\': \'严重\', \'error\': \'错误\', \'warning\': \'警告\', \'info\': \'提示\', \'ok\': \'正常\'}\nCONDITION_LOGICS    = (\'and\', \'or\')\nCONDITION_OPERATORS = (\'=\', \'!=\', \'>\', \'>=\', \'<\', \'<=\', \'between\', \'like\', \'notLike\', \'in\', \'notIn\', \'regExp\', \'notRegExp\')\nACTION_TYPES        = (\'DEBUG\', \'mail\', \'dingTalkRobot\', \'ProfWangAlertHub\', \'HTTPRequest\', \'DataFluxFunc\')\nHTTP_METHODS        = (\'GET\', \'POST\')\nHTTP_BODY_TYPES     = (\'text\', \'json\', \'form\')\nPROF_WANG_LEVEL_MAP = {\'critical\': \'danger\', \'error\': \'danger\', \'warning\': \'warning\', \'info\': \'info\'}\n\n_DFF_is_debug = True\n\ndef _set_mock_data(mock_data):\n    global MOCK_DATA\n    MOCK_DATA = mock_data\n\ndef _as_str(s):\n    if not isinstance(s, six.string_types):\n        s = str(s)\n\n    return six.ensure_str(s)\n\ndef _get_md5(s):\n    h = hashlib.md5()\n    h.update(six.ensure_binary(_as_str(s)))\n\n    return h.hexdigest()\n\ndef _json_dumps(j):\n    return json.dumps(j, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n\ndef _json_copy(j):\n    return json.loads(json.dumps(j))\n\ndef _to_cn_time(d=None):\n    return arrow.get(d).to(\'Asia/Shanghai\').format(\'YYYY-MM-DD HH:mm:ss\')\n\ndef _to_iso_time(d=None):\n    return arrow.get(d).isoformat()\n\ndef __param_targets_check(targets):\n    if not isinstance(targets, list):\n        raise Exception(\'`targets` should be a list\')\n\n    for i, t in enumerate(targets):\n        if not isinstance(t, dict):\n            raise Exception(\'`targets[{}]` should be a dict\'.format(i))\n\n        if not isinstance(t.get(\'alias\'), str):\n            raise Exception(\'`targets[{}][\"alias\"]` should be a str\'.format(i))\n\n        if \'influxQL\' in t and not isinstance(t.get(\'influxQL\'), str):\n            raise Exception(\'`targets[{}][\"influxQL\"]` should be a str\'.format(i))\n\n        if \'expression\' in t and not isinstance(t.get(\'expression\'), str):\n            raise Exception(\'`targets[{}][\"expression\"]` should be a str\'.format(i))\n\ndef __param_rules_check(rules, target_alias):\n    if not isinstance(rules, list):\n        raise Exception(\'`rules` should be a list\')\n\n    for i, r in enumerate(rules):\n        if not isinstance(r, dict):\n            raise Exception(\'`rules[{}]` should be a dict\'.format(i))\n\n        if r.get(\'level\') not in ALL_LEVELS:\n            raise Exception(\'`rules[{}][\"level\"]` should be one of {}\'.format(i, \',\'.join(ALL_LEVELS)))\n\n        if r.get(\'conditionLogic\') not in CONDITION_LOGICS:\n            raise Exception(\'`rules[{}][\"conditionLogic\"]` should be one of {}\'.format(i, \',\'.join(CONDITION_LOGICS)))\n\n        if not isinstance(r.get(\'conditions\'), list):\n            raise Exception(\'`rules[{}][\"conditions\"]` should be a list\'.format(i))\n\n        if list(filter(lambda x: not isinstance(x, dict), r[\'conditions\'])):\n            raise Exception(\'Element of `rule[{}][\"conditions\"]` should be a dict\'.format(i))\n\n        for j, c in enumerate(r[\'conditions\']):\n            if c.get(\'targetAlias\') not in target_alias:\n                raise Exception(\'`rules[{}][\"conditions\"][{}][\"targetAlias\"]` should be one of {}\'.format(i, j, \',\'.join(target_alias)))\n\n            if c.get(\'operator\') not in CONDITION_OPERATORS:\n                raise Exception(\'`rules[{}][\"conditions\"][{}][\"operator\"]` should be one of {}\'.format(i, j, \',\'.join(CONDITION_OPERATORS)))\n\n            if not isinstance(c.get(\'operands\'), list):\n                raise Exception(\'`rules[{}][\"conditions\"][{}][\"operands\"]` should be a list\'.format(i, j))\n\n            if list(filter(lambda x: not isinstance(x, (str, int, float)), c[\'operands\'])):\n                raise Exception(\'Element of `rules[{}][\"conditions\"][{}][\"operands\"]` should be a str, int or float\'.format(i))\n\ndef __param_action_check(actions, parent_path):\n    if not isinstance(actions, list):\n        raise Exception(\'`{}actions` should be a list\'.format(parent_path))\n\n    for i, a in enumerate(actions):\n        if not isinstance(a, dict):\n            raise Exception(\'`{}actions[{}]` should be a dict\'.format(parent_path, i))\n\n        if a.get(\'type\') not in ACTION_TYPES:\n            raise Exception(\'`{}actions[{}][\"type\"]` should be one of {}\'.format(parent_path, i, \',\'.join(ACTION_TYPES)))\n\n        if a[\'type\'] == \'mail\':\n            if not isinstance(a.get(\'to\'), list):\n                raise Exception(\'`{}actions[{}][\"to\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'to\'])):\n                raise Exception(\'Element of `{}actions[{}][\"to\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'dingTalkRobot\':\n            if not isinstance(a.get(\'webhook\'), list):\n                raise Exception(\'`{}actions[{}][\"webhook\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'webhook\'])):\n                raise Exception(\'Element of `{}actions[{}][\"webhook\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'markdown\'), str):\n                raise Exception(\'`{}actions[{}][\"markdown\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'ProfWangAlertHub\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'title\'), str):\n                raise Exception(\'`{}actions[{}][\"title\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'content\') is not None:\n                if not isinstance(a[\'content\'], str):\n                    raise Exception(\'`{}actions[{}][\"content\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'suggestion\') is not None:\n                if not isinstance(a[\'suggestion\'], str):\n                    raise Exception(\'`{}actions[{}][\"suggestion\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'HTTPRequest\':\n            if not isinstance(a.get(\'url\'), list):\n                raise Exception(\'`{}actions[{}][\"url\"]` should be a list\'.format(parent_path, i))\n\n            if list(filter(lambda x: not isinstance(x, str), a[\'url\'])):\n                raise Exception(\'Element of `{}actions[{}][\"url\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'method\') not in HTTP_METHODS:\n                raise Exception(\'`{}actions[{}][\"method\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_METHODS)))\n\n            if a.get(\'headers\') is not None:\n                if not isinstance(a[\'headers\'], dict):\n                    raise Exception(\'`{}actions[{}][\"headers\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'headers\'].values())):\n                    raise Exception(\'Value of `{}actions[{}][\"headers\"]` should be a str\'.format(parent_path, i))\n\n            if a[\'method\'] == \'POST\':\n                if a.get(\'bodyType\') not in HTTP_BODY_TYPES:\n                    raise Exception(\'`{}actions[{}][\"bodyType\"]` should be one of {}\'.format(parent_path, i, \',\'.join(HTTP_BODY_TYPES)))\n\n                if not isinstance(a.get(\'body\'), str):\n                    raise Exception(\'`{}actions[{}][\"body\"]` should be a str\'.format(parent_path, i))\n\n        elif a[\'type\'] == \'DataFluxFunc\':\n            if not isinstance(a.get(\'scriptSetRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptSetRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'scriptRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"scriptRefName\"]` should be a str\'.format(parent_path, i))\n\n            if not isinstance(a.get(\'funcRefName\'), str):\n                raise Exception(\'`{}actions[{}][\"funcRefName\"]` should be a str\'.format(parent_path, i))\n\n            if a.get(\'kwargs\') is not None:\n                if not isinstance(a[\'kwargs\'], dict):\n                    raise Exception(\'`{}actions[{}][\"kwargs\"]` should be a dict\'.format(parent_path, i))\n\n                if list(filter(lambda x: not isinstance(x, str), a[\'kwargs\'].keys())):\n                    raise Exception(\'Key of `{}actions[{}][\"kwargs\"]` should be a str\'.format(parent_path, i))\n\ndef _param_check(trigger_info, targets, condition_check_setting, no_data_check_setting=None, silent_timeout=None, extra_alert_tags=None):\n    # 检查`trigger_info`\n    if not isinstance(trigger_info, dict):\n        raise Exception(\'`trigger_info` should be a dict\')\n\n    if not isinstance(trigger_info.get(\'uuid\'), str):\n        raise Exception(\'`trigger_info[\"uuid\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'name\'), str):\n        raise Exception(\'`trigger_info[\"name\"]` should be a str\')\n\n    if not isinstance(trigger_info.get(\'workspaceToken\'), str):\n        raise Exception(\'`trigger_info[\"workspaceToken\"]` should be a str\')\n\n    dimensions = trigger_info.get(\'dimensions\')\n    if dimensions is not None:\n        if not isinstance(dimensions, list):\n            raise Exception(\'`trigger_info[\"dimensions\"]` should be a list\')\n\n        for index, dimension in enumerate(dimensions):\n            if not isinstance(dimension, str):\n                raise Exception(\'`trigger_info[\"dimensions\"][{}]` should be a str\'.format(index))\n\n    # 检查`targets`\n    __param_targets_check(targets)\n\n    # 检查`condition_check_setting`\n    if not isinstance(condition_check_setting, dict):\n        raise Exception(\'`condition_check_setting` should be a dict\')\n\n    target_alias = [x[\'alias\'] for x in targets]\n    __param_rules_check(condition_check_setting.get(\'rules\'), target_alias=target_alias)\n    __param_action_check(condition_check_setting.get(\'actions\'), parent_path=\'condition_check_setting.\')\n\n    # 检查`no_data_check_setting`\n    if no_data_check_setting is not None:\n        if not isinstance(no_data_check_setting, dict):\n            raise Exception(\'`no_data_check_setting` should be a dict\')\n\n        if no_data_check_setting.get(\'level\') is not None:\n            if no_data_check_setting[\'level\'] not in ALL_LEVELS:\n                raise Exception(\'`no_data_check_setting[\"level\"]` should be one of {}\'.format(\',\'.join(ALL_LEVELS)))\n\n        if no_data_check_setting.get(\'timeout\') is not None:\n            if not isinstance(no_data_check_setting[\'timeout\'], int):\n                raise Exception(\'`no_data_check_setting[\"timeout\"]` should be a int\')\n\n        if no_data_check_setting.get(\'actions\') is not None:\n            __param_action_check(no_data_check_setting[\'actions\'], parent_path=\'no_data_check_setting.\')\n\n    # 检查`silent_timeout`\n    if silent_timeout is not None:\n        if not isinstance(silent_timeout, int):\n            raise Exception(\'`silent_timeout` should be an int\')\n\n    # 检查`extra_alert_tags`\n    if extra_alert_tags is not None:\n        if not isinstance(extra_alert_tags, dict):\n            raise Exception(\'`extra_alert_tags` should be an dict\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.keys())):\n            raise Exception(\'Key of `extra_alert_tags` should be a str\')\n\n        if list(filter(lambda x: not isinstance(x, str), extra_alert_tags.values())):\n            raise Exception(\'Value of `extra_alert_tags` should be a str\')\n\ndef _get_store_scope(trigger_uuid):\n    return \'builtinCheck.simpleCheck@{}\'.format(_get_md5(trigger_uuid))\n\ndef _get_alert_id(trigger_uuid, alert_item_tags_json):\n    str_to_md5 = \'{}/{}\'.format(trigger_uuid, alert_item_tags_json)\n    if _DFF_is_debug:\n        print(\'【告警对象】{}\'.format(str_to_md5))\n    return _get_md5(str_to_md5)\n\ndef _get_alert_store(trigger_uuid, key, alert_id):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.GET(store_key, scope=store_scope)\n\ndef _set_alert_store(trigger_uuid, key, alert_id, value, only_not_exists=False):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.SET(store_key, value=value, scope=store_scope, only_not_exists=only_not_exists)\n\ndef _delete_alert_store(trigger_uuid, key, alert_id):\n    store_scope = _get_store_scope(trigger_uuid)\n    store_key   = \'{}@{}\'.format(key, alert_id)\n    return DFF.DEL(store_key, scope=store_scope)\n\ndef _check_if_alert_level_changed(trigger_uuid, alert_id, level):\n    prev_level = _get_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id) or OK_LEVEL\n    is_level_changed = prev_level != level\n\n    if _DFF_is_debug:\n        print(\'上次检测结果：{}，本次检测结果：{}\'.format(prev_level, level))\n\n    return is_level_changed, prev_level\n\ndef _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed=None, silent_timeout=None):\n    if is_level_changed:\n        # 级别发生改变一定触发动作\n        if _DFF_is_debug:\n            print(\'检测结果改变，一定触发动作。当前检测结果：{}\'.format(level))\n\n        return True\n\n    else:\n        if _DFF_is_debug:\n            print(\'检测结果不变，当前检测结果：{}\'.format(level))\n\n        if level != OK_LEVEL and silent_timeout is not None:\n            # 级别未发生改变时，且保持非`ok`级别，则到达超时时间才触发动作\n            alert_report_time = _get_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id) or 0\n            past_time = _DFF_start_time - alert_report_time\n            should_do_action = past_time > silent_timeout\n\n            if _DFF_is_debug:\n                print(\'距离上次告警经过：{}s\'.format(past_time))\n                if should_do_action:\n                    print(\'由于距离上次告警时间（{}s）已超过沉默时长（{}s），因此需要触发动作\'.format(past_time, silent_timeout))\n                else:\n                    print(\'由于距离上次告警时间（{}s）尚未超过沉默时长（{}s），因此不需要触发动作\'.format(past_time, silent_timeout))\n\n            return should_do_action\n\n    if _DFF_is_debug:\n        print(\'当前检测结果：{}，不需要触发动作\'.format(level))\n\n    return False\n\ndef _update_no_data_duration(trigger_uuid, alert_id, is_no_data):\n    prev_no_data_time = _get_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id)\n    if _DFF_is_debug:\n        if prev_no_data_time:\n            print(\'无数据开始时间：{}\'.format(_to_cn_time(prev_no_data_time)))\n        else:\n            print(\'首次处理无数据告警\')\n\n    if is_no_data:\n        if prev_no_data_time:\n            if _DFF_is_debug:\n                print(\'已经记录过无数据时间，继续...\')\n\n            return _DFF_start_time - prev_no_data_time\n\n        else:\n            _set_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id, value=_DFF_start_time)\n\n            if _DFF_is_debug:\n                print(\'记录无数据时间\')\n\n            return 0\n\n    else:\n        if prev_no_data_time:\n            _delete_alert_store(trigger_uuid, key=\'noDataTime\', alert_id=alert_id)\n\n            if _DFF_is_debug:\n                print(\'清除无数据时间\')\n\n            return _DFF_start_time - prev_no_data_time\n\n        else:\n            if _DFF_is_debug:\n                print(\'数据正常，无需处理\')\n\n            return 0\n\ndef _get_alert_duration(trigger_uuid, alert_id):\n    alert_level_change_time = _get_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id) or _DFF_start_time\n\n    if _DFF_is_debug:\n        print(\'告警级别变化时间：{}\'.format(_to_cn_time(alert_level_change_time)))\n\n    return _DFF_start_time - alert_level_change_time\n\ndef _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action):\n    if level == OK_LEVEL:\n        _delete_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id)\n        _delete_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id)\n        _delete_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id)\n\n    else:\n        if is_level_changed:\n            _set_alert_store(trigger_uuid, key=\'alertLevelChangeTime\', alert_id=alert_id, value=_DFF_start_time)\n            _set_alert_store(trigger_uuid, key=\'alertLevel\', alert_id=alert_id, value=level)\n\n        if should_do_action:\n            _set_alert_store(trigger_uuid, key=\'alertReportTime\', alert_id=alert_id, value=_DFF_start_time)\n\ndef _get_check_value(series_data):\n    # 获取非`time`列的索引号\n    for index, name in enumerate(series_data[\'columns\']):\n        if name == \'time\':\n            continue\n\n        return series_data[\'values\'][0][index]\n\n    return None\n\ndef _get_alert_item_tags_no_data(target_alias):\n    alert_item_tags = {\n        \'noData\': target_alias\n    }\n    return alert_item_tags\n\ndef _get_alert_item_tags_json(alert_item_tags):\n    return _json_dumps(alert_item_tags)\n\ndef _from_alert_item_tags_json(alert_item_tags_json):\n    return json.loads(alert_item_tags_json)\n\ndef __do_condition_math(check_value, operator, operand):\n    if isinstance(operand, list):\n        operand = operand[0]\n\n    try:\n        T = type(check_value)\n        operand = T(operand)\n\n        if operator == \'==\':\n            return check_value == operand\n        elif operator == \'!=\':\n            return check_value != operand\n        elif operator == \'>\':\n            return check_value > operand\n        elif operator == \'>=\':\n            return check_value >= operand\n        elif operator == \'<\':\n            return check_value < operand\n        elif operator == \'<=\':\n            return check_value <= operand\n\n    except Exception as e:\n        print(e)\n        return False\n\ndef _do_condition_eq(check_value, operand):\n    return __do_condition_math(check_value, \'==\', operand[0])\n\ndef _do_condition_ne(check_value, operand):\n    return __do_condition_math(check_value, \'!=\', operand[0])\n\ndef _do_condition_gt(check_value, operand):\n    return __do_condition_math(check_value, \'>\', operand[0])\n\ndef _do_condition_ge(check_value, operand):\n    return __do_condition_math(check_value, \'>=\', operand[0])\n\ndef _do_condition_lt(check_value, operand):\n    return __do_condition_math(check_value, \'<\', operand[0])\n\ndef _do_condition_le(check_value, operand):\n    return __do_condition_math(check_value, \'<=\', operand[0])\n\ndef _do_condition_between(check_value, operand):\n    return __do_condition_math(check_value, \'>=\', operand[0]) and __do_condition_math(check_value, \'<=\', operand[1])\n\ndef _do_condition_like(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return operand[0] in check_value\n\ndef _do_condition_not_like(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return not _do_condition_like(check_value, operand)\n\ndef _do_condition_in(check_value, operand):\n    check_value = _as_str(check_value)\n    return check_value in operand\n\ndef _do_condition_not_in(check_value, operand):\n    return not _do_condition_in(check_value, operand)\n\ndef _do_condition_regexp(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return re.search(operand[0], check_value) is not None\n\ndef _do_condition_not_regexp(check_value, operand):\n    if not isinstance(check_value, str):\n        return False\n    return not _do_condition_regexp(check_value, operand)\n\nDO_CONDITION_FUNC_MAP = {\n    \'=\'        : _do_condition_eq,\n    \'==\'       : _do_condition_eq,\n    \'!=\'       : _do_condition_ne,\n    \'>\'        : _do_condition_gt,\n    \'>=\'       : _do_condition_ge,\n    \'<\'        : _do_condition_lt,\n    \'<=\'       : _do_condition_le,\n    \'between\'  : _do_condition_between,\n    \'like\'     : _do_condition_like,\n    \'notlike\'  : _do_condition_not_like,\n    \'in\'       : _do_condition_in,\n    \'notin\'    : _do_condition_not_in,\n    \'regexp\'   : _do_condition_regexp,\n    \'notregexp\': _do_condition_not_regexp,\n}\n\ndef _merge_condition_result(a, b, logic):\n    if a is None:\n        return b\n\n    if b is None:\n        return a\n\n    if logic == \'and\':\n        return a and b\n    elif logic == \'or\':\n        return a or b\n\n    return None\n\ndef _create_var_dict(trigger_uuid, trigger_name, level, alert_item_tags, alert_id, duration, check_value_map=None, prev_level=None):\n    if prev_level is None:\n        prev_level = OK_LEVEL\n\n    prev_level_name = LEVEL_NAME_MAP[prev_level]\n\n    var_dict = {\n        \'level\'        : level,\n        \'prevLevel\'    : prev_level,\n        \'levelName\'    : LEVEL_NAME_MAP[level],\n        \'prevLevelName\': prev_level_name,\n        \'tags\'         : _json_dumps(alert_item_tags),\n        \'tagsEscaped\'  : None,\n        \'fields\'       : None,\n        \'fieldsEscaped\': None,\n        \'duration\'     : duration * 1000,\n        \'durationHuman\': None,\n        \'ruleId\'       : trigger_uuid,\n        \'ruleName\'     : trigger_name,\n        \'alertId\'      : alert_id,\n    }\n\n    # $alert 改为 __keyevent 后需要兼容的字段\n    var_dict[\'status\']         = var_dict[\'level\']\n    var_dict[\'prevStatus\']     = var_dict[\'prevLevel\']\n    var_dict[\'statusName\']     = var_dict[\'levelName\']\n    var_dict[\'prevStatusName\'] = var_dict[\'prevLevelName\']\n    var_dict[\'eventId\']        = var_dict[\'alertId\']\n\n    # 补全tags.KEY\n    for k, v in alert_item_tags.items():\n        var_dict[\'tags.\' + k] = v\n\n    # 补全fields、fields.targetAlias\n    if check_value_map is not None:\n        var_dict[\'fields\'] = _json_dumps(check_value_map)\n\n        for k, v in check_value_map.items():\n            var_dict[\'fields.\' + k] = v\n\n    # 补全fieldsEscaped、tagsEscaped\n    if var_dict[\'fields\'] is not None:\n        var_dict[\'fieldsEscaped\'] = _json_dumps(var_dict[\'fields\']).strip(\'\"\')\n\n    if var_dict[\'tags\'] is not None:\n        var_dict[\'tagsEscaped\'] = _json_dumps(var_dict[\'tags\']).strip(\'\"\')\n\n    # 补全durationHuman\n    _m, _s = divmod(duration, 60)\n    _h, _m = divmod(_m, 60)\n    _d, _h = divmod(_h, 24)\n    _human = \'{}秒\'.format(_s)\n    if _m > 0:\n        _human = \'{}分钟\'.format(_m) + _human\n    if _h > 0:\n        _human = \'{}小时\'.format(_h) + _human\n    if _d > 0:\n        _human = \'{}天\'.format(_d) + _human\n\n    var_dict[\'durationHuman\'] = _human\n\n    if _DFF_is_debug:\n        print(\'模板变量：{}\'.format(json.dumps(var_dict, ensure_ascii=False, indent=4)))\n\n    return var_dict\n\ndef __render_text(template, var_dict):\n    if template is None:\n        return None\n\n    # 优先使用高级版渲染\n    rendered_text = __render_text_advanced(template, var_dict)\n\n    for k, v in var_dict.items():\n        placeholder = \'#{\' + k + \'}\'\n        if v is None:\n            v = \'\'\n\n        rendered_text = rendered_text.replace(placeholder, _as_str(v))\n\n    # 去除其他未知占位符\n    rendered_text = re.sub(\'\\#\\{[^}]+\\}\', \'\', rendered_text)\n\n    if _DFF_is_debug:\n        # print(\'文案模板：{}\'.format(template))\n        print(\'渲染结果：{}\'.format(rendered_text))\n\n    return rendered_text\n\ndef __split_var_filters(expression):\n    # 分离变量和过滤器部分\n    parts = []\n    try:\n        stack = []\n        part = \'\'\n        for c in expression:\n            if c == \'|\' and len(stack) == 0:\n                parts.append(part)\n                part = \'\'\n                continue\n\n            part += c\n\n            if c in (\"\'\", \'\"\'):\n                if stack and stack[-1] == c:\n                    stack.pop()\n                else:\n                    stack.append(c)\n        if part:\n            parts.append(part)\n\n    except Exception as e:\n        pass\n\n    else:\n        parts = list(map(lambda x: x.strip(), parts))\n\n    var_key = parts[0]\n    filters = parts[1:]\n\n    # 解析过滤器\n    parsed_filters = []\n    for f in filters:\n        f_parts = f.split(\' \')\n\n        f_command = f_parts[0].lower()\n        f_argument = \'\'\n        if len(f_parts) > 1:\n            f_argument = \' \'.join(f_parts[1:])\n\n        parsed_filters.append([f_command, f_argument or None])\n\n    return var_key, parsed_filters\n\ndef __filter_round(x, ndigits):\n    x = float(x)\n\n    only_value = False\n    k_sep      = False\n    if _as_str(ndigits).startswith(\'v\'):\n        ndigits = ndigits[1:]\n        only_value = True\n\n    elif _as_str(ndigits).startswith(\'k\'):\n        ndigits = ndigits[1:]\n        k_sep = True\n\n    ndigits = int(ndigits or 0)\n    x = round(x, ndigits)\n\n    if only_value:\n        return x\n\n    pattern = \'.{}f\'.format(ndigits)\n    if k_sep:\n        pattern = \',\' + pattern\n\n    x = format(x, pattern)\n\n    return x\n\ndef __filter_as_percent(x, ndigits):\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\ndef __filter_to_percent(x, ndigits):\n    x = float(x) * 100\n    x = __filter_round(x, ndigits)\n    return _as_str(x) + \'%\'\n\ndef ___filter_split_unit_args(args):\n    args_parts = args.split(\' \')\n    from_unit = args_parts[0]\n\n    to_unit = \'auto\'\n    if len(args_parts) > 1:\n        to_unit = args_parts[1]\n\n    ndigits = 0\n    if len(args_parts) > 2:\n        ndigits = args_parts[2]\n\n    return from_unit, to_unit, ndigits\n\ndef ___filter_convert_unit(x, from_unit, to_unit, all_units, carry, ndigits=0):\n    x = float(x)\n\n    _search_units = list(map(lambda x: x.lower(), all_units))\n    _search_from  = from_unit.lower()\n    _search_to    = to_unit.lower()\n\n    from_index = -1\n    try:\n        from_index = _search_units.index(_search_from)\n    except ValueError as e:\n        pass\n\n    to_index = -1\n    try:\n        to_index = _search_units.index(_search_to)\n    except ValueError as e:\n        pass\n\n    if from_index < 0:\n        raise Exception(\'Invalid from_unit\')\n\n    if to_index >= 0:\n        x = x * (carry ** (from_index - to_index))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + to_unit\n\n    elif _search_to == \'auto\':\n        _offset = 0\n        _x = x\n        for i in range(len(all_units)):\n            if _x > carry:\n                _offset += 1\n                _x = _x / carry\n            elif _x < 1:\n                _offset -= 1\n                _x = _x * carry\n            else:\n                break\n\n        x = x * (carry ** (_offset * -1))\n        x = __filter_round(x, ndigits)\n        if isinstance(ndigits, six.string_types) and \'v\' in ndigits:\n            return x\n        else:\n            return _as_str(x) + \' \' + all_units[from_index + _offset]\n\n    else:\n        raise Exception(\'Invalid to_unit\')\n\ndef __filter_volumn(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'Byte\', \'KB\', \'MB\', \'GB\', \'TB\'],\n        carry=1000)\n\ndef __filter_volumn_i(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'Byte\', \'KiB\', \'MiB\', \'GiB\', \'TiB\'],\n        carry=1024)\n\ndef __filter_bit_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'bps\', \'Kbps\', \'Mbps\', \'Gbps\'],\n        carry=1024)\n\ndef __filter_byte_rate(x, args):\n    from_unit, to_unit, ndigits = ___filter_split_unit_args(args)\n    return ___filter_convert_unit(x=x, from_unit=from_unit, to_unit=to_unit, ndigits=ndigits,\n        all_units=[\'B/s\', \'KB/s\', \'MB/s\', \'GB/s\'],\n        carry=1024)\n\ndef __filter_upper(x, _):\n    return _as_str(x or \'\').upper()\n\ndef __filter_lower(x, _):\n    return _as_str(x or \'\').lower()\n\ndef __filter_dict_value(x, dict_str):\n    d = json.loads(dict_str)\n    return d.get(_as_str(x)) or \'\'\n\ndef __filter_if_true(x, output):\n    if x is True:\n        return output\n    else:\n        return x\n\ndef __filter_if_false(x, output):\n    if x is False:\n        return output\n    else:\n        return x\n\nRENDER_TEXT_FILTER_FUNC_MAP = {\n    \'round\'    : __filter_round,\n    \'aspercent\': __filter_as_percent,\n    \'topercent\': __filter_to_percent,\n    \'volumn\'   : __filter_volumn,\n    \'volumni\'  : __filter_volumn_i,\n    \'bitrate\'  : __filter_bit_rate,\n    \'byterate\' : __filter_byte_rate,\n    \'upper\'    : __filter_upper,\n    \'lower\'    : __filter_lower,\n    \'dictvalue\': __filter_dict_value,\n    \'iftrue\'   : __filter_if_true,\n    \'iffalse\'  : __filter_if_false,\n}\n\ndef __render_text_advanced(template, var_dict):\n    if template is None:\n        return None\n\n    rendered_text = template\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', rendered_text)\n        if not m:\n            break;\n\n        # 提取过滤处理代码\n        var_key, filters = __split_var_filters(m.group(1))\n\n        # 依次执行过滤处理\n        x = var_dict.get(var_key) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        rendered_text = rendered_text[:start] + _as_str(x) + rendered_text[end:]\n\n    return rendered_text\n\ndef _compute_target_expression(alias_value_map, expression):\n    while True:\n        # 查询模板块\n        m = re.search(\'\\{\\{ (.+?) \\}\\}\', expression)\n        if not m:\n            break;\n\n        # 提取过滤处理代码\n        alias, filters = __split_var_filters(m.group(1))\n\n        x = alias_value_map.get(alias) or None\n        for f_command, f_argument in filters:\n            try:\n                if f_command == \'eval\':\n                    x = DFF.eval(f_argument, {\'x\': x})\n                else:\n                    f_func = RENDER_TEXT_FILTER_FUNC_MAP.get(f_command)\n                    if f_func:\n                        x = f_func(x, f_argument)\n\n            except Exception as e:\n                print(e)\n\n        start, end = m.span()\n        if isinstance(x, (six.string_types)):\n            x = \'\"{}\"\'.format(_as_str(x))\n        expression = expression[:start] + _as_str(x) + expression[end:]\n\n    target_expression_result = None\n    try:\n        target_expression_result =  DFF.eval(expression)\n\n    except Exception as e:\n        print(e)\n\n    finally:\n        return target_expression_result\n\ndef _create_action_debug(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title   = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title   = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': content,\n    }\n\n    return action_content, None\n\ndef _create_action_mail(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    to      = action[\'to\']\n    title   = action[\'title\']\n    content = action.get(\'content\') or title\n\n    title   = __render_text(title, var_dict)\n    content = __render_text(content, var_dict)\n\n    # 动作任务\n    action_task = {\n        \'name\': \'mail\',\n        \'kwargs\': {\n            \'to\'     : to,\n            \'title\'  : title,\n            \'content\': { \'text\': content },\n            \'sender\' : \'DataFlux\',\n        },\n        \'options\': {\n            \'origin\': alert_origin,\n        },\n    }\n\n    # 动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': content,\n        \'extraAlertFields\': {\n            \'to\': _json_dumps(to),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_ding_talk_robot(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    webhook  = action[\'webhook\']\n    title    = action[\'title\']\n    markdown = action[\'markdown\']\n\n    title    = __render_text(title, var_dict)\n    markdown = __render_text(markdown, var_dict)\n\n    # 执行动作\n    if not isinstance(webhook, (list, tuple)):\n        webhook = [webhook]\n\n    # 动作任务\n    action_task = []\n    for _webhook in webhook:\n        action_task.append({\n            \'name\': \'dingTalkRobot\',\n            \'kwargs\': {\n                \'webhook\': _webhook,\n                \'title\'  : title,\n                \'text\'   : markdown,\n                \'msgType\': \'markdown\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': markdown,\n        \'extraAlertFields\': {\n            \'webhook\': _json_dumps(webhook),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_prof_wang_alert_hub(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    url        = action[\'url\']\n    title      = action[\'title\']\n    content    = action.get(\'content\') or title\n    suggestion = action.get(\'suggestion\')\n\n    title      = __render_text(title, var_dict)\n    content    = __render_text(content, var_dict)\n    suggestion = __render_text(suggestion, var_dict)\n\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        alert_status = \'ok\'\n        if var_dict[\'level\'] != \'ok\':\n            alert_status = \'alert\'\n\n        level = var_dict[\'level\']\n        if level == OK_LEVEL:\n            level = var_dict[\'prevLevel\'] or \'info\'\n\n        profwang_level = PROF_WANG_LEVEL_MAP.get(level) or \'info\'\n\n        body = {\n            \'alertId\'    : var_dict[\'alertId\'],\n            \'alertStatus\': alert_status,\n            \'level\'      : profwang_level,\n            \'type\'       : \'alarm\',\n            \'title\'      : title,\n            \'content\'    : content,\n            \'suggestion\' : suggestion,\n            \'origin\'     : \'DataFlux\',\n        }\n\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\'  : \'POST\',\n                \'url\'     : _url,\n                \'body\'    : _json_dumps(body),\n                \'bodyType\': \'json\',\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'     : title,\n        \'content\'   : content,\n        \'suggestion\': suggestion,\n        \'extraAlertFields\': {\n            \'url\': _json_dumps(url),\n        }\n    }\n\n    return action_content, action_task\n\ndef _create_action_http_request(action, var_dict, alert_origin):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title    = var_dict[\'ruleName\']\n    url      = action[\'url\']\n    method   = action[\'method\'].upper()\n    headers  = action.get(\'headers\')\n    body     = action.get(\'body\')     or \'\'\n    bodyType = action.get(\'bodyType\') or \'text\'\n\n    if headers:\n        for k, v in headers.items():\n            headers[k] = __render_text(v, var_dict)\n\n    body = __render_text(body, var_dict)\n\n    # 执行动作\n    if not isinstance(url, (list, tuple)):\n        url = [url]\n\n    # 动作任务\n    action_task = []\n    for _url in url:\n        action_task.append({\n            \'name\': \'HTTPRequest\',\n            \'kwargs\': {\n                \'method\'  : method,\n                \'url\'     : _url,\n                \'headers\' : headers,\n                \'query\'   : None,\n                \'body\'    : body,\n                \'bodyType\': bodyType,\n            },\n            \'options\': {\n                \'origin\': alert_origin,\n            },\n        })\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': _json_dumps({\n            \'url\'     : url,\n            \'method\'  : method,\n            \'headers\' : headers,\n            \'body\'    : body,\n            \'bodyType\': bodyType,\n        }),\n    }\n\n    return action_content, action_task\n\ndef _create_action_dataflux_func(action, var_dict, alert_origin=None):\n    if _DFF_is_debug:\n        print(\'触发动作：{}\'.format(action[\'type\']))\n\n    title               = var_dict[\'ruleName\']\n    script_set_ref_name = action[\'scriptSetRefName\']\n    script_ref_name     = action[\'scriptRefName\']\n    func_ref_name       = action[\'funcRefName\']\n    kwargs              = action.get(\'kwargs\') or {}\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            kwargs[k] = __render_text(v, var_dict)\n\n    # 执行动作\n    DFF.FUNC(\n        script_set_ref_name=script_set_ref_name,\n        script_ref_name=script_ref_name,\n        func_ref_name=func_ref_name,\n        kwargs=kwargs)\n\n    if _DFF_is_debug:\n        print(\'触发动作：{}, 动作内容：{}\'.format(action[\'type\'], _json_dumps(action)))\n\n    # 返回动作内容\n    action_content = {\n        \'title\'  : title,\n        \'content\': _json_dumps({\n            \'scriptSetRefName\': script_set_ref_name,\n            \'scriptRefName\'   : script_ref_name,\n            \'funcRefName\'     : func_ref_name,\n            \'kwargs\'          : kwargs,\n        }),\n    }\n    return action_content, None\n\nCREATE_ACTION_FUNC_MAP = {\n    \'DEBUG\'           : _create_action_debug,\n    \'mail\'            : _create_action_mail,\n    \'dingTalkRobot\'   : _create_action_ding_talk_robot,\n    \'ProfWangAlertHub\': _create_action_prof_wang_alert_hub,\n    \'HTTPRequest\'     : _create_action_http_request,\n    \'DataFluxFunc\'    : _create_action_dataflux_func,\n}\n\n@DFF.API(\'简单检测\', category=\'builtinCheck\', is_hidden=True)\ndef simple_check(trigger_info, targets, condition_check_setting, no_data_check_setting=None, silent_timeout=None, extra_alert_tags=None):\n    \'\'\'\n    简单检测函数\n    \'\'\'\n    # 参数检查\n    _param_check(trigger_info, targets, condition_check_setting, no_data_check_setting, silent_timeout, extra_alert_tags)\n\n    # 包装上下文函数\n    trigger_uuid    = trigger_info[\'uuid\']\n    trigger_name    = trigger_info[\'name\']\n    workspace_token = trigger_info[\'workspaceToken\']\n    dimensions      = trigger_info.get(\'dimensions\') or None\n\n    alert_origin = \'DataFlux触发器：{}\'.format(trigger_name or \'DEBUG\')\n\n    dataway_ref_name = \'df_dataway\'\n\n    dataway = DFF.SRC(dataway_ref_name, token=workspace_token)\n    if _DFF_is_debug:\n        dataway.client.debug = True\n\n    def get_alert_id(alert_item_tags):\n        return _get_alert_id(trigger_uuid, alert_item_tags)\n\n    def create_var_dict(level, alert_item_tags, alert_id, duration=None, check_value_map=None, prev_level=None):\n        return _create_var_dict(trigger_uuid, trigger_name, level, alert_item_tags, alert_id, duration, check_value_map, prev_level)\n\n    def check_if_alert_level_changed(alert_id, level):\n        return _check_if_alert_level_changed(trigger_uuid, alert_id, level)\n\n    def check_if_alert_should_do_action(alert_id, level, is_level_changed):\n        return _check_if_alert_should_do_action(trigger_uuid, alert_id, level, is_level_changed, silent_timeout)\n\n    def update_no_data_duration(alert_id, is_no_data):\n        return _update_no_data_duration(trigger_uuid, alert_id, is_no_data)\n\n    def get_alert_duration(alert_id):\n        return _get_alert_duration(trigger_uuid, alert_id)\n\n    def store_alert_info(alert_id, level, is_level_changed, should_do_action):\n        return _store_alert_info(trigger_uuid, alert_id, level, is_level_changed, should_do_action)\n\n    # 查询检测目标数据\n    # 目标数据，结构如下：\n    #     {\n    #         \"<Alert Item Tags JSON>\": {\n    #             \"<Target Alias>\": \"<Check Value>\"\n    #         }\n    #     }\n\n    target_data_map    = {}\n    no_data_alias_list = []\n\n    # 先搜集查询数据\n    for target in filter(lambda x: x.get(\'influxQL\'), targets):\n        db_res = None\n        if MOCK_DATA:\n            db_res = MOCK_DATA[target[\'alias\']]\n        else:\n            bind_params = {}\n            if \'$now\' in target[\'influxQL\']:\n                bind_params[\'now\'] = _to_iso_time(_DFF_trigger_time)\n\n            db_res = DFF.SRC(\'df_influxdb\').query(target[\'influxQL\'], bind_params=bind_params)\n\n        target_alias = target[\'alias\']\n\n        valid_series_data = []\n        if \'series\' in db_res:\n            for s in db_res[\'series\']:\n                if not s.get(\'tags\') or not all(s.get(\'tags\').values()):\n                    continue\n\n                valid_series_data.append(s)\n\n        if valid_series_data:\n            for series_data in valid_series_data:\n                alert_item_tags_json = _get_alert_item_tags_json(series_data[\'tags\'])\n\n                if alert_item_tags_json not in target_data_map:\n                    target_data_map[alert_item_tags_json] = {}\n\n                target_data_map[alert_item_tags_json][target_alias] = _get_check_value(series_data)\n\n        else:\n            no_data_alias_list.append(target_alias)\n\n    # 再搜集expression数据\n    for alert_item_tags_json, alias_value_map in target_data_map.items():\n        for target in filter(lambda x: x.get(\'expression\'), targets):\n            target_alias      = target[\'alias\']\n            target_expression = target[\'expression\']\n\n            target_data_map[alert_item_tags_json][target_alias] = _compute_target_expression(alias_value_map, target_expression)\n\n    if _DFF_is_debug:\n        print(\'检测对象及数据：{}\'.format(target_data_map))\n\n    keyevents_to_write = []\n    actions_to_perform = []\n\n    # 1. 执行无数据判断\n    no_data_check_level   = no_data_check_setting.get(\'level\') or \'critical\'\n    no_data_check_timeout = no_data_check_setting.get(\'timeout\') or 0\n    no_data_check_actions = no_data_check_setting.get(\'actions\')\n\n    # 配置了触发动作才实际运行\n    if _DFF_is_debug:\n        print(\'=== 开始执行无数据判断 ===\')\n\n    if no_data_check_actions:\n        for target in filter(lambda x: x.get(\'influxQL\'), targets):\n            # 无数据告警仅针对查询数据\n            target_alias = target[\'alias\']\n\n            alert_item_tags      = _get_alert_item_tags_no_data(target_alias)\n            alert_item_tags_json = _get_alert_item_tags_json(alert_item_tags)\n            alert_id             = get_alert_id(alert_item_tags_json)\n\n            is_no_data = target_alias in no_data_alias_list\n            no_data_duration = update_no_data_duration(alert_id, is_no_data)\n\n            # 计算告警级别\n            level = OK_LEVEL\n            if is_no_data:\n                if no_data_duration > no_data_check_timeout:\n                    level = no_data_check_level\n\n                    if _DFF_is_debug:\n                        print(\'`{}`没有数据持续（{}s），超过指定容许时长（{}s）！\'.format(target_alias, no_data_duration, no_data_check_timeout))\n\n                else:\n                    if _DFF_is_debug:\n                        print(\'`{}`没有数据持续（{}s），但尚未超过指定容许时长（{}s）！\'.format(target_alias, no_data_duration, no_data_check_timeout))\n\n            else:\n                if _DFF_is_debug:\n                    print(\'`{}`存在数据\'.format(target_alias))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, level)\n\n            # 处于沉默期间的，不执行动作\n            if not check_if_alert_should_do_action(alert_id, level, is_level_changed):\n                should_do_action = False\n\n            # 触发动作\n            if should_do_action:\n                check_value_map = {\n                    target_alias: None\n                }\n                var_dict = create_var_dict(\n                        level=level,\n                        prev_level=prev_level,\n                        alert_item_tags=alert_item_tags,\n                        alert_id=alert_id,\n                        duration=no_data_duration,\n                        check_value_map=check_value_map)\n\n                for action in no_data_check_actions:\n                    action_type = action[\'type\']\n\n                    # 记录执行动作\n                    _action = _json_copy(action)\n                    action_content = None\n                    action_task    = None\n                    try:\n                        action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n                    except Exception as e:\n                        print(e)\n                    else:\n                        if action_task:\n                            if isinstance(action_task, list):\n                                actions_to_perform.extend(action_task)\n                            else:\n                                actions_to_perform.append(action_task)\n\n                    # 记录写入__keyevent指标\n                    _title              = action_content[\'title\']\n                    _content            = action_content.get(\'content\')          or None\n                    _suggestion         = action_content.get(\'suggestion\')       or None\n                    _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n\n                    keyevent = {\n                        \'title\'          : _title,\n                        \'timestamp\'      : _DFF_start_time,\n                        \'event_id\'       : alert_id,\n                        \'source\'         : \'datafluxTrigger\',\n                        \'status\'         : level,\n                        \'rule_id\'        : trigger_uuid,\n                        \'rule_name\'      : trigger_name,\n                        \'type\'           : \'noData\',\n                        \'alert_item_tags\': alert_item_tags,\n                        \'action_type\'    : action_type,\n                        \'content\'        : _content,\n                        \'suggestion\'     : _suggestion,\n                        \'duration\'       : no_data_duration,\n                        \'dimensions\'     : dimensions,\n                        \'tags\'           : extra_alert_tags,\n                        \'fields\'         : _extra_alert_fields,\n                    }\n                    keyevents_to_write.append(keyevent)\n\n            # 记录告警信息\n            store_alert_info(alert_id, level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 2. 执行条件判断（级别从高到低）\n    if _DFF_is_debug:\n        print(\'=== 开始执行条件判断 ===\')\n\n    condition_rules = condition_check_setting[\'rules\']\n    condition_rules.sort(key=lambda x: ALL_LEVELS.index(x[\'level\']))\n    condition_actions = condition_check_setting[\'actions\']\n\n    # 配置了触发动作才实际运行\n    if condition_actions:\n        # 遍历检测对象\n        for alert_item_tags_json, check_value_map in target_data_map.items():\n            alert_id        = get_alert_id(alert_item_tags_json)\n            alert_item_tags = _from_alert_item_tags_json(alert_item_tags_json)\n            duration        = get_alert_duration(alert_id)\n\n            if _DFF_is_debug:\n                print(\'检测对象：{}\'.format(alert_item_tags_json))\n\n            # 遍历规则\n            matched_level = OK_LEVEL\n            for rule in condition_rules:\n                rule_level      = rule[\'level\']\n                condition_logic = rule[\'conditionLogic\']\n                conditions      = rule[\'conditions\']\n\n                # 遍历条件\n                is_rule_matched = None\n                for index, condition in enumerate(conditions):\n                    target_alias = condition[\'targetAlias\']\n                    check_value = check_value_map.get(target_alias)\n\n                    condition_result = False\n                    if check_value is not None:\n                        operator = condition[\'operator\']\n                        operands = condition[\'operands\']\n\n                        condition_func = DO_CONDITION_FUNC_MAP[operator.lower()]\n                        condition_result = condition_func(check_value, operands)\n\n                        if _DFF_is_debug:\n                            if index == 0:\n                                print(\'开始检查 {} 规则\'.format(rule_level))\n\n                            _logic_name = \'\'\n                            if index > 0:\n                                _logic_name = condition_logic.upper() + \' \'\n                            print(\'> {}条件{}：{}({}) {} {} 结果：{}\'.format(\n                                    _logic_name, index + 1, target_alias, check_value, operator, operands, condition_result))\n\n                    is_rule_matched = _merge_condition_result(a=is_rule_matched, b=condition_result, logic=condition_logic)\n\n                    # 短路判断\n                    if condition_logic == \'and\' and not is_rule_matched:\n                        # `and`逻辑遇到`false`\n                        break\n                    elif condition_logic == \'or\' and is_rule_matched:\n                        # `or`逻辑遇到`true`\n                        break\n\n                # 计算告警级别\n                if is_rule_matched:\n                    matched_level = rule_level\n                    break;\n\n            if _DFF_is_debug:\n                print(\'检测结果：{}\'.format(matched_level))\n\n            should_do_action = True\n\n            # 判断级别是否发生改变\n            is_level_changed, prev_level = check_if_alert_level_changed(alert_id, matched_level)\n\n            # 处于沉默期间的，不做任何处理\n            if not check_if_alert_should_do_action(alert_id, matched_level, is_level_changed):\n                should_do_action = False\n\n            # 触发动作\n            if should_do_action:\n                var_dict = create_var_dict(\n                        level=matched_level,\n                        prev_level=prev_level,\n                        alert_item_tags=alert_item_tags,\n                        alert_id=alert_id,\n                        duration=duration,\n                        check_value_map=check_value_map)\n\n                for action in condition_actions:\n                    action_type = action[\'type\']\n\n                    # 记录执行动作\n                    _action = _json_copy(action)\n                    action_content = None\n                    action_task    = None\n                    try:\n                        action_content, action_task = CREATE_ACTION_FUNC_MAP[action_type](_action, var_dict, alert_origin)\n                    except Exception as e:\n                        print(e)\n                    else:\n                        if action_task:\n                            if isinstance(action_task, list):\n                                actions_to_perform.extend(action_task)\n                            else:\n                                actions_to_perform.append(action_task)\n\n                    # 记录写入__keyevent指标\n                    _title              = action_content[\'title\']\n                    _content            = action_content.get(\'content\')          or None\n                    _suggestion         = action_content.get(\'suggestion\')       or None\n                    _extra_alert_fields = action_content.get(\'extraAlertFields\') or None\n\n                    keyevent = {\n                        \'title\'          : _title,\n                        \'timestamp\'      : _DFF_start_time,\n                        \'event_id\'       : alert_id,\n                        \'source\'         : \'datafluxTrigger\',\n                        \'status\'         : matched_level,\n                        \'rule_id\'        : trigger_uuid,\n                        \'rule_name\'      : trigger_name,\n                        \'type\'           : None,\n                        \'alert_item_tags\': alert_item_tags,\n                        \'action_type\'    : action_type,\n                        \'content\'        : _content,\n                        \'suggestion\'     : _suggestion,\n                        \'duration\'       : duration,\n                        \'dimensions\'     : dimensions,\n                        \'tags\'           : extra_alert_tags,\n                        \'fields\'         : _extra_alert_fields,\n                    }\n                    keyevents_to_write.append(keyevent)\n\n            # 记录告警信息\n            store_alert_info(alert_id, matched_level, is_level_changed=is_level_changed, should_do_action=should_do_action)\n\n    # 批量写入告警数据\n    if _DFF_is_debug:\n        print(\'写入__keyevent数据：{}\'.format(_json_dumps(keyevents_to_write)))\n\n    if len(keyevents_to_write) > 0:\n        _resp = dataway.write_keyevents(keyevents_to_write)\n        print(\'DataWay返回: {}\'.format(_resp))\n\n    # 批量执行动作\n    if _DFF_is_debug:\n        print(\'执行动作：{}\'.format(_json_dumps(actions_to_perform)))\n\n    if len(actions_to_perform) > 0:\n        _resp = DFF.MSG_BATCH(actions_to_perform)\n        print(\'MessageDesk返回: {}\'.format(_resp))\n', 'bdba41cb855ed418c9583223e18f9030', '2020-04-02 17:47:45', '2020-06-08 13:01:12'),
('3', 'scpt-ft_pred', 'FT预测函数', '预测函数脚本集', 'sset-ft_lib', 'ft_pred', '1', 'python', 'import numpy as np\nimport random\nimport requests\nimport pandas as pd\nfrom math import sqrt\nfrom statsmodels.tsa.api import ExponentialSmoothing\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n@DFF.API(\'平均值预测\', category=\'prediction\')\ndef avg_prediction(dps):\n    \'\'\'\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[\'avg_forecast\'] = train[1].mean()\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    mean = train[\'avg_forecast\'][0]\n\n    ret = []\n    for i in range(50):\n        time_point += time_diff\n        value = round(mean,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'移动均值预测\', category=\'prediction\')\ndef moving_prediction(dps):\n    \'\'\'\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[\'moving_avg_forecast\'] = train[1].rolling(60).mean().iloc[-1]\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    mean = train[\'moving_avg_forecast\'][0]\n    ret = []\n    for i in range(50):\n        time_point += time_diff\n        value = round(mean,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'Holt函数预测\', category=\'prediction\')\ndef holt_prediction(dps):\n    \"\"\"\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \"\"\"\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    fit1 = ExponentialSmoothing(np.asarray(train[1]),seasonal_periods=7,trend=\'add\',seasonal=\'add\').fit()\n    y_hat_avg = fit1.forecast(50)\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    ret = []\n    for i in y_hat_avg:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n@DFF.API(\'MA函数预测\', category=\'prediction\')\ndef ma_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = ARMA(data, order=(0,7))\n    model_fit = model.fit(disp=False)\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n    print(yhat)\n    print(time_diff)\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'AR函数预测\', category=\'prediction\')\ndef ar_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = AR(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+30)\n\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'ARIMA函数预测\', category=\'prediction\')\ndef arima_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = AR(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\ndef test_prediction():\n    dps =[[1581777800000, -33.55444223812124],[1581777810000, -50.83663612870732],[1581777820000, -39.55672134160139],[1581777830000, -38.39518359982622],[1581777840000, -36.22997922350511],[1581777850000, -36.78477147596941],\n[1581777860000, -20.12616034401329],\n[1581777870000, -36.54856240481572],\n[1581777880000, -30.31006152436570],\n[1581777890000, -42.53321660981094],\n[1581777900000, -13.20427195011425],\n[1581777910000, -14.53546668748857],\n[1581777920000, -15.55725246006902],\n[1581777930000, -26.76065716597191],\n[1581777940000, -35.55599811602032]]\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = ExponentialSmoothing(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n    print(yhat)\n    print(time_diff)\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n', '8bb88a6c1c66af816e187f161f2bdae4', 'import numpy as np\nimport random\nimport requests\nimport pandas as pd\nfrom math import sqrt\nfrom statsmodels.tsa.api import ExponentialSmoothing\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n@DFF.API(\'平均值预测\', category=\'prediction\')\ndef avg_prediction(dps):\n    \'\'\'\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[\'avg_forecast\'] = train[1].mean()\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    mean = train[\'avg_forecast\'][0]\n\n    ret = []\n    for i in range(50):\n        time_point += time_diff\n        value = round(mean,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'移动均值预测\', category=\'prediction\')\ndef moving_prediction(dps):\n    \'\'\'\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[\'moving_avg_forecast\'] = train[1].rolling(60).mean().iloc[-1]\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    mean = train[\'moving_avg_forecast\'][0]\n    ret = []\n    for i in range(50):\n        time_point += time_diff\n        value = round(mean,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'Holt函数预测\', category=\'prediction\')\ndef holt_prediction(dps):\n    \"\"\"\n    一个对时序数据进行平均值预测\n    参数：\n        dps【必须】: [ , ... ]\n    返回：\n        [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \"\"\"\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    fit1 = ExponentialSmoothing(np.asarray(train[1]),seasonal_periods=7,trend=\'add\',seasonal=\'add\').fit()\n    y_hat_avg = fit1.forecast(50)\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    ret = []\n    for i in y_hat_avg:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n@DFF.API(\'MA函数预测\', category=\'prediction\')\ndef ma_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = ARMA(data, order=(0,7))\n    model_fit = model.fit(disp=False)\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n    print(yhat)\n    print(time_diff)\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'AR函数预测\', category=\'prediction\')\ndef ar_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = AR(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+30)\n\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n@DFF.API(\'ARIMA函数预测\', category=\'prediction\')\ndef arima_prediction(dps):\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = AR(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\ndef test_prediction():\n    dps =[[1581777800000, -33.55444223812124],[1581777810000, -50.83663612870732],[1581777820000, -39.55672134160139],[1581777830000, -38.39518359982622],[1581777840000, -36.22997922350511],[1581777850000, -36.78477147596941],\n[1581777860000, -20.12616034401329],\n[1581777870000, -36.54856240481572],\n[1581777880000, -30.31006152436570],\n[1581777890000, -42.53321660981094],\n[1581777900000, -13.20427195011425],\n[1581777910000, -14.53546668748857],\n[1581777920000, -15.55725246006902],\n[1581777930000, -26.76065716597191],\n[1581777940000, -35.55599811602032]]\n    train = pd.DataFrame(dps)\n    train.index = pd.to_datetime(train[0],unit=\'ms\',origin=pd.Timestamp(\'1970-01-01 08:00:00\'))\n    train[1] = train[1].astype(\"float64\")\n    train[0] = train[0].astype(\"int64\")\n    time_point = train[0][len(train[0])-1]\n    time_diff = train[0][len(train[0])-1] - train[0][len(train[0])-2]\n    data = train[1].tolist()\n    model = ExponentialSmoothing(data)\n    model_fit = model.fit()\n    # make prediction\n    yhat = model_fit.predict(len(data), len(data)+20)\n    print(yhat)\n    print(time_diff)\n    ret = []\n    for i in yhat:\n        time_point += time_diff\n        value = round(i,2)\n        if np.isnan(value):\n            value = None\n        ret.append([\n            int(time_point),\n            value\n        ])\n    return ret\n\n', '8bb88a6c1c66af816e187f161f2bdae4', '2020-04-02 17:47:45', '2020-04-02 17:48:37'),
('4', 'scpt-ft_tran', 'FT转换函数', '转换函数脚本集', 'sset-ft_lib', 'ft_tran', '1', 'python', 'import pandas as pd\nimport numpy as np\nimport math\n# from functools import reduce\n# import traceback\n\n@DFF.API(\'ABS绝对值转换\', category=\'transformation\')\ndef abs_transformation(dps):\n    \'\'\'\n    返回数字的绝对值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").abs()\n    return tran.values.tolist()\n\n@DFF.API(\'AVG平均值转换\', category=\'transformation\')\ndef avg_transformation(dps):\n    \'\'\'\n    返回数字的平均值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").mean()\n    return tran.values.tolist()\n\n@DFF.API(\'MAX最大值转换\', category=\'transformation\')\ndef max_transformation(dps):\n    \'\'\'\n    返回数字的最大值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").max()\n    return tran.values.tolist()\n\n@DFF.API(\'MIN最小值转换\', category=\'transformation\')\ndef min_transformation(dps):\n    \'\'\'\n    返回数字的最小值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").min()\n    return tran.values.tolist()\n\n@DFF.API(\'Even向上取偶数转换\',category=\'transformation\')\ndef even_transformation(dps):\n    \'\'\'\n    将数字向上舍入为最接近的偶数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if math.ceil(df[1][i])%2==0:\n            df[1][i]=math.ceil(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])+1\n    return df.values.tolist()\n\n@DFF.API(\'Odd向上取奇数转换\',category=\'transformation\')\ndef odd_transformation(dps):\n    \'\'\'\n    将数字向上舍入为最接近的奇数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if math.ceil(df[1][i])%2!=0:\n            df[1][i]=math.ceil(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])+1\n    return df.values.tolist()\n\n@DFF.API(\'Fact取整阶乘转换\',category=\'transformation\')\ndef fact_transformation(dps):\n    \'\'\'\n    返回数字的阶乘。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        product = 1\n        for j in range(math.ceil(df[1][i])):\n            product=product*(j+1)\n        df[1][i]=product\n        # df[1][i]=math.factorial(math.ceil(df[1][i]))\n    return df.values.tolist()\n\n@DFF.API(\'SumSQ平方和转换\',category=\'transformation\')\ndef sumsq_transformation(dps):\n    \'\'\'\n    返回数字的平方和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    df[1]=list(map(lambda x:x**2*2,df[1]))\n    return df.values.tolist()\n\n\n@DFF.API(\'RoundUp绝对值增大取整转换\',category=\'transformation\')\ndef roundup_transformation(dps):\n    \'\'\'\n     向绝对值增大的方向舍入数字。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if df[1][i]<0:\n            df[1][i]=math.floor(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'RoundDown绝对值减小取整转换\',category=\'transformation\')\ndef rounddown_transformation(dps):\n    \'\'\'\n      向绝对值减小的方向舍入数字。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if df[1][i]>0:\n            df[1][i]=math.floor(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Power指数幂转换\',category=\'transformation\')\ndef power_transformation(dps,significance):\n    \'\'\'\n       返回给定次幂的结果。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        significance:指定指数\n    \'\'\'\n    try:\n        significance=float(significance)\n        df=pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i]**significance\n        return df.values.tolist()\n    except:\n        # info=traceback.format_exc()\n        info=significance+\' is not a number. \'\n        return info\n\n@DFF.API(\'Ceiling取整转换\',category=\'transformation\')\ndef ceiling_transformation(dps,significance):\n    \'\'\'\n    将数字四舍五入为最接近的整数或最接近的指定基数的倍数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        significance:指定基数的倍数\n    \'\'\'\n    try:\n        significance=float(significance)\n        df=pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            if (df[1][i]/significance)>=1:\n                df[1][i]=round(df[1][i]/significance)*significance\n            else:df[1][i]=(round(df[1][i]/significance)+1)*significance\n        return df.values.tolist()\n    except:\n        # info=traceback.format_exc()\n        info=significance+\' is not a number. \'\n        print(info)\n        return info\n\n@DFF.API(\'AccumuAll累加转换\',category=\'transformation\')\ndef accumuall_transformation(dps):\n    \'\'\'\n    将前n个元素累加求和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    total=0\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        total+=df[1][i]\n        df[1][i]=total\n    return df.values.tolist()\n\n@DFF.API(\'Accumu2求和转换\',category=\'transformation\')\ndef accumu2_transformation(dps):\n    \'\'\'\n    将每相邻2个元素累加求和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    m=df[1].copy()\n    for i in range(0,len(df[1])):\n        if i>0:\n            df[1][i]=m[i-1]+df[1][i]\n    return df.values.tolist()\n\n@DFF.API(\'ACOSH反双曲余弦转换\',category=\'transformation\')\ndef acosh_transformation(dps):\n    \'\'\'\n    返回数字的反双曲余弦值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    #acosh(number),number必须大于/等于1\n    df[1]=df[1].abs()\n    for i in range(0,len(df[1])):\n        if df[1][i]<1:\n            df[1][i]=math.acosh(df[1][i]+1)\n        else:\n             df[1][i]=math.acosh(df[1][i])\n    return df.values.tolist()\n\n\n\ndef test_acosh():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return acosh_transformation(dps)\n\ndef test_accumu2():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return accumu2_transformation(dps)\n\n\ndef test_abs():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return abs_transformation(dps)\n\ndef test_avg():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return avg_transformation(dps)\n\ndef test_max():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return max_transformation(dps)\n\ndef test_min():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return min_transformation(dps)\n\ndef test_even():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return even_transformation(dps)\n\ndef test_odd():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return odd_transformation(dps)\n\ndef test_fact():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return fact_transformation(dps)\n\ndef test_sumsq():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return sumsq_transformation(dps)\n\ndef test_roundup():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return roundup_transformation(dps)\n\ndef test_rounddown():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return rounddown_transformation(dps)\ndef test_power():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    a=\"2\"\n    return power_transformation(dps,a)\n\ndef test_ceiling():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return ceiling_transformation(dps,2)\n\ndef test_accumall():\n    dps = [[1576765000100, 20.3], [1576765000200, 50], [1576765000300, 25.8]]\n    return accumuall_transformation(dps)\n\n\n\n\n', '4ed62b83d3fdc8987b03322679caebe4', 'import pandas as pd\nimport numpy as np\nimport math\n# from functools import reduce\n# import traceback\n\n@DFF.API(\'ABS绝对值转换\', category=\'transformation\')\ndef abs_transformation(dps):\n    \'\'\'\n    返回数字的绝对值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").abs()\n    return tran.values.tolist()\n\n@DFF.API(\'AVG平均值转换\', category=\'transformation\')\ndef avg_transformation(dps):\n    \'\'\'\n    返回数字的平均值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").mean()\n    return tran.values.tolist()\n\n@DFF.API(\'MAX最大值转换\', category=\'transformation\')\ndef max_transformation(dps):\n    \'\'\'\n    返回数字的最大值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").max()\n    return tran.values.tolist()\n\n@DFF.API(\'MIN最小值转换\', category=\'transformation\')\ndef min_transformation(dps):\n    \'\'\'\n    返回数字的最小值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    tran = pd.DataFrame(dps)\n    tran[1] = tran[1].astype(\"float64\").min()\n    return tran.values.tolist()\n\n@DFF.API(\'Even向上取偶数转换\',category=\'transformation\')\ndef even_transformation(dps):\n    \'\'\'\n    将数字向上舍入为最接近的偶数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if math.ceil(df[1][i])%2==0:\n            df[1][i]=math.ceil(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])+1\n    return df.values.tolist()\n\n@DFF.API(\'Odd向上取奇数转换\',category=\'transformation\')\ndef odd_transformation(dps):\n    \'\'\'\n    将数字向上舍入为最接近的奇数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if math.ceil(df[1][i])%2!=0:\n            df[1][i]=math.ceil(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])+1\n    return df.values.tolist()\n\n@DFF.API(\'Fact取整阶乘转换\',category=\'transformation\')\ndef fact_transformation(dps):\n    \'\'\'\n    返回数字的阶乘。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        product = 1\n        for j in range(math.ceil(df[1][i])):\n            product=product*(j+1)\n        df[1][i]=product\n        # df[1][i]=math.factorial(math.ceil(df[1][i]))\n    return df.values.tolist()\n\n@DFF.API(\'SumSQ平方和转换\',category=\'transformation\')\ndef sumsq_transformation(dps):\n    \'\'\'\n    返回数字的平方和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    df[1]=list(map(lambda x:x**2*2,df[1]))\n    return df.values.tolist()\n\n\n@DFF.API(\'RoundUp绝对值增大取整转换\',category=\'transformation\')\ndef roundup_transformation(dps):\n    \'\'\'\n     向绝对值增大的方向舍入数字。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if df[1][i]<0:\n            df[1][i]=math.floor(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'RoundDown绝对值减小取整转换\',category=\'transformation\')\ndef rounddown_transformation(dps):\n    \'\'\'\n      向绝对值减小的方向舍入数字。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        if df[1][i]>0:\n            df[1][i]=math.floor(df[1][i])\n        else:\n            df[1][i]=math.ceil(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Power指数幂转换\',category=\'transformation\')\ndef power_transformation(dps,significance):\n    \'\'\'\n       返回给定次幂的结果。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        significance:指定指数\n    \'\'\'\n    try:\n        significance=float(significance)\n        df=pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i]**significance\n        return df.values.tolist()\n    except:\n        # info=traceback.format_exc()\n        info=significance+\' is not a number. \'\n        return info\n\n@DFF.API(\'Ceiling取整转换\',category=\'transformation\')\ndef ceiling_transformation(dps,significance):\n    \'\'\'\n    将数字四舍五入为最接近的整数或最接近的指定基数的倍数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        significance:指定基数的倍数\n    \'\'\'\n    try:\n        significance=float(significance)\n        df=pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            if (df[1][i]/significance)>=1:\n                df[1][i]=round(df[1][i]/significance)*significance\n            else:df[1][i]=(round(df[1][i]/significance)+1)*significance\n        return df.values.tolist()\n    except:\n        # info=traceback.format_exc()\n        info=significance+\' is not a number. \'\n        print(info)\n        return info\n\n@DFF.API(\'AccumuAll累加转换\',category=\'transformation\')\ndef accumuall_transformation(dps):\n    \'\'\'\n    将前n个元素累加求和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    total=0\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        total+=df[1][i]\n        df[1][i]=total\n    return df.values.tolist()\n\n@DFF.API(\'Accumu2求和转换\',category=\'transformation\')\ndef accumu2_transformation(dps):\n    \'\'\'\n    将每相邻2个元素累加求和。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    m=df[1].copy()\n    for i in range(0,len(df[1])):\n        if i>0:\n            df[1][i]=m[i-1]+df[1][i]\n    return df.values.tolist()\n\n@DFF.API(\'ACOSH反双曲余弦转换\',category=\'transformation\')\ndef acosh_transformation(dps):\n    \'\'\'\n    返回数字的反双曲余弦值。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    #acosh(number),number必须大于/等于1\n    df[1]=df[1].abs()\n    for i in range(0,len(df[1])):\n        if df[1][i]<1:\n            df[1][i]=math.acosh(df[1][i]+1)\n        else:\n             df[1][i]=math.acosh(df[1][i])\n    return df.values.tolist()\n\n\n\ndef test_acosh():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return acosh_transformation(dps)\n\ndef test_accumu2():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return accumu2_transformation(dps)\n\n\ndef test_abs():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return abs_transformation(dps)\n\ndef test_avg():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return avg_transformation(dps)\n\ndef test_max():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return max_transformation(dps)\n\ndef test_min():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return min_transformation(dps)\n\ndef test_even():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return even_transformation(dps)\n\ndef test_odd():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return odd_transformation(dps)\n\ndef test_fact():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return fact_transformation(dps)\n\ndef test_sumsq():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return sumsq_transformation(dps)\n\ndef test_roundup():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return roundup_transformation(dps)\n\ndef test_rounddown():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return rounddown_transformation(dps)\ndef test_power():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    a=\"2\"\n    return power_transformation(dps,a)\n\ndef test_ceiling():\n    dps = [[1576765000100, 20.3], [1576765000200, -20.5], [1576765000300, 25.8]]\n    return ceiling_transformation(dps,2)\n\ndef test_accumall():\n    dps = [[1576765000100, 20.3], [1576765000200, 50], [1576765000300, 25.8]]\n    return accumuall_transformation(dps)\n\n\n\n\n', '4ed62b83d3fdc8987b03322679caebe4', '2020-04-02 17:47:45', '2020-04-02 17:48:37'),
('5', 'scpt-ft_tran_str', 'FT转换函数-文本转换', '文本转换用于字符串处理', 'sset-ft_lib', 'ft_tran_str', '1', 'python', 'import pandas as pd\n\n@DFF.API(\'Clean删除不可打印字符\',category=\'transformation\')\ndef clean_transformation(dps):\n    \'\'\'\n    从文本中删除不可打印的字符，去掉了前31个特殊的ASCII字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        word = \'\'\n        for j in df[1][i]:\n            if ord(j)>31:\n                word +=j\n        print(df[1][i])\n        df[1][i]=word\n        print(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Fixed数字格式化\',category=\'transformation\')\ndef fixed_transformation(dps,no_commas=\'True\',decimals=2):\n    \'\'\'\n    将数字格式设置为具有固定小数位数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        decimals:进行四舍五入后小数点后面需要保留的位数,如果是负数，则在小数点左侧进行四舍五入。若此参数                         decimals省略，则默认此参数为2。\n        no_commas:如果是 true，返回的结果不包含逗号千分位。如果是false或省略不写，返回的结果包含逗号千分位。\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        decimals=int(decimals)\n        for i in range(0,len(df[1])):\n            df[1][i]=round(df[1][i],decimals)\n            if no_commas.title()==\'False\':\n                df[1][i]=\'{:,}\'.format(df[1][i])\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'Left截取左边字符\',category=\'transformation\')\ndef left_transformation(dps,num_chars=1):\n    \'\'\'\n    返回文本值中最左边的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        num_chars:需要截取字符的个数。默认截取1个\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][0:num_chars]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'Right截取右边字符\',category=\'transformation\')\ndef right_transformation(dps,num_chars=None):\n    \'\'\'\n    返回文本值中最右边的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        num_chars:需要截取字符的个数。默认截取1个\n    \'\'\'\n    try:\n        if num_chars is None:\n            num_chars = 1\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][-num_chars:]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'LEN字符串长度\',category=\'transformation\')\ndef len_transformation(dps):\n    \'\'\'\n    返回文本字符串中的字符数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        df[1][i]=len(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Lower字符小写转换\',category=\'transformation\')\ndef lower_transformation(dps):\n    \'\'\'\n    将文本转换为小写。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        df[1][i]=df[1][i].lower()\n    return df.values.tolist()\n\n@DFF.API(\'MID截取字符\',category=\'transformation\')\ndef mid_transformation(dps,start_num=1,num_chars=1,):\n    \'\'\'\n     在文本字符串中,从您所指定的位置开始返回特定数量的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        start_num:从左侧第几位开始截取。\n        num_chars:需要截取字符的个数。默认截取1个。\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        start_num=int(start_num)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][start_num-1:start_num-1+num_chars]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'PROPER首字母转大写\',category=\'transformation\')\ndef proper_transformation(dps,sep=None):\n    \'\'\'\n    将字符串的首字母转为大写，其余变为小写规范化输出\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        sep【默认是空格】：要转换的文本的切分符\n    \'\'\'\n    try:\n        if sep is None:\n            sep = \' \'\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            arr = df[1][i].split(sep)\n            df[1][i]=\' \'.join(list(map(normallize,arr)))\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'REPLACE替换目标字符\',category=\'transformation\')\ndef replace_transformation(dps,target,replace):\n    \'\'\'\n    替换字符串中的目标字符\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        target：被替换的字符\n        replace:要替换的字符\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].replace(target,replace)\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'REPT复制目标文本\',category=\'transformation\')\ndef rept_transformation(dps,target,num):\n    \'\'\'\n    复制目标文本追加到文本末尾\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        target：要复制的字符\n        num:复制次数\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i] + target*int(num)\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'SUBSTITUTE替换文本\',category=\'transformation\')\ndef substitute_transformation(dps,replace):\n    \'\'\'\n    在文本字符串中用新文本替换旧文本\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        replace：进行替换的字符\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= replace\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'TRIM去除文本空格\',category=\'transformation\')\ndef trim_transformation(dps):\n    \'\'\'\n    去除文本中空格\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].replace(\' \',\'\')\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'UPPER将文本转为大写\',category=\'transformation\')\ndef upper_transformation(dps):\n    \'\'\'\n    将文本转为大写\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].upper()\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\ndef normallize(name):\n    return name.capitalize()\n\n\ndef test_proper():\n    dps = [[1576765000000, \"sfoaj fajfioaj\"], [1576765001000, \"qeAja afq\"], [1576765002000, \"afakjfa fafa\"], [1576765003000, \"aot ha\"],[1576765004000, \"faq fa\"], [1576765005000, \"bafla ajw\"], [1576765006000, \"afkaj qop\"], [1576765007000, \"fajqq jlj\"],[1576765008000, \"dga afw\"], [1576765009000, \"dfa awo\"], [1576765010000, \"qwk weqb\"], [1576765011000, \"fakljqv bse\"]]\n    # return proper_transformation(dps)\n    # return replace_transformation(dps,\'fa\',\'AA\')\n    # return right_transformation(dps, \'3\')\n    # return rept_transformation(dps,\"AA\",\"2\")\n    # return substitute_transformation(dps,\"aaaa\")\n    # return trim_transformation(dps)\n    return upper_transformation(dps)\n\ndef test_mid():\n    dps = [[1576765000100, \"111DTTTTTSd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return mid_transformation(dps,1,3)\n\ndef test_lower():\n    dps = [[1576765000100, \"111DTTTTTSd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return lower_transformation(dps)\n\ndef test_len():\n    dps = [[1576765000100, \"111Dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return len_transformation(dps)\n\ndef test_left():\n    dps = [[1576765000100, \"111dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss\\t等等\']]\n    return left_transformation(dps,\'3\')\ndef test_clean():\n    dps = [[1576765000100, \"111dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss\\t等等\']]\n    return clean_transformation(dps)\n\ndef test_fixed():\n    dps = [[1576765000100, 438272.7651], [1576765000200, 43318272.7651], [1576765000300, 43823472.51]]\n    return fixed_transformation(dps,\"false\",\"1\")\n\n\n\n\n\n\n', '961549ae168cd3d5933063c87881b918', 'import pandas as pd\n\n@DFF.API(\'Clean删除不可打印字符\',category=\'transformation\')\ndef clean_transformation(dps):\n    \'\'\'\n    从文本中删除不可打印的字符，去掉了前31个特殊的ASCII字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        word = \'\'\n        for j in df[1][i]:\n            if ord(j)>31:\n                word +=j\n        print(df[1][i])\n        df[1][i]=word\n        print(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Fixed数字格式化\',category=\'transformation\')\ndef fixed_transformation(dps,no_commas=\'True\',decimals=2):\n    \'\'\'\n    将数字格式设置为具有固定小数位数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        decimals:进行四舍五入后小数点后面需要保留的位数,如果是负数，则在小数点左侧进行四舍五入。若此参数                         decimals省略，则默认此参数为2。\n        no_commas:如果是 true，返回的结果不包含逗号千分位。如果是false或省略不写，返回的结果包含逗号千分位。\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        decimals=int(decimals)\n        for i in range(0,len(df[1])):\n            df[1][i]=round(df[1][i],decimals)\n            if no_commas.title()==\'False\':\n                df[1][i]=\'{:,}\'.format(df[1][i])\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'Left截取左边字符\',category=\'transformation\')\ndef left_transformation(dps,num_chars=1):\n    \'\'\'\n    返回文本值中最左边的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        num_chars:需要截取字符的个数。默认截取1个\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][0:num_chars]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'Right截取右边字符\',category=\'transformation\')\ndef right_transformation(dps,num_chars=None):\n    \'\'\'\n    返回文本值中最右边的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        num_chars:需要截取字符的个数。默认截取1个\n    \'\'\'\n    try:\n        if num_chars is None:\n            num_chars = 1\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][-num_chars:]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'LEN字符串长度\',category=\'transformation\')\ndef len_transformation(dps):\n    \'\'\'\n    返回文本字符串中的字符数。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        df[1][i]=len(df[1][i])\n    return df.values.tolist()\n\n@DFF.API(\'Lower字符小写转换\',category=\'transformation\')\ndef lower_transformation(dps):\n    \'\'\'\n    将文本转换为小写。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    df=pd.DataFrame(dps)\n    for i in range(0,len(df[1])):\n        df[1][i]=df[1][i].lower()\n    return df.values.tolist()\n\n@DFF.API(\'MID截取字符\',category=\'transformation\')\ndef mid_transformation(dps,start_num=1,num_chars=1,):\n    \'\'\'\n     在文本字符串中,从您所指定的位置开始返回特定数量的字符。\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        start_num:从左侧第几位开始截取。\n        num_chars:需要截取字符的个数。默认截取1个。\n    \'\'\'\n    try:\n        df=pd.DataFrame(dps)\n        num_chars=int(num_chars)\n        start_num=int(start_num)\n        for i in range(0,len(df[1])):\n            df[1][i]=df[1][i][start_num-1:start_num-1+num_chars]\n        return df.values.tolist()\n    except Exception:\n        print(\'参数错误\')\n\n@DFF.API(\'PROPER首字母转大写\',category=\'transformation\')\ndef proper_transformation(dps,sep=None):\n    \'\'\'\n    将字符串的首字母转为大写，其余变为小写规范化输出\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        sep【默认是空格】：要转换的文本的切分符\n    \'\'\'\n    try:\n        if sep is None:\n            sep = \' \'\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            arr = df[1][i].split(sep)\n            df[1][i]=\' \'.join(list(map(normallize,arr)))\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'REPLACE替换目标字符\',category=\'transformation\')\ndef replace_transformation(dps,target,replace):\n    \'\'\'\n    替换字符串中的目标字符\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        target：被替换的字符\n        replace:要替换的字符\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].replace(target,replace)\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'REPT复制目标文本\',category=\'transformation\')\ndef rept_transformation(dps,target,num):\n    \'\'\'\n    复制目标文本追加到文本末尾\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        target：要复制的字符\n        num:复制次数\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i] + target*int(num)\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'SUBSTITUTE替换文本\',category=\'transformation\')\ndef substitute_transformation(dps,replace):\n    \'\'\'\n    在文本字符串中用新文本替换旧文本\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n        replace：进行替换的字符\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= replace\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'TRIM去除文本空格\',category=\'transformation\')\ndef trim_transformation(dps):\n    \'\'\'\n    去除文本中空格\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].replace(\' \',\'\')\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\n@DFF.API(\'UPPER将文本转为大写\',category=\'transformation\')\ndef upper_transformation(dps):\n    \'\'\'\n    将文本转为大写\n    参数：\n        dps【必须】: [ [<UNIX时间戳（毫秒）>, <数值>], [<UNIX时间戳（毫秒）>, <数值>], ... ]\n    \'\'\'\n    try:\n        df = pd.DataFrame(dps)\n        for i in range(0,len(df[1])):\n            df[1][i]= df[1][i].upper()\n        return df.values.tolist()\n    except Exception:\n        print(\"参数错误\")\n\ndef normallize(name):\n    return name.capitalize()\n\n\ndef test_proper():\n    dps = [[1576765000000, \"sfoaj fajfioaj\"], [1576765001000, \"qeAja afq\"], [1576765002000, \"afakjfa fafa\"], [1576765003000, \"aot ha\"],[1576765004000, \"faq fa\"], [1576765005000, \"bafla ajw\"], [1576765006000, \"afkaj qop\"], [1576765007000, \"fajqq jlj\"],[1576765008000, \"dga afw\"], [1576765009000, \"dfa awo\"], [1576765010000, \"qwk weqb\"], [1576765011000, \"fakljqv bse\"]]\n    # return proper_transformation(dps)\n    # return replace_transformation(dps,\'fa\',\'AA\')\n    # return right_transformation(dps, \'3\')\n    # return rept_transformation(dps,\"AA\",\"2\")\n    # return substitute_transformation(dps,\"aaaa\")\n    # return trim_transformation(dps)\n    return upper_transformation(dps)\n\ndef test_mid():\n    dps = [[1576765000100, \"111DTTTTTSd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return mid_transformation(dps,1,3)\n\ndef test_lower():\n    dps = [[1576765000100, \"111DTTTTTSd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return lower_transformation(dps)\n\ndef test_len():\n    dps = [[1576765000100, \"111Dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss等等\']]\n    return len_transformation(dps)\n\ndef test_left():\n    dps = [[1576765000100, \"111dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss\\t等等\']]\n    return left_transformation(dps,\'3\')\ndef test_clean():\n    dps = [[1576765000100, \"111dd\\ndd\"], [1576765000200, \'aqs\\tss\'], [1576765000300, \'ss\\t等等\']]\n    return clean_transformation(dps)\n\ndef test_fixed():\n    dps = [[1576765000100, 438272.7651], [1576765000200, 43318272.7651], [1576765000300, 43823472.51]]\n    return fixed_transformation(dps,\"false\",\"1\")\n\n\n\n\n\n\n', '961549ae168cd3d5933063c87881b918', '2020-04-02 17:47:45', '2020-04-02 17:48:37'),
('6', 'scpt-ft_bat_log', 'FT批处理函数-日志', '批处理函数脚本-日志处理', 'sset-ft_lib', 'ft_bat_log', '1', 'python', '\'\'\'\n注意！\n本脚本由驻云官方提供，且在脚本中大量使用了DataFlux.f(x)的内部接口。\n这些内部接口可能会跟随系统更新随时改变，请勿在您的用户脚本中使用！\n\'\'\'\n\nimport time\nimport re\nimport simplejson as json\nimport jsonpath_ng\nimport pygrok\nimport time\nimport arrow\nimport dateparser\n\n_DFF_is_debug = True\n\ndef _jsonpath_value_dumps(matched):\n    values = []\n    for m in matched:\n        if isinstance(m.value, (list, tuple, dict)):\n            v = json.dumps(m.value, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n        else:\n            v = str(m.value)\n\n        values.append(v)\n\n    return \',\'.join(values)\n\ndef _overwrite_fields(log_data):\n    try:\n        points = log_data[\'points\']\n    except Exception as e:\n        return\n\n    for p in points:\n        try:\n            tags = p.get(\'__tags\')\n            if not tags:\n                continue\n\n            timestamp_us = tags.pop(\'__timestampUs\', None)\n            if not timestamp_us:\n                continue\n\n            parsed_datetime = dateparser.parse(timestamp_us)\n            p[\'__timestampUs\'] = arrow.get(parsed_datetime).timestamp * 1000000\n\n        except Exception as e:\n            continue\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'rules\'   : { \'type\': \'list\', \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（按JSON字段提取）\', category=\'builtinBatchLog\')\ndef log_process_json_abstract(log_data, rules):\n    \'\'\'\n    按照JSON方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        jsonpath_obj_map = {}\n        for r in rules:\n            alias     = r[\'alias\']\n            json_path = r[\'jsonPath\']\n            jsonpath_obj_map[alias] = jsonpath_ng.parse(json_path)\n\n        for p in points:\n            try:\n                target = json.loads(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n\n            for r in rules:\n                alias = r[\'alias\']\n\n                abstracted_value = None\n                try:\n                    matched = jsonpath_obj_map[alias].find(target)\n                    abstracted_value = _jsonpath_value_dumps(matched)\n\n                except Exception as e:\n                    continue\n\n                if abstracted_value:\n                    alias = r[\'alias\']\n                    p[\'__tags\'] = p.get(\'__tags\') or {}\n                    p[\'__tags\'][alias] = abstracted_value\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'regexp\'  : { \'type\': \'str\',  \'raiseTypeError\': True },\n    \'rules\'   : { \'type\': \'list\', \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（按正则分组提取）\', category=\'builtinBatchLog\', is_hidden=True)\ndef log_process_regexp_abstract(log_data, regexp, rules):\n    \'\'\'\n    按照正则分组方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        regexp = re.compile(regexp)\n\n        for p in points:\n            try:\n                m = regexp.search(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n            else:\n                if not m:\n                    continue\n\n            for r in rules:\n                abstracted_value = None\n                try:\n                    group_index = int(r[\'groupIndex\'].replace(\'$\', \'\'))\n                    abstracted_value = m.group(group_index)\n                except Exception as e:\n                    continue\n\n                if abstracted_value:\n                    alias = r[\'alias\']\n                    p[\'__tags\'] = p.get(\'__tags\') or {}\n                    p[\'__tags\'][alias] = abstracted_value\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'pattern\' : { \'type\': \'str\',  \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（grok方式提取）\', category=\'builtinBatchLog\')\ndef log_process_grok_abstract(log_data, pattern):\n    \'\'\'\n    按照grok方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        grok   = pygrok.Grok(pattern)\n\n        for p in points:\n            try:\n                m = grok.match(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n            else:\n                if not m:\n                    continue\n\n            p[\'__tags\'] = p.get(\'__tags\') or {}\n            p[\'__tags\'].update(m)\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n', '3941c69e26a7df5e7267a1412e709cbc', '\'\'\'\n注意！\n本脚本由驻云官方提供，且在脚本中大量使用了DataFlux.f(x)的内部接口。\n这些内部接口可能会跟随系统更新随时改变，请勿在您的用户脚本中使用！\n\'\'\'\n\nimport time\nimport re\nimport simplejson as json\nimport jsonpath_ng\nimport pygrok\nimport time\nimport arrow\nimport dateparser\n\n_DFF_is_debug = True\n\ndef _jsonpath_value_dumps(matched):\n    values = []\n    for m in matched:\n        if isinstance(m.value, (list, tuple, dict)):\n            v = json.dumps(m.value, ensure_ascii=False, sort_keys=True, separators=(\',\', \':\'))\n        else:\n            v = str(m.value)\n\n        values.append(v)\n\n    return \',\'.join(values)\n\ndef _overwrite_fields(log_data):\n    try:\n        points = log_data[\'points\']\n    except Exception as e:\n        return\n\n    for p in points:\n        try:\n            tags = p.get(\'__tags\')\n            if not tags:\n                continue\n\n            timestamp_us = tags.pop(\'__timestampUs\', None)\n            if not timestamp_us:\n                continue\n\n            parsed_datetime = dateparser.parse(timestamp_us)\n            p[\'__timestampUs\'] = arrow.get(parsed_datetime).timestamp * 1000000\n\n        except Exception as e:\n            continue\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'rules\'   : { \'type\': \'list\', \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（按JSON字段提取）\', category=\'builtinBatchLog\')\ndef log_process_json_abstract(log_data, rules):\n    \'\'\'\n    按照JSON方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        jsonpath_obj_map = {}\n        for r in rules:\n            alias     = r[\'alias\']\n            json_path = r[\'jsonPath\']\n            jsonpath_obj_map[alias] = jsonpath_ng.parse(json_path)\n\n        for p in points:\n            try:\n                target = json.loads(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n\n            for r in rules:\n                alias = r[\'alias\']\n\n                abstracted_value = None\n                try:\n                    matched = jsonpath_obj_map[alias].find(target)\n                    abstracted_value = _jsonpath_value_dumps(matched)\n\n                except Exception as e:\n                    continue\n\n                if abstracted_value:\n                    alias = r[\'alias\']\n                    p[\'__tags\'] = p.get(\'__tags\') or {}\n                    p[\'__tags\'][alias] = abstracted_value\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'regexp\'  : { \'type\': \'str\',  \'raiseTypeError\': True },\n    \'rules\'   : { \'type\': \'list\', \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（按正则分组提取）\', category=\'builtinBatchLog\', is_hidden=True)\ndef log_process_regexp_abstract(log_data, regexp, rules):\n    \'\'\'\n    按照正则分组方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        regexp = re.compile(regexp)\n\n        for p in points:\n            try:\n                m = regexp.search(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n            else:\n                if not m:\n                    continue\n\n            for r in rules:\n                abstracted_value = None\n                try:\n                    group_index = int(r[\'groupIndex\'].replace(\'$\', \'\'))\n                    abstracted_value = m.group(group_index)\n                except Exception as e:\n                    continue\n\n                if abstracted_value:\n                    alias = r[\'alias\']\n                    p[\'__tags\'] = p.get(\'__tags\') or {}\n                    p[\'__tags\'][alias] = abstracted_value\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n\nkwargs_hint = {\n    \'log_data\': { \'type\': \'dict\', \'raiseTypeError\': True },\n    \'pattern\' : { \'type\': \'str\',  \'raiseTypeError\': True },\n}\n@DFF.API(\'日志处理（grok方式提取）\', category=\'builtinBatchLog\')\ndef log_process_grok_abstract(log_data, pattern):\n    \'\'\'\n    按照grok方式处理日志\n    \'\'\'\n    try:\n        points = log_data[\'points\']\n        grok   = pygrok.Grok(pattern)\n\n        for p in points:\n            try:\n                m = grok.match(p[\'fields\'][\'__content\'])\n            except Exception as e:\n                continue\n            else:\n                if not m:\n                    continue\n\n            p[\'__tags\'] = p.get(\'__tags\') or {}\n            p[\'__tags\'].update(m)\n\n    except Exception as e:\n        pass\n\n    finally:\n        _overwrite_fields(log_data)\n\n        DFF.KODOX(\'df-logging-parsed\', log_data)\n\n        if _DFF_is_debug:\n            print(log_data)\n', '3941c69e26a7df5e7267a1412e709cbc', '2020-05-30 14:57:33', '2020-06-05 07:39:04');

INSERT INTO `biz_main_script_set` (`seq`, `id`, `title`, `description`, `refName`, `type`, `createTime`, `updateTime`) VALUES
('1', 'sset-demo', '示例脚本集', '主要包含了一些用于展示DataFlux.f(x) 功能的脚本，可以删除', 'demo', 'user', '2019-12-09 15:15:05', '2020-02-26 00:59:55'),
('2', 'sset-ft_lib', '场景支持', '场景支持脚本集', 'ft_lib', 'official', '2020-04-02 17:47:45', '2020-04-02 17:48:37');

INSERT INTO `wat_main_organization` (`seq`, `id`, `uniqueId`, `name`, `markers`, `isDisabled`, `createTime`, `updateTime`) VALUES
('1', 'o-sys', 'system', 'System Organization', NULL, '0', '2017-07-28 18:08:03', '2018-05-24 00:47:06');

INSERT INTO `wat_main_user` (`seq`, `id`, `organizationId`, `username`, `passwordHash`, `name`, `mobile`, `markers`, `roles`, `customPrivileges`, `isDisabled`, `createTime`, `updateTime`) VALUES
('1', 'u-admin', 'o-sys', 'admin', '03449cf93ebd8f67f652f9a82b2148380b2597eedd777963245472be3311e75f3ae516244b6d7648b9b044e2523c2840bdf86a852037db7e58e9b216539b2d21', '系统管理员', NULL, NULL, 'sa', '*', '0', '2017-07-28 18:08:03', '2019-02-25 03:19:24');



/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;